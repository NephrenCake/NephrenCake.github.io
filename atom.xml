<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>浅幽丶奈芙莲的个人博客</title>
  
  <subtitle>NephrenCake Blog</subtitle>
  <link href="https://nephrencake.gitee.io/atom.xml" rel="self"/>
  
  <link href="https://nephrencake.gitee.io/"/>
  <updated>2021-10-31T01:38:07.829Z</updated>
  <id>https://nephrencake.gitee.io/</id>
  
  <author>
    <name>NephrenCake</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>计算机组成原理-完结目录</title>
    <link href="https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
    <id>https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</id>
    <published>2021-10-02T02:53:51.000Z</published>
    <updated>2021-10-31T01:38:07.829Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算机组成原理-完结目录"><a href="#计算机组成原理-完结目录" class="headerlink" title="计算机组成原理-完结目录"></a>计算机组成原理-完结目录</h1><p>教程视频传送门：</p><ol><li><a href="https://www.bilibili.com/video/BV1BE411D7ii">王道计算机考研 计算机组成原理</a></li></ol><p>推荐资源：</p><ol><li>王道配套书</li></ol><p>不能只看视频，==要过一遍书，做一遍题==！</p><table><thead><tr><th align="center"><a href="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/">计算机组成原理-Part1——计算机系统概述</a></th></tr></thead><tbody><tr><td align="center"><strong><a href="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/">计算机组成原理-Part2——数据的表示和运算</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/">计算机组成原理-Part3——存储系统</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/">计算机组成原理-Part4——指令系统</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/">计算机组成原理-Part5——中央处理器</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/">计算机组成原理-Part6——总线</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/">计算机组成原理-Part7——I/O 系统</a></strong></td></tr></tbody></table>]]></content>
    
    
    <summary type="html">完结目录</summary>
    
    
    
    <category term="计算机组成原理" scheme="https://nephrencake.gitee.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="408" scheme="https://nephrencake.gitee.io/tags/408/"/>
    
  </entry>
  
  <entry>
    <title>计算机组成原理-Part7</title>
    <link href="https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/"/>
    <id>https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/</id>
    <published>2021-10-02T02:53:29.000Z</published>
    <updated>2021-10-31T01:36:13.713Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算机组成原理-Part7——I-O-系统"><a href="#计算机组成原理-Part7——I-O-系统" class="headerlink" title="计算机组成原理-Part7——I/O 系统"></a>计算机组成原理-Part7——I/O 系统</h1><p>[TOC]</p><h2 id="I-O-系统基本概念"><a href="#I-O-系统基本概念" class="headerlink" title="I/O 系统基本概念"></a>I/O 系统基本概念</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ul><li>I/O 设备：可以将数据输入到计算机，或者可以接收计算机输出数据的外部设备，包括<strong>输入设备、输出设备、外存设备</strong>。</li><li>I/O 接口：又称 I/O 控制器（I/O Controller）、设备控制器，负责协调主机与外部设备之间的数据传输<ul><li>I/O 控制器就是一块芯片，常被集成在主板上</li><li>现在的 I/O 接口（芯片）也会被集成在南桥芯片内部</li></ul></li><li>数据流：键盘 -&gt; I/O 接口的数据寄存器 -&gt; 数据总线 -&gt; CPU某寄存器 -&gt; 主存（变量 i 的对应位置）</li></ul><h3 id="控制方式"><a href="#控制方式" class="headerlink" title="控制方式"></a>控制方式</h3><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028202903178.png" alt="image-20211028202903178" style="zoom:80%;"><ul><li><strong>程序查询方式</strong>：CPU 不断轮询检查 I/O 控制器中的“状态寄存器”，<strong>直到检测到状态为“已完成”之后</strong>，才从数据寄存器取出输入数据（如程序中的输入等待）</li><li><strong>程序中断方式</strong>：等待键盘 I/O 时 CPU 可以先去执行其他程序，<strong>键盘 I/O 完成后 I/O 控制器向 CPU 发出中断请求，CPU 响应中断请求</strong>，并取走输入数据</li><li><strong>DMA 控制方式</strong>：主存与高速 I/O 设备之间有一条直接数据通路（DMA总线）。CPU 向 DMA 接口发出“读/写”命令，并指明主存地址、磁盘地址、读写数据量等参数。<ul><li>DMA 控制器自动控制磁盘与主存的数据读写，<strong>DMA 控制器与主存每次传送1个字</strong>，<strong>每当完成一整块数据读写</strong>（如 1KB 为一整块） ，才向 CPU 发出一次中断请求。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211027224155076.png" alt="image-20211027224155076" style="zoom:80%;"></li></ul></li><li><strong>通道控制方式</strong>：相比 DMA 控制方式，不仅不与高速外设进行直接相连，包括低速外设都交由通道来管理，且通道程序保存在内存中。<ul><li>针对场景：I/O 密集型场景</li><li>通道：可以理解为是“弱鸡版的 CPU”。通道可以识别并执行一系列通道指令，通道指令种类、功能通常比较单一</li><li>步骤：<ol><li>CPU 向通道发出 I/O 指令。指明通道程序在内存中的位置，并指明要操作的是哪个 I/O 设备。CPU 就可以去做其他事情</li><li>通道执行内存中的通道程序，控制 I/O 设备完成一系列任务</li><li>通道执行完规定的任务后，向 CPU 发出中断请求，之后 CPU 对中断进行处理</li></ol></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211027224229248.png" alt="image-20211027224229248" style="zoom:80%;"></li></ul></li></ul><h3 id="系统基本组成"><a href="#系统基本组成" class="headerlink" title="系统基本组成"></a>系统基本组成</h3><ul><li>一般来说，I/O 系统由 I/O 软件和 I/O 硬件两部分构成。</li><li>I/O 硬件：包括外部<strong>设备、I/O 接口 、I/O 总线</strong>等。</li><li>I/O 软件：包括<strong>驱动程序、用户程序、管理程序、升级补丁</strong>等。通常采用 <strong>I/O 指令</strong>和<strong>通道指令</strong>实现信息交换。<ul><li>I/O 指令：CPU 指令的一部分<ul><li>操作码（CPU 对 IO 接口做什么）+ 命令码（IO 接口对设备做什么）+ 设备码（对哪个设备进行操作）</li></ul></li><li>通道指令：通道能识别的指令（提前编制好放在主存中）<ul><li>在含有通道的计算机中，CPU 执行 I/O 指令对通道发出命令，由通道执行一系列通道指令，代替 CPU 对 I/O 设备进行管理</li></ul></li></ul></li></ul><h2 id="外部设备"><a href="#外部设备" class="headerlink" title="外部设备"></a>外部设备</h2><h3 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h3><ul><li>外部设备<ul><li>输入设备<ul><li>用于向计算机系统输入命令和文本、数据等信息的部件。</li><li>键盘、鼠标</li></ul></li><li>输出设备<ul><li>用于将计算机系统中的信息输出到计算机外部进行显示、交换等的部件。</li><li>显示器、打印机</li></ul></li><li>外存储器（考点：磁盘存取时间的计算）<ul><li>指除计算机内存及CPU缓存等以外的存储器。</li><li>磁盘存储器、磁盘阵列、光盘存储器、SSD</li></ul></li></ul></li></ul><h3 id="显示器"><a href="#显示器" class="headerlink" title="显示器"></a>显示器</h3><h4 id="性能标准（考点：显存的计算）"><a href="#性能标准（考点：显存的计算）" class="headerlink" title="性能标准（考点：显存的计算）"></a>性能标准（考点：显存的计算）</h4><ul><li>屏幕大小：以对角线长度表示，常用的有 12～29 英寸等。</li><li>分辨率：所能表示的像素个数，<strong>宽、高的像素的乘积</strong>，如1920×1280。</li><li>灰度级：显示的像素点的亮暗差别，典型的有 8 位（256级）、16 位等。<strong>n 位可以表示 2^n^ 种不同的亮度或颜色</strong>。</li><li>刷新：单位时间内扫描整个屏幕内容的次数，通常显示器刷新频率在 60～120 Hz。</li><li>显示存储器（VRAM）：<ul><li>也称刷新存储器，<strong>显存</strong>。由图像分辨率和灰度级决定，分辨率越高，灰度级越多，刷新存储器容量越大。</li><li>显存除了作为当前显示帧的缓存，还会用于保存用于渲染的图像数据，如 3D 模型。</li><li>集成显卡计算机中，通常分配一片内存作为显存</li><li><strong>VRAM 容量 = 分辨率 × 灰度级位数</strong><ul><li>例：1440 × 900 × 3 B ≈ 3.7MB（最低工作要求，能够存储一帧图像的信息）</li></ul></li><li><strong>VRAM 带宽 = 分辨率 × 灰度级位数 × 帧频</strong><ul><li>例：3.7 × 60 Hz = 222MB/s</li></ul></li></ul></li><li>显示器分类<ul><li>阴极射线管（CRT）显示器</li><li>液晶（LCD）显示器</li><li>发光二极管（LED）显示器</li></ul></li></ul><h4 id="阴极射线管显示器（考点：字符点阵的存储）"><a href="#阴极射线管显示器（考点：字符点阵的存储）" class="headerlink" title="阴极射线管显示器（考点：字符点阵的存储）"></a>阴极射线管显示器（考点：字符点阵的存储）</h4><p>按显示信息内容不同可分为</p><ul><li><strong>字符显示器</strong>（计算器）</li><li>图形显示器（特别用途设备）<ul><li>优点：分辨率高且显示的曲线平滑。</li><li>缺点：当显示复杂图形时，会有闪烁感。</li></ul></li><li>图像显示器（用户友好界面）</li></ul><p>字符显示器：</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028115857070.png" alt="image-20211028115857070"></li><li>显示字符的方法以点阵为基础。<ul><li><strong>点阵是指由 m×n 个点组成的阵列</strong>。</li><li>点阵的多少取决于显示<strong>字符的质量</strong>和<strong>字符窗口</strong>的大小。<ul><li><strong>字符窗口是指每个字符在屏幕上所占的点数</strong>，包括字符显示点阵和字符间隔。</li></ul></li></ul></li><li>对应于每个字符窗口，所需显示<strong>字符的 ASCII 代码被存放在视频存储器 VRAM</strong> 中，以备刷新。</li><li>将<strong>点阵存入由 ROM 构成的字符发生器</strong>中，在 CRT 进行光栅扫描的过程中，从字符发生器中依次读出某个字符的点阵，按照点阵中 0 和 1 代码不同控制扫描电子束的开或关，从而在屏幕上显示出字符。<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028115930559.png" alt="image-20211028115930559" style="zoom:80%;"></li><li>按扫描方式不同可分为<ul><li>光栅扫描显示器</li><li>随机扫描显示器</li></ul></li></ul></li></ul><h3 id="打印机"><a href="#打印机" class="headerlink" title="打印机"></a>打印机</h3><ul><li>按印字原理不同可分为<ul><li><strong>击打式打印机</strong>：利用机械动作使印字机构与色带和纸相撞而打印字符<ul><li>优：设备成本低 、印字质量好、防伪性好</li><li>缺：噪声大、速度慢</li></ul></li><li><strong>非击打式打印机</strong>：采用电、磁、光、喷墨等物理、化学方法来印刷字符<ul><li>优：速度快、噪声小</li><li>缺：成本高</li></ul></li></ul></li><li>按打印机工作方式不同可分为<ul><li><strong>串行打印机</strong>：逐字打印、速度慢</li><li><strong>行式打印机</strong>：逐行打印、速度快</li></ul></li><li>按工作方式可分为<ul><li><strong>针式打印机</strong>：<ul><li>原理：在联机状态下，主机发出打印命令，经接口、检测和控制电路，间歇驱动纵向送纸和打印头横向移动，同时驱动打印机间歇冲击色带，在纸上打印出所需内容。</li><li>特点：擅长多层复写打印，实现各种票据或蜡纸等的打印。工作原理简单，造价低廉，耗材（色带）便宜，但打印分辨率和打印速度不够高。</li></ul></li><li><strong>喷墨式打印机</strong>：<ul><li>原理：带电的喷墨雾点经过电极偏转后，直接在纸上形成所需字形。彩色喷墨打印机基于三基色原理，即分别喷射 3 种颜色墨滴，按一定的比例混合出所要求的颜色。</li><li>特点：打印噪声小，可实现高质量彩色打印；通常打印速度比针式打印机快；但防水性差，高质量打印需要专用打印纸。</li></ul></li><li><strong>激光打印机</strong>：<ul><li>原理：计算机输出的二进制信息，经过调制后的激光束扫描，在感光鼓上形成潜像，再经过显影、转印和定影，便在纸上得到所需的字符或图像。</li><li>特点：打印质量高、速度快、噪声小、处理能力强；但耗材多、价格较贵、不能复写打印多份，且对纸张的要求高。感光鼓（也称为硒鼓）是激光打印机的核心部件。</li></ul></li></ul></li></ul><h3 id="外部存储器"><a href="#外部存储器" class="headerlink" title="外部存储器"></a>外部存储器</h3><h4 id="磁表面存储器简介"><a href="#磁表面存储器简介" class="headerlink" title="磁表面存储器简介"></a>磁表面存储器简介</h4><ul><li>磁表面存储器，是指把磁性材料薄薄地涂在金属铝或塑料表面上作为载磁体来存储信息。磁盘存储器、磁带存储器和磁鼓存储器均属于磁表面存储器。</li><li>磁表面存储器的优点：<ul><li>存储容量大，位价格低； </li><li>记录介质可以重复使用；</li><li>记录信息可以长期保存而不丢失，甚至可以脱机存档； </li><li>非破坏性读出，读出时不需要再生。</li></ul></li><li>磁表面存储器的缺点：<ul><li>存取速度慢；</li><li>机械结构复杂；</li><li>对工作环境要求较高。</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028122201199.png" alt="image-20211028122201199"></li><li>原理：当磁头和磁性记录介质有相对运动时，通过电磁转换（/切割磁感线）完成读/写操作。</li><li>编码方法：按某种方案，把一串二进制信息变换成存储介质磁层中一个磁化翻转状态的序列。</li><li>磁记录方式：通常采用调频制（FM）和改进型调频制（MFM）的记录方式。</li></ul><h4 id="磁盘设备"><a href="#磁盘设备" class="headerlink" title="磁盘设备"></a>磁盘设备</h4><h5 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h5><ul><li>硬盘存储器<ul><li>硬盘存储器由磁盘驱动器、磁盘控制器和盘片组成。</li><li>磁盘驱动器：核心部件是磁头组件（磁头移动臂）和盘片组件（驱动轴盘片），温彻斯特盘是一种可移动头固定盘片的硬盘存储器。</li><li>磁盘控制器：是硬盘存储器和主机的接口，主流的标准有IDE、SCSI、SATA等。</li></ul></li><li>存储区域：<ul><li><strong>一块硬盘</strong>含有<strong>若干个盘片</strong>；（即有好几个盘）</li><li><strong>每个盘片</strong>可以有 1 或 2 面<strong>盘面</strong>；（即磁盘的正反面都可以利用，每个盘面对应 1 个磁头）</li><li><strong>每个记录面</strong>划分为<strong>若干条磁道</strong>；（<strong>所有记录面上的同一条磁道组成一个柱面</strong>）</li><li>而<strong>每条磁道</strong>又划分为<strong>若干个扇区</strong>；（切蛋糕）</li><li><strong>扇区（也称块）是磁盘读写的最小单位</strong>，也就是说磁盘<strong>按块存取</strong>。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028124407864.png" alt="image-20211028124407864"></li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028124733827.png" alt="image-20211028124733827"></li></ul><h5 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h5><ul><li>磁盘的容量：一个磁盘所能存储的字节总数称为磁盘容量。磁盘容量有非格式化容量和格式化容量之分。<ul><li>非格式化容量：指磁记录表面理论氪用的磁化单元总数。</li><li>格式化容量：指按照某种特定的记录格式后所能利用、存储的信息总量。</li></ul></li><li>记录密度：记录密度是指盘片单位面积上记录的二进制的信息量，通常以道密度、位密度和面密度表示。<ul><li>道密度：沿磁盘半径方向单位长度上的磁道数；</li><li>位密度：一条磁道单位长度上能记录的二进制代码位数；</li><li>面密度：位密度和道密度的乘积。</li><li>注意：磁盘所有磁道记录的信息量一定是相等的，并不是圆越大信息越多，故<strong>越内侧的磁道位密度越大</strong>。</li></ul></li><li>平均存取时间：<ul><li><code>平均存取时间 = 磁盘控制器延迟（可能会有）+ 寻道时间（磁头移动到目的磁道）+ 旋转延迟时间（磁头定位到所在扇区）+ 传输时间（传输数据所花费的时间）</code></li><li>一般寻道时间会在题中给出，旋转延迟时间可以按数学期望（即转半圈）来计算，同时通过转速也可以确定转一个扇区的时间</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028130946542.png" alt="image-20211028130946542" style="zoom:80%;"></li></ul></li><li>数据传输率：磁盘存储器在单位时间内向主机传送数据的字节数，称为数据传输率。<ul><li>假设磁盘转数为 r（转/秒），每条磁道容量为 N 个字节，则数据传输率为 D<del>r</del>=r×N</li></ul></li></ul><h5 id="磁盘地址"><a href="#磁盘地址" class="headerlink" title="磁盘地址"></a>磁盘地址</h5><ul><li>主机向磁盘控制器发送寻址信息，包括<ul><li>驱动器号：一台电脑可能有多个硬盘</li><li>柱面（磁道）号：移动磁头臂（寻道）</li><li>盘面号：激活某个磁头</li><li>扇区号：通过旋转将特定扇区划过磁头下方</li></ul></li><li>例：若系统中有 4 个驱动器，每个驱动器带一个磁盘，每个磁盘 256 个磁道、16 个盘面，每个盘面划分为 16个扇区，则每个扇区地址要 18 位二进制代码<ul><li>驱动器号（2bit）+ 柱面（磁道）号（8bit）+ 盘面号（4bit）+ 扇区号（4bit） </li></ul></li></ul><h5 id="硬盘的工作过程"><a href="#硬盘的工作过程" class="headerlink" title="硬盘的工作过程"></a>硬盘的工作过程</h5><ul><li>硬盘的主要操作是<strong>寻址、读盘、写盘</strong>。每个操作都对应一个控制字，硬盘工作时，第一步是取控制字，第二步是执行控制字。</li><li>硬盘属于机械式部件，其读写操作是串行的，不可能在同一时刻既读又写，也不可能在同一时刻读两组数据或写两组数据。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028150614567.png" alt="image-20211028150614567" style="zoom:80%;"></li></ul><h4 id="磁盘阵列"><a href="#磁盘阵列" class="headerlink" title="磁盘阵列"></a>磁盘阵列</h4><ul><li>RAID（ Redundant Array of Inexpensive Disks，廉价冗余磁盘阵列）是将多个独立的物理磁盘组成一个独立的逻辑盘，数据在多个物理盘上分割交叉存储、并行访问，具有更好的存储性能、可靠性和安全性。</li><li>RAID 有 RAID1～RAID5 的几种方案，无论何时有磁盘损坏，都可以随时拔出受损的磁盘再插入好的磁盘，而数据不会损坏。（raid 数越高，冗余越少、越稳定）<ul><li>RAID0：无冗余和无校验的磁盘阵列。（条带化，提高存取速度，没有容错能力）</li><li>RAID1：镜像磁盘阵列。（镜像磁盘互为备份，容量减少一半）</li><li>RAID2：采用纠错的海明码的磁盘阵列。（开始通过数据校验提高容错能力）</li><li>RAID3：位交叉奇偶校验的磁盘阵列。</li><li>RAID4：块交叉奇偶校验的磁盘阵列。</li><li>RAID5：无独立校验的奇偶校验磁盘阵列。</li></ul></li><li>RAID 通过同时使用多个磁盘，提高传输率；通过在多个磁盘上并行存取来大幅提高存储系统的数据吞吐量；通过镜像功能，提高安全可靠性；通过数据校验，提供容错能力。</li></ul><h4 id="光盘存储器"><a href="#光盘存储器" class="headerlink" title="光盘存储器"></a>光盘存储器</h4><ul><li>利用光学原理读/写信息的存储装置，采用聚焦激光束对盘式介质以非接触的方式记录信息。</li><li>光盘的类型：<ul><li>CD-ROM：只读型光盘，只能读出其中内容，不能写入或修改。</li><li>CD-R：只可写入一次信息，之后不可修改。</li><li>CD-RW：可读可写光盘，可以重复读写。</li><li>DVD-ROM：高容量的 CD-ROM，DVD 表示通用数字化多功能光盘。</li></ul></li></ul><h4 id="固态硬盘"><a href="#固态硬盘" class="headerlink" title="固态硬盘"></a>固态硬盘</h4><ul><li>由 Flash Memory 组成，以及其他硬件和软件的支持。</li><li>闪存（Flash Memory）是在 E2PROM 的基础上发展起来的，<strong>本质上是只读存储器</strong>。</li></ul><h2 id="I-O-接口"><a href="#I-O-接口" class="headerlink" title="I/O 接口"></a>I/O 接口</h2><h3 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h3><ul><li><strong>数据缓冲</strong>：通过<strong>数据缓冲寄存器</strong>（DBR）达到主机和外设工作速度的匹配</li><li><strong>错误或状态监测</strong>：通过<strong>状态寄存器</strong>反馈设备的各种错误、状态信息，供 CPU 查用</li><li><strong>控制和定时</strong>：接收从控制总线发来的控制信号、时钟信号</li><li><strong>数据格式转换</strong>：“串-并”、“并-串”等格式转换</li><li><strong>与主机和设备通信</strong>：实现“主机—I/O接口—I/O设备”之间的通信</li></ul><h3 id="结构与原理"><a href="#结构与原理" class="headerlink" title="结构与原理"></a>结构与原理</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028162350702.png" alt="image-20211028162350702" style="zoom: 67%;"><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028202711609.png" alt="image-20211028202711609" style="zoom:67%;"></li><li><strong>内部接口</strong>：内部接口与系统总线相连，实质上是与内存、CPU 相连。</li><li><strong>外部接口</strong>：外部接口通过接口电缆与外设相连。外部接口的数据传输可能是串行方式，因此 I/O 接口需具有串/并转换功能。</li><li><strong>工作原理</strong>：<ul><li>发命令：发送<strong>命令字</strong>到 I/O 控制寄存器，向设备发送命令（需要驱动程序的协助）</li><li>读状态：从状态寄存器读取<strong>状态字</strong>，获得设备或 I/O 控制器的状态信息</li><li>读/写数据：从数据缓冲寄存器发送或读取数据，完成主机与外设的数据交换</li></ul></li><li>控制寄存器、状态寄存器在<strong>使用时间上错开</strong>，因此有的 I/O 接口中可将二者合二为一</li><li><strong>IO 控制器中的一个个寄存器也称为一个个 I/O 端口</strong></li></ul><h3 id="I-O-端口"><a href="#I-O-端口" class="headerlink" title="I/O 端口"></a>I/O 端口</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028164339128.png" alt="image-20211028164339128" style="zoom: 67%;"><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028163713330.png" alt="image-20211028163713330" style="zoom:80%;"></li><li><strong>统一编址</strong>：靠不同的<strong>地址码</strong>区分内存和 I/O 设备。<strong>访存类的指令都</strong>可以访问 I/O 端口。又称<strong>存储器映射方式</strong>。<ul><li>优点：1. 不需要专门的输入/输出指令，所有访存指令可直接访问端口，程序设计灵活性高；2. 端口有较大的编址空间；3. 读写控制逻辑电路简单</li><li>缺点：1. 端口占用了主存地址空间，使主存地址空间变小；2. 外设寻址时间长（地址位数多，地址译码速度慢）</li></ul></li><li><strong>独立编址</strong>：靠不同的<strong>指令</strong>区分内存和 I/O 设备。<strong>只能用专门的 I/O 指令</strong>可以访问 I/O 端口。<ul><li>优点：1. 使用专用 I/O 指令，程序编制清晰；2. I/O 端口地址位数少，<strong>地址译码速度快</strong>；3. I/O 端口的地址不占用主存地址空间</li><li>缺点：1. I/O 指令类型少，一般只能对端口进行传送操作；2. <strong>程序设计灵活性差</strong>；3. 需要 CPU 提供存储器读/写、I/O 设备读/写两组控制信号，增加了控制逻辑电路的复杂性</li></ul></li></ul><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul><li>按数据传送方式可分为<ul><li><strong>并行接口</strong>：一个字节或一个字所有位同时传送。</li><li><strong>串行接口</strong>：一位一位地传送。</li><li>这里指的是<strong>外设和接口一侧</strong>的传送方式，为了传输到主机和接口一侧，接口要完成数据格式转换。</li></ul></li><li>按主机访问I/O设备的控制方式可分为<ul><li>程序查询接口</li><li>中断接口</li><li>DMA 接口</li></ul></li><li>按功能选择的灵活性可分为<ul><li>可编程接口</li><li>不可编程接口</li></ul></li></ul><h2 id="I-O-方式"><a href="#I-O-方式" class="headerlink" title="I/O 方式"></a>I/O 方式</h2><h3 id="程序查询方式"><a href="#程序查询方式" class="headerlink" title="程序查询方式"></a>程序查询方式</h3><ul><li>优点：接口<strong>设计简单</strong>、设备量少。</li><li>缺点：CPU 在信息传送过程中要<strong>花费很多时间用于查询和等待</strong>，而且在一段时间内只能和一台外设交换信息，效率大大降低。</li><li>例题<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211028211418243.png" alt="image-20211028211418243"></li></ul></li><li><strong>独占查询</strong>：CPU 100%的时间都在查询I/O状态，完全串行</li><li><strong>定时查询</strong>：在保证数据不丢失的情况下，每隔一段时间 CPU 就查询一次 I/O 状态。查询的间隔内 CPU 可以执行其他程序</li></ul><h3 id="中断系统"><a href="#中断系统" class="headerlink" title="中断系统"></a>中断系统</h3><h4 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h4><ol><li>中断请求</li></ol><ul><li>中断源向 CPU 发送中断请求信号。</li></ul><ol start="2"><li>中断响应</li></ol><ul><li><strong>中断请求标记</strong>：<ul><li>为了区分不同的中断源，中断系统需对每个中断源设置<strong>中断请求标记触发器 INTR</strong>，当其状态为“1”时，表示中断源有请求。</li><li>这些触发器可组成<strong>中断请求标记寄存器</strong>，该寄存器可集中在CPU中，也可分散在各个中断源中。</li><li>对于<strong>外中断</strong>，CPU <strong>响应中断的时间</strong>是在每条<strong>指令执行阶段的结束前</strong>。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029131814925.png" alt="image-20211029131814925" style="zoom:80%;"></li></ul></li><li><strong>响应中断的条件</strong>：<ol><li>中断源有中断请求。</li><li>一条指令执行完毕，且没有更紧迫的任务。</li><li>允许中断：当前 PSW 的 IF（Interrupt Flag）为真<ul><li>IF=1 表示开中断（允许中断）；IF=0 表示关中断（不允许中断）</li><li>关中断的作用：实现<strong>原子操作</strong>（必须一次性做完的操作）</li><li>除非是不得不响应的非屏蔽中断（如掉电）</li></ul></li></ol></li><li><strong>中断判优</strong>：多个中断源同时提出请求时通过中断判优逻辑响应一个中断源。<ul><li>中断判优既可以用硬件实现，也可用软件实现：<ul><li>硬件实现是通过<strong>硬件排队器</strong>实现的，它既可以设置在 CPU 中，也可以分散在各个中断源中；</li><li>软件实现是通过<strong>查询程序</strong>实现的。（相比硬件更慢，通常使用硬件排队器）</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029132629325.png" alt="image-20211029132629325"></li><li><strong>优先级设置</strong>：<ol><li>硬件故障中断属于最高级，其次是软件中断；</li><li>非屏蔽中断优于可屏蔽中断；</li><li>DMA 请求优于 I/O 设备传送的中断请求</li><li>高速设备优于低速设备；</li><li>输入设备优于输出设备；</li><li>实时设备优于普通设备。</li></ol></li></ul></li></ul><ol start="3"><li>中断处理</li></ol><ul><li><strong>中断隐指令</strong>：CPU 在检测到中断请求时自动完成的一系列动作<ul><li><strong>关中断</strong>。为了在保护现场期间不被新的中断打断，必须关中断，从而保证被中断的程序在中断服务程序执行完毕之后能接着正确地执行下去。</li><li><strong>保存断点</strong>。为了保证在中断服务程序执行完毕后能正确地返回到原来的程序，必须将原来程序的断点（即程序计数器（PC）的内容）保存起来。可以存入堆栈或指定单元。</li><li><strong>引出中断服务程序</strong>。引出中断服务程序的实质就是取出中断服务程序的入口地址并传送给程序计数器（PC）。<ul><li>软件向量法</li><li>硬件向量法<ul><li>由<strong>硬件</strong>产生指向<strong>中断向量</strong>的<strong>向量地址</strong></li><li><strong>中断向量</strong>指向<strong>中断服务程序</strong>的<strong>入口地址</strong></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029204003689.png" alt="image-20211029204003689" style="zoom:80%;"></li></ul></li></ul></li><li>以及其他任务……</li></ul></li><li><strong>中断服务程序</strong>。<ul><li><strong>保护现场</strong>：保存通用寄存器和状态寄存器的内容（eg：保存ACC寄存器的值），以便返回原程序后可以恢复 CPU 环境。可使用堆栈，也可以使用特定存储单元。</li><li><strong>中断服务</strong>（设备服务）：主体部分，如通过程序控制需打印的字符代码送入打印机的缓冲存储器中（eg：中断服务的过程中有可能修改 ACC 寄存器的值）</li><li><strong>恢复现场</strong>：通过出栈指令或取数指令把之前保存的信息送回寄存器中（eg：把原程序算到一般的 ACC 值恢复原样）</li><li><strong>中断返回</strong>：通过中断返回指令回到原程序断点处。</li></ul></li></ul><h4 id="多重中断"><a href="#多重中断" class="headerlink" title="多重中断"></a>多重中断</h4><ul><li><p><strong>单重中断</strong>：执行中断服务程序时不响应新的中断请求。</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029205105146.png" alt="image-20211029205105146" style="zoom: 67%;"></li></ul></li><li><p><strong>多重中断</strong>：又称<strong>中断嵌套</strong>，执行中断服务程序时可响应新的中断请求。</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029210138245.png" alt="image-20211029210138245" style="zoom:67%;"><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029205642997.png" alt="image-20211029205642997"></li></ul></li><li><p>中断屏蔽技术：</p><ul><li><p>CPU 要具备多重中断的功能，须满足下列条件（主要用于多重中断）</p><ul><li>在中断服务程序中提前设置开中断指令。</li><li>优先级别高的中断源有权中断优先级别低的中断源。</li></ul></li><li><p>屏蔽字设置的规律：</p><ol><li><p>一般用“1”表示屏蔽，“0”表示正常申请。</p></li><li><p>每个中断源对应一个屏蔽字（在处理该中断源的中断服务程序时，屏蔽寄存器中的内容为该中断源对应的屏蔽字）。</p></li><li><p>屏蔽字中“1”越多，优先级越高。每个屏蔽字中至少有一个“1”（至少要能屏蔽自身的中断）。</p></li></ol></li><li><p>例题：</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029221003756.png" alt="image-20211029221003756"></li></ul></li></ul></li></ul><h3 id="程序中断方式"><a href="#程序中断方式" class="headerlink" title="程序中断方式"></a>程序中断方式</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029220829035.png" alt="image-20211029220829035"></li><li>例题：<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029221803157.png" alt="image-20211029221803157"></li></ul></li></ul><h3 id="DMA-方式"><a href="#DMA-方式" class="headerlink" title="DMA 方式"></a>DMA 方式</h3><h4 id="DMA-传送过程"><a href="#DMA-传送过程" class="headerlink" title="DMA 传送过程"></a>DMA 传送过程</h4><ul><li><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211029232153701.png" alt="image-20211029232153701"></p></li><li><p>DMA 控制器工作流程：</p><ul><li>CPU 向 DMA 控制器指明要<strong>输入还是输出</strong>；要传送<strong>多少数据</strong>；数据在<strong>主存、外设中的地址</strong>。</li><li>接受外设发出的 DMA 请求（外设传送一个字的请求），并向 CPU 发出<strong>总线请求</strong>。</li><li>CPU 响应此总线请求，发出总线响应信号，接管总线控制权，进入 DMA 操作周期。</li><li>确定传送数据的<strong>主存单元地址及长度</strong>，并能自动修改<strong>主存地址计数</strong>和<strong>传送长度计数</strong>。</li><li>规定数据在主存和外设间的<strong>传送方向</strong>，发出读写等控制信号，执行数据传送操作。</li><li>向 CPU 报告 DMA 操作的结束。</li></ul></li><li><p>DMA 组成：</p><ul><li>主存地址计数器：简称 AR，存放要交换数据的主存地址。</li><li>传送长度计数器：简称 WC，用来记录传送数据的长度，计数溢出时，数据即传送完毕，自动发中断请求信号。</li><li>数据缓冲寄存器：用于暂存每次传送的数据。</li><li>DMA 请求触发器：每当 I/O 设备准备好数据后给出一个控制信号，使 DMA 请求触发器置位。</li><li>控制/状态逻辑：由控制和时序电路及状态标志组成，用于指定传送方向，修改传送参数，并对DMA请求信号和CPU响应信号进行协调和同步。</li><li>中断机构：当一个数据块传送完毕后触发中断机构，向CPU提出中断请求。</li></ul></li><li><p>注：在 DMA 传送过程中，DMA 控制器将接管 CPU 的地址总线、数据总线和控制总线，CPU 的主存控制信号被禁止使用。而当 DMA 传送结束后，将恢复 CPU 的一切权利并开始执行其操作。</p></li><li><p>CPU 和 DMA 控制器访问主存可能产生冲突</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211030223325881.png" alt="image-20211030223325881"></li></ul></li></ul><h4 id="DMA-传送方式"><a href="#DMA-传送方式" class="headerlink" title="DMA 传送方式"></a>DMA 传送方式</h4><ul><li>主存和 DMA 控制器之间有一条数据通路，因此主存和 I/O 设备之间交换信息时，不通过 CPU。但当 I/O 设备和 CPU 同时访问主存时，可能发生冲突，为了有效地使用主存，DMA 控制器与 CPU 通常采用以下 3 种方法使用主存。<ol><li>停止 CPU 访问主存<ul><li>优点：控制简单</li><li>缺点：CPU 处于不工作状态或保持状态未充分发挥 CPU 对主存的利用率</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211031092120253.png" alt="image-20211031092120253"></li></ul></li><li>DMA 与 CPU 交替访存<ul><li>一个 CPU 周期，分为 C1 和 C2 两个周期。C1 专供 DMA 访存，C2 专供 CPU 访存</li><li>优点：不需要总线使用权的申请、建立和归还过程</li><li>缺点：硬件逻辑更为复杂</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211031092135200.png" alt="image-20211031092135200"></li></ul></li><li>周期挪用（周期窃取）<ul><li>周期指<strong>存取周期</strong>。</li><li>DMA 访问主存有三种可能：<ul><li>CPU 此时不访存（不冲突）</li><li>CPU 正在访存（存取周期结束让出总线）</li><li>CPU 与 DMA 同时请求访存（I/O 访存优先）</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part7/image-20211031092208256.png" alt="image-20211031092208256"></li></ul></li></ol></li></ul><h4 id="DMA-方式的特点"><a href="#DMA-方式的特点" class="headerlink" title="DMA 方式的特点"></a>DMA 方式的特点</h4><ul><li>主存和 DMA 接口之间有一条直接数据通路。 由于 DMA 方式传送数据不需要经过 CPU，因此不必中断现行程序，<strong>I/O 与主机并行工作，程序和传送并行工作</strong>。</li><li>DMA 方式具有下列特点：<ul><li>它使主存与 CPU 的固定联系脱钩，主存既可被 CPU 访问，又可被外设访问。 </li><li>在数据块传送时，主存地址的确定、传送数据的计数等都由硬件电路直接实现。</li><li>主存中要开辟专用缓冲区，及时供给和接收外设的数据。</li><li>DMA 传送速度快，CPU 和外设并行工作，提高了系统效率。</li><li>DMA 在传送开始前要通过程序进行预处理，结束后要通过中断方式进行后处理。</li></ul></li></ul><table><thead><tr><th></th><th>中断</th><th>DMA</th></tr></thead><tbody><tr><td>数据传输</td><td>程序控制</td><td>硬件控制</td></tr><tr><td>中断请求</td><td>每次传送数据</td><td>只有后处理</td></tr><tr><td>响应时间</td><td>指令周期结束</td><td>总线空闲即可</td></tr><tr><td>场景</td><td>CPU 控制低速设备</td><td>MDA 控制器控制高速设备</td></tr><tr><td>优先级</td><td>低于 DMA</td><td>高于中断</td></tr><tr><td>异常处理</td><td>能处理异常事件</td><td>仅传送数据</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">IO 系统</summary>
    
    
    
    <category term="计算机组成原理" scheme="https://nephrencake.gitee.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机组成原理-Part6</title>
    <link href="https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/"/>
    <id>https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/</id>
    <published>2021-10-02T02:53:23.000Z</published>
    <updated>2021-10-30T13:47:30.115Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算机组成原理-Part6——总线"><a href="#计算机组成原理-Part6——总线" class="headerlink" title="计算机组成原理-Part6——总线"></a>计算机组成原理-Part6——总线</h1><p>[TOC]</p><h2 id="总线概述"><a href="#总线概述" class="headerlink" title="总线概述"></a>总线概述</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211025162259335.png" alt="image-20211025162259335"></li><li><strong>总线</strong>是一组能为多个部件<strong>分时共享</strong>的<strong>公共</strong>信息传送<strong>线路</strong>。</li><li>同一时刻只能有<strong>一个设备控制</strong>总线传输操作，可以有<strong>一个或多个设备</strong>从总线<strong>接收</strong>数据。</li><li>特点：<ul><li>共享：指总线上可以挂接多个部件，各个部件之间互相交换的信息都可以通过这组线路分时共享。</li><li>分时：指同一时刻只允许有一个部件向总线发送信息，如果系统中有多个部件，则它们只能分时地向总线发送信息。</li></ul></li><li>特性：<ul><li>机械特性：尺寸、形状、管脚数、排列顺序</li><li>电气特性：传输方向（数据双向、地址/控制单向）和有效的电平范围（高电平、低电平的定义范围）</li><li>功能特性：每根传输线的功能（地址、数据、控制）</li><li>时间特性：信号的时序关系</li></ul></li><li><strong>数据通路</strong>描述的是数据的路径，是逻辑上的概念。<strong>数据总线</strong>是承载数据的物理载体，<strong>地址总线</strong>一样可以成为数据通路的一部分。</li></ul><h3 id="总线分类（重点）"><a href="#总线分类（重点）" class="headerlink" title="总线分类（重点）"></a>总线分类（重点）</h3><ul><li>按数据传输格式：<ul><li>串行总线<ul><li>优点：1. 只需要一条传输线，成本低廉，广泛应用于长距离传输；2. 不容易发生跳变；3. 应用于计算机内部时，可以节省布线空间。</li><li>缺点：1. 在数据发送和接收的时候要进行拆卸和装配，要考虑串行-并行转换的问题。</li></ul></li><li>并行总线<ul><li>优点：1. 总线的逻辑时序比较简单，电路实现起来比较容易。</li><li>缺点：1. 信号线数量多，占用更多的布线空间；2. 远距离传输成本高昂；3. 由于工作频率较高时，并行的信号线之间会产生严重干扰，对每条线等长的要求也越高，所以无法持续提升工作频率。</li></ul></li><li>并行总线传输速度一定比串行总线快，是错误的。</li></ul></li><li>按总线功能（连接的部件）：<ul><li>片内总线<ul><li>片内总线是芯片内部的总线。</li><li>是 CPU 芯片内部<strong>寄存器与寄存器</strong>之间、<strong>寄存器与 ALU</strong> 之间的公共连接线。</li></ul></li><li>系统总线<ul><li>系统总线是计算机系统内各功能部件（CPU、主存、I/O接口）之间相互连接的总线。</li><li>按系统总线<strong>传输信息内容</strong>的不同，又可分为3类：<strong>数据总线</strong>（Data Bus）、<strong>地址总线</strong>（Address Bus）和<strong>控制总线</strong>（Control Bus）。</li><li>数据总线（双向）：用来传输各功能部件之间的数据信息，其位数与<strong>机器字长、存储字长有关</strong>。</li><li>地址总线（单向）：用来指出数据总线上的源数据或目的数据所在的主存单元或 I/O 端口的地址，地址总线的位数与<strong>主存地址空间大小</strong>和<strong>设备数量</strong>有关。</li><li>控制总线（单根单向，整体双向）：传输的是控制信息，包括 <strong>CPU 送出的控制命令</strong>和<strong>主存（或外设）返回 CPU 的反馈信号</strong>。</li></ul></li><li>通信总线<ul><li>通信总线是用于计算机系统之间或计算机系统与其他系统（如远程通信设备、测试设备）之间信息传送的总线，通信总线也称为外部总线。</li></ul></li></ul></li><li>按时序控制方式：<ul><li>同步总线</li><li>异步总线</li></ul></li></ul><h3 id="系统总线的结构（重点）"><a href="#系统总线的结构（重点）" class="headerlink" title="系统总线的结构（重点）"></a>系统总线的结构（重点）</h3><ul><li>单总线结构<ul><li>结构：CPU、主存、I/O设备（通过I/O接口）都连接在<strong>一组</strong>总线（而不是只有一根信号线）上，允许 I/O 设备之间、I/O 设备和 CPU 之间或 I/O 设备与主存之间直接交换信息。</li><li>优点：结构简单，成本低，易于接入新的设备。</li><li>缺点：带宽低、负载重，多个部件只能争用唯一的总线，且不支持并行传送操作。CPU、IO 的速度差异导致总线性能并不能完全发挥。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211025190248221.png" alt="image-20211025190248221" style="zoom:80%;"></li></ul></li><li>双总线结构<ul><li>结构：双总线结构有两条总线，一条是主存总线，用于 CPU、主存和通道之间进行数据传送；另一条是 I/O 总线，用于多个外部设备与通道之间进行数据传送。</li><li>优点：将较低速的 I/O 设备从单总线上分离出来，实现存储器总线和 I/O 总线分离。</li><li>缺点：需要增加通道等硬件设备。</li><li>主存总线支持突发（猝发）传送：<strong>只需要送出一个首地址，就可以读写多个地址连续（超出一次数据总线的传输能力）的数据，而不需要每次都发送子地址</strong>。</li><li>通道是具有特殊功能的处理器，能对 I/O 设备进行统一管理。 通道程序放在主存中。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211025190342462.png" alt="image-20211025190342462" style="zoom:80%;"></li></ul></li><li>三总线结构<ul><li>结构：三总线结构是在计算机系统各部件之间采用 3 条各自独立的总线来构成信息通路，这3条总线分别为主存总线、I/O 总线和直接内存访问 DMA 总线。</li><li>优点：<strong>提高了 I/O 设备的性能</strong>，使其更快地响应命令，提高系统吞吐量。</li><li>缺点：系统工作效率较低。（三条总线同时刻只有一个能工作）</li><li>DMA：Direct Memory Access，直接内存访问。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211025190627701.png" alt="image-20211025190627701" style="zoom:80%;"></li></ul></li><li>四总线结构<ul><li>桥接器：用于连接不同的总线，具有数据缓冲、转换和控制功能。</li><li>靠近 CPU 的总线速度较快。</li><li>每级总线的设计遵循总线标准。</li></ul></li></ul><h3 id="总线的性能指标"><a href="#总线的性能指标" class="headerlink" title="总线的性能指标"></a>总线的性能指标</h3><ol><li><p>总线的传输周期（总线周期）</p><ul><li><strong>一次总线操作所需的时间</strong>（包括申请阶段、 寻址阶段、传输阶段和结束阶段），通常由若干个总线时钟周期构成。</li><li>总线周期与总线时钟周期的对应关系要具体按照题意，有可能会有多对一、一对一、一对多。</li></ul></li><li><p>总线时钟周期</p><ul><li>即<strong>机器的时钟周期</strong>。总线也要受 CPU 时钟的控制。</li><li>现在的计算机中，总线时钟周期也有可能由桥接器提供</li></ul></li><li><p>总线的工作频率</p><ul><li>总线上各种操作的频率，为<strong>总线周期的倒数</strong>。 </li><li>若总线周期=N个时钟周期，则总线的工作频率=时钟频率/N。 </li><li>实际上指<strong>一秒内传送几次数据</strong>。</li></ul></li><li><p>总线的时钟频率</p><ul><li>即机器的时钟频率，为<strong>时钟周期的倒数</strong>。 </li><li>若时钟周期为T，则时钟频率为1/T。 </li><li>实际上指<strong>一秒内有多少个时钟周期</strong>。</li></ul></li><li><p>总线宽度</p><ul><li>又称为<strong>总线位宽</strong>，它是总线上<strong>同时能够传输的数据位数</strong>，</li><li>一般说总线宽度，通常是指<strong>数据总线的根数</strong>，如 32 根数据信号线称为 32 位（bit）总线。</li></ul></li><li><p>总线带宽</p><ul><li>可理解为总线的<strong>数据传输速率</strong>，即<strong>单位时间内总线上可传输数据的位数</strong>，通常用每秒钟传送信息的字节数来衡量，单位可用字节/秒（B/s）表示。</li><li>总线带宽 = 总线工作频率 × 总线宽度（bit/s）= 总线工作频率 × (总线宽度/8)（B/s）= 总线宽度/总线周期（bit/s）= (总线宽度/8)/总线周期（B/s）</li><li>总线带宽是指总线本身所能达到的<strong>最高传输速率</strong>。在计算实际的<strong>有效数据传输率</strong>时，要用实际传输的数据量（<strong>去除冗余校验位之后</strong>）除以耗时。</li><li>注意，串行总线和并行总线之间的比较就需要用到：总线带宽 = 总线工作频率 × 总线宽度</li></ul></li><li><p>总线复用</p><ul><li><p>总线复用是指<strong>一种信号线在不同的时间传输不同的信息</strong>。</p><p>可以使用<strong>较少的线</strong>传输更多的信息，从而节省了空间和成本。</p></li></ul></li><li><p>信号线数</p><ul><li>地址总线、数据总线和控制总线 3 种总线数的总和称为信号线数。</li><li><strong>只传输一位 bit 数据的先叫做信号线</strong></li></ul></li></ol><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211026092049020.png" alt="image-20211026092049020"></li></ul><h2 id="总线仲裁"><a href="#总线仲裁" class="headerlink" title="总线仲裁"></a>总线仲裁</h2><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li>针对问题：总线作为一种共享设备，不可避免地会出现同一时刻有多个主设备竞争总线控制权的问题。</li><li><strong>主设备：</strong>获得总线控制权的设备。</li><li><strong>从设备：</strong>被主设备访问的设备，只能响应从主设备发来的各种总线命令。</li></ul><h3 id="总线仲裁分类"><a href="#总线仲裁分类" class="headerlink" title="总线仲裁分类"></a>总线仲裁分类</h3><h4 id="集中仲裁方式"><a href="#集中仲裁方式" class="headerlink" title="集中仲裁方式"></a>集中仲裁方式</h4><ul><li><p>主要工作流程：</p><ul><li>主设备发出请求信号；</li><li>若多个主设备同时要使用总线，则由总线控制器的判优、仲裁逻辑按一定的优先等级顺序确定哪个主设备能使用总线；</li><li>获得总线使用权的主设备开始传送数据。</li></ul></li><li><p>链式查询方式</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211026094129115.png" alt="image-20211026094129115"></li><li>BS、BR、BG 都是一根信号线</li><li>优先级：<ul><li>离总线控制器越近的部件，其优先级越高；</li><li>离总线控制器越远的部件，其优先级越低。</li></ul></li><li>优点：链式查询方式优先级固定。只需很少几根控制线就能按一定优先次序实现总线控制，结构简单，扩充容易。</li><li>缺点：对硬件电路的故障敏感，并且优先级不能改变。当优先级高的部件频繁请求使用总线时，会使<strong>优先级较低的部件长期不能使用总线</strong>。</li></ul></li><li><p>计数器定时查询方式</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211026111543103.png" alt="image-20211026111543103" style="zoom:80%;"></li><li>结构特点：用一个计数器控制总线使用权，相对链式查询方式多了一组设备地址线（设备地址线是一组线），少了一根总线响应线 BG；它仍共用一根总线请求线。</li><li>优点：<ul><li>计数初始值可以改变优先次序</li><li>对电路的故障没有链式敏感</li></ul></li><li>缺点：<ul><li>增加了控制线数。若设备有 n 个，则需 [log2n]+2 条控制线</li><li>控制相对比链式查询相对复杂</li></ul></li></ul></li><li><p>独立请求方式</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211026112051799.png" alt="image-20211026112051799" style="zoom:80%;"></li><li>结构特点：每一个设备均有一对总线请求线 BRi 和总线允许线 BGi。</li><li>优点：<ul><li>响应速度快，总线允许信号 BG 直接从控制器发送到有关设备，不必在设备间传递或者查询。</li><li>对优先次序的控制相当灵活。</li></ul></li><li>缺点：<ul><li>控制线数量多。若设备有n个，则需要 2n+1 条控制线。</li><li>总线的控制逻辑更加复杂</li></ul></li></ul></li></ul><blockquote><p>“总线忙”信号的建立者是<strong>获得总线控制权的设备</strong>，而不是仲裁器发出。</p></blockquote><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211026112251788.png" alt="image-20211026112251788"></p><h4 id="分布仲裁方式"><a href="#分布仲裁方式" class="headerlink" title="分布仲裁方式"></a>分布仲裁方式</h4><ul><li>特点：不需要中央仲裁器，每个潜在的主模块都有自己的仲裁器和仲裁号，多个仲裁器竞争使用总线。</li><li>步骤<ul><li>当设备有总线请求时，它们就<strong>把各自唯一的仲裁号发送到共享的仲裁总线上</strong>；</li><li><strong>每个仲裁器</strong>将从仲裁<strong>总线上得到的仲裁号</strong>与自己的仲裁号进行<strong>比较</strong>；</li><li>如果仲裁总线上的号优先级高，则它的总线请求不予响应，并撤销它的仲裁号；</li><li>最后，获胜者的仲裁号保留在仲裁总线上。</li></ul></li></ul><h2 id="总线操作和定时"><a href="#总线操作和定时" class="headerlink" title="总线操作和定时"></a>总线操作和定时</h2><h3 id="基本概念-2"><a href="#基本概念-2" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li>总线周期的四个阶段<ul><li>申请分配阶段：由需要使用总线的主模块（或主设备）提出申请，经总线仲裁机构决定将下一传输周期的总线使用权。也可将此阶段细分为<strong>传输请求</strong>和<strong>总线仲裁</strong>两个阶段。</li><li>寻址阶段：获得使用权的主模块通过总线发出本次要访问的从模块的地址及有关命令，<strong>启动参与本次传输的从模块</strong>。</li><li>传输阶段：主模块和从模块进行<strong>数据交换</strong>，可单向或双向进行数据传送。</li><li>结束阶段：主模块的<strong>有关信息</strong>均从系统总线上<strong>撤除</strong>，让出总线使用权。</li></ul></li><li>针对问题：占用总线的一对设备如何进行数据传输</li><li><strong>总线定时</strong>，指总线在双方交换数据的过程中需要时间上配合关系的控制，实质是一种协议或规则<ul><li>同步通信(同步定时方式)：由<strong>统一时钟</strong>控制数据传送</li><li>异步通信(异步定时方式)：采用<strong>应答方式</strong>，没有公共时钟标准</li><li>半同步通信：同步、异步结合</li><li>分离式通信：充分挖掘系统总线每瞬间的潜力</li></ul></li></ul><h3 id="同步定时方式"><a href="#同步定时方式" class="headerlink" title="同步定时方式"></a>同步定时方式</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211026141316738.png" alt="image-20211026141316738"></li><li>同步定时方式：总线控制器采用一个<strong>统一的时钟信号</strong>来协调发送和接收双方的传送定时关系。</li><li>假设：CPU 作为主设备，选择某个输入设备作为从设备<ol><li>CPU 在 T1 时刻的上升沿给出地址信息</li><li>在T2的上升沿给出读命令（低电平有效），与地址信息相符合的输入设备按命令进行一系列的内部操作，且必须在T3的上升沿来之前将CPU所需的数据送到数据总线上。</li><li>CPU在T3时钟周期内，将数据线上的信息传送到其内部寄存器中。</li><li>CPU在T4的上升沿撤销读命令，输入设备不再向数据总线上传送数据，撤销它对数据总线的驱动。</li></ol></li><li>优点：<strong>传送速度快</strong>，具有较高的传输速率；<strong>总线控制逻辑简单</strong>。</li><li>缺点：主从设备属于强制性同步；不能及时进行数据通信的有效性检验，<strong>可靠性较差</strong>。</li><li>同步通信适用于<strong>总线长度较短</strong>（总线长度短，传输更稳定）及总线<strong>所接部件的存取时间比较接近</strong>的系统。</li></ul><h3 id="异步定时方式"><a href="#异步定时方式" class="headerlink" title="异步定时方式"></a>异步定时方式</h3><ul><li>在异步定时方式中，没有统一的时钟，也没有固定的时间间隔，完全依靠传送双方相互制约的“握手”信号来实现定时控制。</li><li>主设备提出交换信息的“请求”信号（指明操作与地址），经接口传送到从设备；从设备接到主设备的请求后，通过接口向主设备发出“回答”信号。</li><li>根据“请求”和“回答”信号的撤销是否互锁，分为以下 3 种类型。<ul><li><strong>不互锁方式</strong>：速度最快，可靠性最差<ul><li>主设备发出“请求”信号后，<strong>不必等</strong>到接到从设备的“回答”信号，而是经过一段时间，便撤销“请求”信号。</li><li>而从设备在接到“请求”信号后，发出“回答”信号，并经过一段时间，<strong>自动撤销</strong>“回答”信号。双方不存在互锁关系。</li></ul></li><li><strong>半互锁方式</strong>：<ul><li>主设备发出“请求”信号后，<strong>必须等</strong>接到从设备的“回答”信号后，才撤销“请求”信号，有互锁的关系。</li><li>而从设备在接到“请求”信号后，发出“回答”信号，但不必等待获知主设备的“请求”信号已经撤销，而是隔一段时间后<strong>自动撤销</strong>“回答”信号，不存在互锁关系。</li></ul></li><li><strong>全互锁方式</strong>：最可靠，速度最慢<ul><li>主设备发出“请求”信号后，<strong>必须等</strong>从设备“回答”后，才撤销“请求”信号；</li><li>从设备发出“回答”信号，<strong>必须等</strong>获知主设备“请求”信号已撤销后，再撤销其“回答”信号。双方存在互锁关系。</li></ul></li></ul></li><li>优点：总线周期长度可变，能保证两个工作速度相差很大的部件或设备之间<strong>可靠</strong>地进行信息交换，自动适应时间的配合。</li><li>缺点：比同步控制方式稍复杂一些，速度比同步定时方式慢。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211026144100498.png" alt="image-20211026144100498"></li></ul><h3 id="半同步通信（仅了解）"><a href="#半同步通信（仅了解）" class="headerlink" title="半同步通信（仅了解）"></a>半同步通信（仅了解）</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part6/image-20211026144310741.png" alt="image-20211026144310741"></li><li>半同步通信：统一时钟的基础上，增加一个“等待”响应信号 WAIT</li><li>当从设备跟不上主设备速度时，从设备通过 WAIT 信号线要求主设备等待</li></ul><h3 id="分离式通信（仅了解）"><a href="#分离式通信（仅了解）" class="headerlink" title="分离式通信（仅了解）"></a>分离式通信（仅了解）</h3><ul><li>上述三种通信的共同点<ul><li>主模块发地址 、命令 —— 使用总线</li><li>从模块准备数据 —— 不使用总线，<strong>总线空闲</strong></li><li>从模块向主模块发数据 —— 使用总线</li></ul></li><li>分离式通信的一个总线传输周期<ul><li>子周期 1：主模块申请占用总线，使用完后放弃总线的使用权</li><li>子周期 2：从模块申请占用总线，将各种信息送至总线上</li></ul></li><li>特点：<ul><li>各模块均有权申请占用总线</li><li>采用同步方式通信，不等对方回答</li><li>各模块准备数据时，不占用总线</li><li>总线利用率提高</li></ul></li></ul><h2 id="总线标准"><a href="#总线标准" class="headerlink" title="总线标准"></a>总线标准</h2><h3 id="基本概念-3"><a href="#基本概念-3" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li>总线标准是国际上公布或推荐的互连各个模块的标准，是把各种不同的模块组成计算机系统时必须遵守的规范。</li><li><strong>系统总线</strong>：<strong>通常与 CPU 直接相连</strong>，用于连接 CPU 与北桥芯片、或 CPU 与主存等</li><li><strong>局部总线</strong>：<strong>没有直接与 CPU 连接</strong>，通常是连接高速的<strong>北桥芯片</strong>，用于连接了很多重要的硬件部件（如显卡、声卡等）</li><li><strong>设备总线</strong>、<strong>通信总线</strong>：通常由<strong>南桥芯片</strong>控制，用于连接计算机与计算机，或连接计算机与外部 I/O 设备</li></ul><h3 id="总线标准的发展"><a href="#总线标准的发展" class="headerlink" title="总线标准的发展"></a>总线标准的发展</h3><ul><li>系统总线<ul><li>ISA（Industry Standard Architecture）：<strong>并行总线</strong>，1984 年提出</li><li>EISA（Extended ISA）：<strong>并行总线</strong>，1988 年提出，在 ISA 基础上增加位宽</li><li>FBS、QPI：<strong>串行总线</strong>，Intel 提出的两种系统总线，用于连接 CPU 和北桥芯片；QPI 又称 multi-FSB</li></ul></li><li>局部总线<ul><li>VESA（Video Electronics Standard Architecture）：<strong>并行总线</strong>，1991 年提出，用于传输图像，但渐渐赶不上 CPU 的发展</li><li>PCI（Peripheral Component Interconnect）：<strong>并行总线</strong>，1992 年提出，速度和 VESA 差不多，但工作频率可以独立于 CPU 主频，用于连接显卡、声卡、网卡等，支持即插即用</li><li>AGP（Accelerated Graphics Port）：<strong>并行总线</strong>，1996 年提出，从 PCI2.1 基础上扩展而来，用于连接显存和主存</li><li>PCI-E（PCI-Express（3GIO））：<strong>串行总线</strong>，2001 年提出，工作频率很高，支持全双工通信</li></ul></li><li>设备/通信总线<ul><li>连接各种外设<ul><li>RS-232C（Recommended Standard）：<strong>串行总线</strong>，1970 年提出，用于极慢速的电传打印机</li><li>SCSI（Small Computer System Interface）：<strong>并行总线</strong>，1986 年提出，用于连接硬盘、打印机、扫描仪等</li><li>PCMCIA（Personal Computer Memory Card International Association）：<strong>并行总线</strong>，1991 年提出，用于连接外部存储卡，目的是增强个人电脑的信息互换</li><li>USB（Universal Serial Bus）：<strong>串行总线</strong>，1996 年提出，采用差模信号，每次只能传输 1 bit，工作频率可以很高</li></ul></li><li>连接硬盘<ul><li>IDE（Integrated Drive Electronics）：<strong>并行总线</strong>，19986 年提出，又称 Parallel ATA，主要用于连接硬盘、光驱等</li><li>SATA（Serial Advanced Technology Attachment）：<strong>串行总线</strong>，2001 年提出，Serial ATA，主要用于连接硬盘、光驱等</li></ul></li></ul></li></ul><h3 id="串行总线取代并行总线的原因"><a href="#串行总线取代并行总线的原因" class="headerlink" title="串行总线取代并行总线的原因"></a>串行总线取代并行总线的原因</h3><ul><li>并行总线：<ul><li>用 m 根线每次传送 m 个比特，用高/低电平表示 1/0，通常采用同步定时方式。</li><li>由于线间信号干扰，因此总线工作频率不能太高。</li><li>另外，各条线不能有长度差，长距离并行传输时工艺难度大。</li></ul></li><li>串行总线：<ul><li>用两根线每次传送一个比特，采用“差模信号”表示 1/0，通常采用异步定时方式，总线工作频率可以很高。</li><li>现在的串行总线通常基于包传输，如 80bit 为一个数据包，包与包之间有先后关系，因此可以用多个数据通路分别串行传输多个数据包，某种程度上现在的串行总线也有“并行”的特点</li></ul></li></ul>]]></content>
    
    
    <summary type="html">总线</summary>
    
    
    
    <category term="计算机组成原理" scheme="https://nephrencake.gitee.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机组成原理-Part5</title>
    <link href="https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/"/>
    <id>https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/</id>
    <published>2021-10-02T02:53:18.000Z</published>
    <updated>2021-10-26T15:16:15.323Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算机组成原理-Part5——中央处理器"><a href="#计算机组成原理-Part5——中央处理器" class="headerlink" title="计算机组成原理-Part5——中央处理器"></a>计算机组成原理-Part5——中央处理器</h1><p>[TOC]</p><h2 id="CPU-的功能和结构"><a href="#CPU-的功能和结构" class="headerlink" title="CPU 的功能和结构"></a>CPU 的功能和结构</h2><h3 id="CPU-的功能"><a href="#CPU-的功能" class="headerlink" title="CPU 的功能"></a>CPU 的功能</h3><ol><li><strong>指令控制</strong>：完成<strong>取指令</strong>、<strong>分析指令</strong>和<strong>执行指令</strong>的操作，即程序的顺序控制。</li><li><strong>操作控制</strong>：CPU 产生并管理一条指令中的若干操作信号，从而<strong>控制相应的部件</strong>按指令要求进行动作。</li><li><strong>时间控制</strong>：为<strong>每条指令按时间顺序</strong>提供应有的控制信号。</li><li><strong>数据加工</strong>：对数据进行<strong>算术和逻辑运算</strong>。</li><li><strong>中断处理</strong>：管理<strong>总线</strong>及<strong>输入输出</strong>；处理<strong>异常情况</strong>和<strong>特殊请求</strong>。</li></ol><ul><li>运算器：<strong>数据加工</strong></li><li>控制器：<strong>操作控制</strong>、<strong>时间控制</strong>、<strong>指令控制</strong>、<strong>中断处理</strong></li></ul><h3 id="运算器的基本结构"><a href="#运算器的基本结构" class="headerlink" title="运算器的基本结构"></a>运算器的基本结构</h3><ul><li>运算器结构：<ul><li>算术逻辑单元：进行算术/逻辑运算。</li><li>通用寄存器组：如 AX、BX、CX、DX、SP 等。用于存放操作数（包括源操作数、目的操作数及中间结果）和各种地址信息等。</li><li>暂存寄存器：用于暂存<strong>从主存读来的数据</strong>，这个数据不能立即存放在通用寄存器中，否则会破坏其原有内容。</li><li>累加寄存器：是一个通用寄存器，用于暂时存放 ALU 运算的结果信息，用于实现加法运算。</li><li>程序状态字寄存器（PSW）：保留由算术逻辑运算指令或测试指令的结果的各种状态信息，如溢出标志（OP）、符号标志（SF）、零标志（ZF）、进位标志（CF）等。<strong>这些位参与并决定微操作的形成</strong>。</li><li>移位器：对运算结果进行移位运算。</li><li>计数器：控制乘除运算的操作步数。</li></ul></li></ul><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023104243472.png" alt="image-20211023104243472" style="zoom:80%;"><ul><li><strong>专用数据通路方式</strong>：根据指令执行过程中的数据和地址的流动方向安排连接线路。<ul><li>特点：<strong>性能较高</strong>，基本不存在数据冲突现象，但<strong>结构复杂</strong>，硬件量大，不易实现。</li></ul></li><li>针对问题：<ul><li>如果 ALU 与寄存器组直接用导线连接，相当于多个寄存器同时并且一直向 ALU 传输数据</li></ul></li><li>改进方向：<ul><li>解决方法1：使用<strong>多路选择器</strong>根据控制信号选择一路输出</li><li>解决方法2：使用<strong>三态门</strong>可以控制每一路是否输出</li></ul></li></ul><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023105644925.png" alt="image-20211023105644925" style="zoom:80%;"><ul><li><strong>CPU 内部单总线方式</strong>：将所有寄存器的输入端和输出端都连接到一条公共的通路上。<ul><li>特点：结构简单，容易实现，但数据传输存在较多冲突的现象，性能较低。</li></ul></li><li>针对问题：<ul><li>ALU 的计算结果会在电流不稳定时直接传回 CPU 内部总线，破坏寄存器信息</li></ul></li><li>改进方向：<ul><li>在 ALU 后加入<strong>暂存寄存器</strong>和<strong>三态门</strong>来等待稳定之后再将结果写入寄存器</li><li>通常会把 ALU 后的寄存器添加移位功能</li></ul></li></ul><h3 id="控制器的基本结构"><a href="#控制器的基本结构" class="headerlink" title="控制器的基本结构"></a>控制器的基本结构</h3><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023121447512.png" alt="image-20211023121447512"></p><ol><li>程序计数器：存放下一条指令在主存中的地址。CPU 会根据 PC 的内容去主存中取指令。通常 PC 有自增功能，有的也会交给 ALU 进行加一。</li><li>指令寄存器：用于保存当前正在执行的指令。</li><li>指令译码器：仅<strong>对操作码字段进行译码</strong>，向控制器提供特定的操作信号。</li><li>微操作信号发生器：根据 <strong>IR 的内容</strong>（指令）、<strong>PSW 的内容</strong>（状态信息）及<strong>时序信号</strong>（时序脉冲），产生控制整个计算机系统所需的各种控制信号，其结构有<strong>组合逻辑型</strong>和<strong>存储逻辑型</strong>两种。</li><li>时序系统：用于产生各种时序信号，它们都是由统一时钟（CLOCK）分频得到。</li><li>存储器地址寄存器：用于存放所要访问的主存单元地址。</li><li>存储器数据寄存器：用于存放向主存写入的信息或从主存中读出的信息。</li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023122455417.png" alt="image-20211023122455417"></p><ul><li>用户可见寄存器：通用寄存器组、程序状态字寄存器 PSW、程序计数器 PC</li><li>用户不可见寄存器：MAR、MDR、IR、暂存寄存器</li></ul><h2 id="指令执行过程"><a href="#指令执行过程" class="headerlink" title="指令执行过程"></a>指令执行过程</h2><h3 id="指令周期"><a href="#指令周期" class="headerlink" title="指令周期"></a>指令周期</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023123259434.png" alt="image-20211023123259434" style="zoom:80%;"><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023123229871.png" alt="image-20211023123229871" style="zoom:80%;"></li><li><strong>指令周期</strong>：CPU 从主存中每<strong>取出并执行</strong>一条指令所需的全部时间。<ul><li>指令周期常常用若干<strong>机器周期</strong>（<strong>CPU 周期</strong>）来表示。</li><li>一个机器周期又包含若干<strong>时钟周期</strong>（也称为<strong>节拍</strong>、<strong>T 周期</strong>、<strong>主频的倒数</strong>或 <strong>CPU 时钟周期</strong>，它是 CPU 操作的<strong>最基本单位</strong>）。</li></ul></li><li>每个指令周期内机器周期数可以<strong>不等</strong>，每个机器周期内的节拍数也可以<strong>不等</strong>。<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023124356581.png" alt="image-20211023124356581" style="zoom:80%;"></li></ul></li></ul><h3 id="指令周期数据流"><a href="#指令周期数据流" class="headerlink" title="指令周期数据流"></a>指令周期数据流</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023130107891.png" alt="image-20211023130107891" style="zoom:80%;"></li><li><p>四个工作周期都有CPU访存操作，只是访存的目的不同。</p><ul><li><strong>取指周期</strong>是为了<strong>取指令</strong></li><li><strong>间址周期</strong>是为了<strong>取有效地址</strong></li><li><strong>执行周期</strong>是为了<strong>取操作数</strong></li><li><strong>中断周期</strong>是为了<strong>保存程序断点</strong></li></ul></li><li><p name="取指周期">取指周期（所有指令都一样）</p><ul><li>(PC) → MAR：当前指令地址送至存储器地址寄存器</li><li>1 → R：CU 发出控制信号，经控制总线传到主存，这里是给<strong>读信号接口</strong>输送一个高电平</li><li>M(MAR) → MDR：将 MAR 所指主存中的内容经数据总线送入 MDR</li><li>(MDR) → IR：将 MDR 中的内容（此时是指令）送入 IR</li><li>OP(IR) → ID：将指令（IR 中的操作码部分）传入指令译码器（Instruction Decoder，ID）</li><li>(PC)+1 → PC：CU 发出控制信号，形成下一条指令地址</li></ul></li><li><p>间址周期（所有指令都一样）</p><ul><li>Ad(IR/MDR) → MAR：将指令的地址码送入 MAR</li><li>1 → R：CU 发出控制信号，启动主存做<strong>读操作</strong></li><li>M(MAR) → MDR：将 MAR 所指主存中的内容经数据总线送入 MDR</li><li>(MDR) → Ad(IR)：将最终的有效地址（IR 中的地址码部分）送至指令的地址码字段</li></ul></li><li><p>执行周期（各不相同）</p><ul><li>执行周期的任务是根据 IR 中的指令字的操作码和操作数通过 ALU 操作产生执行结果。不同指令的执行周期操作不同，因此没有统一的数据流向。</li><li>CLA：clear ACC（ACC清零）<ul><li>0 → AC</li></ul></li><li>LDA X：load ACC（把X所指内容取到ACC）<ul><li>Ad ( IR ) → MAR</li><li>1 → R</li><li>M ( MAR ) → MDR</li><li>MDR → AC</li></ul></li><li>JMP X：（无条件转移)<ul><li>Ad(IR) → PC</li></ul></li><li>BAN X：Branch ACC Negative（当ACC为负时转移）<ul><li>A<del>0</del> • Ad (IR) + (!A<del>0</del>) • (PC) → PC</li></ul></li></ul></li><li><p>中断周期</p><ul><li>中断：暂停当前任务去完成其他任务。为了能够恢复当前任务，需要保存断点。一般使用堆栈来保存断点，假设 SP 指向栈顶元素（低地址部分）。进栈操作是先修改指针，后存入数据。</li><li>中断周期三个任务：<ul><li>保存断点</li><li>形成中断服务程序的入口地址</li><li>关中断</li></ul></li><li>(SP)-1 → SP，(SP) → MAR：CU 控制 SP 减1，修改后的地址送入 MAR</li><li>1 → W：CU 发出控制信号，启动主存做<strong>写操作</strong></li><li>(PC) → MDR：将断点（PC 内容）送入 MDR</li><li>向量地址 → PC：CU 控制将中断服务程序的入口地址（由向量地址形成部件产生）送入 PC</li></ul></li></ul><h3 id="指令执行方案"><a href="#指令执行方案" class="headerlink" title="指令执行方案"></a>指令执行方案</h3><ul><li>一个指令周期通常要包括几个时间段（执行步骤），每个步骤完成指令的一部分功能，几个依次执行的步骤完成这条指令的全部功能。</li><li>单指令周期<ul><li>所有指令都选用相同的执行时间来完成，指令之间串行执行。</li><li>指令周期取决于执行时间最长的指令的执行时间。对于可以在更短时间内完成的指令，要使用等待至最长的周期，会降低系统的运行速度。</li></ul></li><li>多指令周期<ul><li>对不同类型的指令选用不同的执行步骤来完成 ，指令之间串行执行。</li><li>可选用不同个数的时钟周期来完成不同指令的执行过程 。需要更复杂的硬件设计。</li></ul></li><li>流水线方案<ul><li>在每一个时钟周期启动一条指令，让多条指令处在<strong>不同的执行步骤</strong>中<strong>同时运行</strong>，指令之间并行执行。</li></ul></li></ul><h2 id="数据通路（大题高频）"><a href="#数据通路（大题高频）" class="headerlink" title="数据通路（大题高频）"></a>数据通路（大题高频）</h2><ul><li>数据通路：数据在功能部件之间传送的路径。<ul><li>寄存器-寄存器</li><li>寄存器-主存</li><li>寄存器-ALU</li></ul></li><li>数据通路的基本结构：<ul><li>CPU 内部单总线方式（ALU 必须配合暂存寄存器使用）</li><li>CPU 内部多总线方式（相比单总线成本更高，但是数据传输更有效率）</li><li>专用数据通路方式</li></ul></li></ul><h3 id="单总线结构"><a href="#单总线结构" class="headerlink" title="单总线结构"></a>单总线结构</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023141239720.png" alt="image-20211023141239720" style="zoom:80%;"></li><li>寄存器之间数据传送<ul><li>(PC)→Bus：PCout 有效（PC 内容送总线）</li><li>Bus→MAR：MARin 有效（总线内容送 MAR）</li></ul></li><li>主存与CPU之间的数据传送<ul><li>(PC)→Bus→MAR：PCout 和 MARin 有效（现行指令地址→MAR）</li><li>1→R：CU 发读命令</li><li>MEM(MAR)→MDR：MDRinE 有效</li><li>MDR→Bus→IR：MDRout 和 IRin 有效（现行指令→IR）</li></ul></li><li>执行算术或逻辑运算<ul><li>Ad(IR/MDR)→Bus→MAR：MDRout 和 MARin 有效</li><li>1→R：CU 发读命令</li><li>MEM(MAR)→数据线→MDR：MDRin 有效</li><li>MDR→Bus→Y：MDRout 和 Yin 有效（操作数→Y）</li><li>(ACC)+(Y)→Z：ACCout 和 ALUin 有效（CU 向 ALU 发送加命令）</li><li>Z→ACC：Zout 和 ACCin 有效（结果→ACC）</li></ul></li></ul><h3 id="专用通路结构"><a href="#专用通路结构" class="headerlink" title="专用通路结构"></a>专用通路结构</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023145547590.png" alt="image-20211023145547590" style="zoom:80%;"></li><li>取指周期<ul><li>(PC)→MAR：C0有效</li><li>(MAR)→主存：C1有效</li><li>1→R：控制单元向主存发送读命令</li><li>M(MAR)→MDR：C2有效</li><li>(MDR)→IR：C3有效</li><li>(PC)+1→PC</li><li>Op(IR)→CU：C4有效</li></ul></li></ul><blockquote><p>数据通路是计组大题高频考点！一定要做题巩固！</p></blockquote><h2 id="控制器的功能和工作原理"><a href="#控制器的功能和工作原理" class="headerlink" title="控制器的功能和工作原理"></a>控制器的功能和工作原理</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><ul><li>硬布线控制器：使用<strong>纯硬件</strong>的方式来实现控制器的功能</li><li>微程序控制器：使用<strong>软硬件结合</strong>的方式来实现控制器的功能</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023185221891.png" alt="image-20211023185221891"></li><li>CPU 会在每个时钟周期内发出一个<strong>微命令</strong>，即完成一个对应<strong>微操作</strong>的控制信号。<ul><li>如，微命令1 使得 PCout、MARin 有效，从而完成对应的微操作1 (PC)-&gt;MAR。</li><li>微命令与微操作是相互对应的</li><li>特点：<ol><li>多个<strong>相容</strong>的微操作是可以<strong>在一个节拍内并行</strong>的。比如使用专用通路结构就可以提高微命令的并行数量</li><li>同一个微操作可能在不同的指令阶段中重复使用</li><li>在定长机器周期中，通常以<strong>访存所需节拍数</strong>作为参考</li><li>若实际所需节拍数较少，可将微操作安排在机器周期末尾进行</li></ol></li></ul></li><li>根据<strong>指令操作码</strong>、<strong>当前的机器周期</strong>、<strong>节拍信号</strong>、<strong>机器状态条件</strong>，即可确定现在这个节拍下应该发出哪些“微命令”</li><li>CPU 所处的周期由四个不同的触发器确定。</li></ul><h3 id="硬布线控制器"><a href="#硬布线控制器" class="headerlink" title="硬布线控制器"></a>硬布线控制器</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023190651176.png" alt="image-20211023190651176"></li><li>设计步骤<ol><li><a href="#取指周期">分析每个阶段的微操作序列</a>（哪些指令在<strong>什么阶段</strong>、<strong>什么条件</strong>下会使用到<strong>什么微操作</strong>）</li><li>选择 CPU 的控制方式（定长/不定长周期，每个周期的节拍数）</li><li>安排微操作时序</li><li>电路设计（逻辑表达式与电路设计）<ul><li>列出操作时间表</li><li>写出微操作的最简表达式</li><li>画出逻辑图</li></ul></li></ol></li><li>安排微操作时序的原则<ol><li>微操作的<strong>先后顺序</strong>不得随意更改</li><li><strong>被控对象不同</strong>的微操作尽量安排在<strong>一个节拍</strong>内完成</li><li>占用<strong>时间较短</strong>的微操作尽量安排在<strong>一个节拍</strong>内完成，允许有先后顺序</li></ol></li><li>取指周期：<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023194220430.png" alt="image-20211023194220430" style="zoom:80%;"></li><li>M ( MAR ) -&gt; MDR：从主存取数据，用时较长，因此必须一个时钟周期才能保证微操作的完成</li><li>MDR -&gt; IR：是 CPU 内部寄存器之间的数据传送，速度很快，因此在一个时钟周期内可以紧接着完成第二个微命令 OP ( IR ) -&gt; ID。</li></ul></li><li>间址周期：<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023194429655.png" alt="image-20211023194429655" style="zoom:80%;"></li></ul></li><li>执行周期：<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211023194744913.png" alt="image-20211023194744913" style="zoom:80%;"></li></ul></li><li>缺点：<ul><li>指令越多，设计、实现就越复杂，因此<strong>一般用于 RISC</strong>（精简指令集系统） </li><li>如果扩充一条新的指令，则控制器的设计就需要大改，因此<strong>扩充指令较困难</strong>。</li></ul></li><li>优点：<ul><li>由于使用纯硬件实现控制，因此执行<strong>速度很快</strong>。</li><li><strong>微操作控制信号</strong>由组合逻辑电路根据当前的指令码、状态和时序，<strong>即时产生</strong>。</li></ul></li></ul><h3 id="微程序控制器（考点）"><a href="#微程序控制器（考点）" class="headerlink" title="微程序控制器（考点）"></a>微程序控制器（考点）</h3><h4 id="基本原理-1"><a href="#基本原理-1" class="headerlink" title="基本原理"></a>基本原理</h4><h5 id="微程序的概念"><a href="#微程序的概念" class="headerlink" title="微程序的概念"></a>微程序的概念</h5><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211024094522115.png" alt="image-20211024094522115"></li><li>基本思想：采用“存储程序”的思想，CPU 出厂前将所有指令的“微程序”存入“控制器存储器”中</li><li>层次结构：<ul><li>程序：由<strong>指令序列</strong>组成</li><li><strong>微程序</strong>：<ul><li>由<strong>微指令序列</strong>组成，<strong>每一种指令对应一个微程序</strong></li><li><strong>取指、间指、执行、中断周期微程序段</strong>，共同<strong>组成一个微程序</strong></li></ul></li><li><strong>指令</strong>：是对程序执行步骤的描述<ul><li>指令/微程序是对微指令功能的“封装”</li><li>是一个微指令序列</li></ul></li><li><strong>微指令</strong>：是对指令执行步骤的描述<ul><li>微指令中可能包含多个<strong>微命令</strong></li><li><strong>每一个微命令对应一个微操作</strong>，<strong>每一个微命令对应一条输出控制线</strong></li></ul></li></ul></li></ul><h5 id="微程序控制器基本结构"><a href="#微程序控制器基本结构" class="headerlink" title="微程序控制器基本结构"></a>微程序控制器基本结构</h5><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211024180509785.png" alt="image-20211024180509785"></li><li><strong>微地址形成部件</strong>：产生初始微地址和后继微地址，以保证微指令的连续执行。</li><li><strong>CMAR</strong>：又称 <strong>μPC</strong>，微地址寄存器，接收微地址形成部件送来的微地址，为在 CM 中读取微指令作准备。</li><li><strong>地址译码</strong>：将地址码转化为存储单元控制信号。</li><li><strong>控制存储器 CM</strong>：简称<strong>控存</strong>，用于存放各指令对应的微程序，控制存储器是由<strong>只读存储器 ROM</strong> 构成。</li><li><strong>CMDR</strong>：别名：<strong>μIR</strong>，用于存放从 CM 中取出的微指令，它的位数同微指令字长相等。</li><li><strong>顺序逻辑</strong>：<ul><li>根据<strong>指令地址码的寻址特征位</strong>判断是否要跳过间址周期</li><li>根据<strong>中断信号</strong>判断是否进入中断周期</li></ul></li><li>所有指令的<strong>取指周期</strong>、<strong>间址周期</strong>、<strong>中断周期</strong>所对应的微指令序列都一样，<strong>可以共享使用</strong>。</li><li>取指周期微程序：（常考）<ul><li>通常是公用的，故如果某指令系统中有 n 条机器指令，则 CM 中微程序（段）的个数至少是 <strong>n+1</strong> 个</li><li>一些早期的 CPU、物联网设备的 CPU <strong>可以不提供间接寻址和中断功能</strong>，因此这类 CPU 可以不包含间址周期、中断周期的微程序段</li></ul></li><li>有的选择题中，虽然取指周期、执行周期<strong>在物理上</strong>是两个微程序，但<strong>逻辑上</strong>应该把它们看作一个整体。即，<strong>一条指令对应一个微程序</strong>。</li></ul><h4 id="微指令的设计"><a href="#微指令的设计" class="headerlink" title="微指令的设计"></a>微指令的设计</h4><h5 id="微指令的格式"><a href="#微指令的格式" class="headerlink" title="微指令的格式"></a>微指令的格式</h5><ul><li>水平型微指令：一条微指令能定义多个可并行的微命令。<ul><li>优点：微程序短，执行速度快；</li><li>缺点：微指令长，编写微程序较麻烦。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211024182849406.png" alt="image-20211024182849406"></li></ul></li><li>垂直型微指令：一条微指令只能定义<strong>一个</strong>微命令<strong>，</strong>由微操作码字段规定具体功能<ul><li>优点：微指令短、简单、规整，便于编写微程序；</li><li>缺点：微程序长，执行速度慢，工作效率低。 </li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211024182907683.png" alt="image-20211024182907683"></li></ul></li><li>混合型微指令：<ul><li>在垂直型的基础上增加一些不太复杂的并行操作。</li><li>微指令较短，仍便于编写；微程序也不长，执行速度加快。</li></ul></li></ul><blockquote><ul><li><strong>相容性</strong>微命令：可以并行完成的微命令。</li><li><strong>互斥性</strong>微命令：不允许并行完成的微命令。</li></ul></blockquote><h5 id="微指令的编码方式（高频）"><a href="#微指令的编码方式（高频）" class="headerlink" title="微指令的编码方式（高频）"></a>微指令的编码方式（高频）</h5><p><strong>微指令的编码方式</strong>，又称<strong>微指令的控制方式</strong>，指如何对微指令的控制字段进行编码，以形成控制信号。目标是在保证速度的情况下，尽量缩短微指令字长。</p><ol><li>直接编码（直接控制）方式<ul><li>在微指令的操作控制字段中，<strong>每一位代表一个微操作命令</strong>，某位为 1 表示该控制信号有效</li><li>优点：简单、直观，执行速度快，操作并行性好。</li><li>缺点：微指令字长过长，n 个微命令就要求微指令的操作字段有 n 位，造成控存容量极大。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211024205924843.png" alt="image-20211024205924843"></li></ul></li><li>字段直接编码方式<ul><li>将微指令的控制字段分成若干“段”，<strong>每段经译码后发出控制信号</strong>微命令字段分段的原则：<ul><li><strong>互斥性</strong>微命令分在<strong>同一段内</strong>，<strong>相容性</strong>微命令分在<strong>不同段内</strong>。不同段/不同互斥类/相容的微命令可以并行</li><li><strong>每个小段</strong>中包含的<strong>信息位不能太多</strong>，否则将增加译码线路的复杂性和译码时间。</li><li>一般<strong>每个小段还要留出一个状态</strong>，表示本字段不发出任何微命令。因此，当某字段的长度为 3 位时，最多只能表示 7 个互斥的微命令，<strong>通常用 000 表示不操作</strong>。</li></ul></li><li>优点：可以缩短微指令字长。</li><li>缺点：要通过译码电路后再发出微命令，因此比直接编码方式慢 。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211024210134498.png" alt="image-20211024210134498"></li></ul></li><li>字段间接编码方式<ul><li>一个字段的某些微命令需由另一个字段中的某些微命令来解释，由于不是靠字段直接译码发出的微命令，故称为字段间接编码，又称隐式编码。</li><li>优点：可进一步缩短微指令字长。</li><li>缺点：削弱了微指令的并行控制能力。通常作为字段直接编码方式的一种辅助手段。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211024210333131.png" alt="image-20211024210333131"></li></ul></li></ol><h5 id="微指令的地址形成方式"><a href="#微指令的地址形成方式" class="headerlink" title="微指令的地址形成方式"></a>微指令的地址形成方式</h5><ol><li><strong>断定方式</strong>（常考）：微指令的下地址字段直接指出后继微指令的地址。</li><li>根据机器指令的<strong>操作码</strong>形成：当机器指令取至指令寄存器后，微指令的地址由操作码经微地址形成部件形成。（不常考）</li><li>增量<strong>计数器法</strong>（常考）：(CMAR) + 1 -&gt; CMAR/μPC</li><li>分支转移：操作控制字段+转移方式+转移地址。转移方式，指明判别条件；转移地址，指明转移成功后的去向。（不常考）</li><li>通过测试网络（不常考）</li><li>由<strong>硬件</strong>产生微程序入口地址：<ul><li>第一条微指令地址：由<strong>硬件</strong>产生（用专门的硬件记录取指周期微程序首地址）</li><li>中断周期：由<strong>硬件</strong>产生中断周期微程序首地址（用专门的硬件记录）</li></ul></li></ol><h4 id="微程序控制单元的设计"><a href="#微程序控制单元的设计" class="headerlink" title="微程序控制单元的设计"></a>微程序控制单元的设计</h4><ul><li><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211024232747714.png" alt="image-20211024232747714"></p></li><li><p>设计步骤：</p><ol><li><p>分析每个阶段的微操作序列</p></li><li><p>写出对应机器指令的微操作命令及节拍安排</p><ul><li><p> 写出每个周期所需要的微操作（参照硬布线）</p></li><li><p>补充微程序控制器特有的微操作：</p><p>a. 取指周期：</p><ul><li>Ad ( CMDR ) -&gt; CMAR（每条微指令结束都要进行）</li><li>OP ( IR ) -&gt; 微地址形成部件 -&gt; CMAR（取指周期结束，根据指令操作码确定其执行周期的微程序首地址）</li></ul><p>b. 执行周期：</p><ul><li>Ad(CMDR) -&gt; CMAR（每条微指令结束都要进行，最后一条微指令的下地址是 0）</li></ul></li></ul></li><li><p>确定微指令格式</p><ul><li>根据<strong>微操作个数</strong>决定采用何种<strong>编码方式</strong>，以确定微指令的<strong>操作控制字段的位数</strong>。</li><li>根据 CM 中存储的<strong>微指令总数</strong>，确定微指令的<strong>顺序控制字段的位数</strong>。</li><li>最后按操作控制字段位数和顺序控制字段位数就可确定<strong>微指令字长</strong>。</li></ul></li><li><p>编写微指令码点</p><ul><li>根据操作控制字段每一位代表的微操作命令，编写每一条微指令的码点。</li></ul></li></ol></li><li><p>读出微指令：</p><ul><li>取指周期的<strong>第一条微指令地址由硬件自动给出</strong></li><li>用微指令 a 的下地址表示 b 的地址。上一条微指令的下地址部分转到下一条微指令的译码，同样需要消耗一个节拍</li></ul></li></ul><h4 id="微程序设计分类"><a href="#微程序设计分类" class="headerlink" title="微程序设计分类"></a>微程序设计分类</h4><ol><li>静态微程序设计和动态微程序设计<ul><li>静态：微程序无需改变，采用 ROM</li><li>动态：通过改变<strong>微指令</strong>和<strong>微程序</strong>改变机器指令。有利于仿真，采用 EPROM</li></ul></li><li>毫微程序设计<ul><li>微程序设计：用微程序解释机器指令</li><li>毫微程序设计：用毫微程序解释微程序</li><li><strong>毫微指令与微指令</strong>的关系好比<strong>微指令与机器指令</strong>的关系</li></ul></li></ol><h3 id="硬布线与微程序的比较"><a href="#硬布线与微程序的比较" class="headerlink" title="硬布线与微程序的比较"></a>硬布线与微程序的比较</h3><table><thead><tr><th>对比项目</th><th>微程序控制器</th><th>硬布线控制器</th></tr></thead><tbody><tr><td>工作原理</td><td>读出微程序</td><td>电路即时产生</td></tr><tr><td>执行速度</td><td>慢</td><td>快</td></tr><tr><td>规整性</td><td>较规整</td><td>繁琐，不规整</td></tr><tr><td>应用场合</td><td>CISC CPU</td><td>RISC CPU</td></tr><tr><td>易扩充性</td><td>易扩充修改</td><td>困难</td></tr></tbody></table><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211024235150778.png" alt="image-20211024235150778"></p><h2 id="指令流水线（大题考察）"><a href="#指令流水线（大题考察）" class="headerlink" title="指令流水线（大题考察）"></a>指令流水线（大题考察）</h2><h3 id="指令流水线的基本概念"><a href="#指令流水线的基本概念" class="headerlink" title="指令流水线的基本概念"></a>指令流水线的基本概念</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul><li>指令流水：一条指令的执行过程可以分成多个阶段（或过程）。根据计算机的不同，具体的分法、数量也不同。<ul><li>取指（Instruction Fetch，IF）、分析（Instruction Decode，ID）、执行（Execute）、Memory、Writeback</li><li>分成 5 个阶段的 RISC 是比较常见的考法。</li></ul></li></ul><h4 id="执行方式"><a href="#执行方式" class="headerlink" title="执行方式"></a>执行方式</h4><ul><li>顺序执行方式：<ul><li>总耗时T = n×3t = 3nt</li><li>传统冯·诺依曼机采用顺序执行方式，又称串行执行方式。</li><li>优点：控制简单，硬件代价小。</li><li>缺点：执行指令的速度较慢，在任何时刻，处理机中只有一条指令在执行，各功能部件的利用率很低。</li></ul></li><li>一次重叠执行方式：<ul><li>总耗时T = 3t + (n-1)×2t = (1+2n)t</li><li>优点：程序的执行时间缩短了1/3，各功能部件的利用率明显提高。</li><li>缺点：需要付出硬件上较大开销的代价，控制过程也比顺序执行复杂了。</li></ul></li><li>二次重叠执行方式：<ul><li>总耗时T = 3t + (n-1)×t = (2+n)t</li><li>与顺序执行方式相比，指令的执行时间缩短近2/3。这是一种理想的指令执行方式，在正常情况下，处理机中同时有3条指令在执行。</li></ul></li></ul><h4 id="表示方法（重要）"><a href="#表示方法（重要）" class="headerlink" title="表示方法（重要）"></a>表示方法（重要）</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025092552132.png" alt="image-20211025092552132"></li><li>指令执行过程图：主要用于分析指令执行过程以及影响流水线的因素</li><li>时空图：主要用于分析流水线的性能</li></ul><h4 id="性能指标（重要）"><a href="#性能指标（重要）" class="headerlink" title="性能指标（重要）"></a>性能指标（重要）</h4><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025093530754.png" alt="image-20211025093530754"></p><p>一条指令的执行分为 k 个阶段，每个阶段耗时 Δt ，一般取 Δt=一个时钟周期（应该是机器周期，在理想情况下可以当做时钟周期）</p><ul><li>吞吐率：在单位时间内流水线所完成的任务数量。<ul><li>吞吐率计算公式：$TP = \frac{n}{T_{k}}$</li><li>实际吞吐率为：$TP = \frac{n}{(k+n-1)Δt}$</li><li>当连续输入的任务 n→∞ 时，最大吞吐率为 TPmax=1/Δt。</li></ul></li><li>加速比：完成同样一批任务，<strong>不使用流水线所用的时间</strong>与<strong>使用流水线所用的时间</strong>之比。<ul><li>加速比公式：$S = \frac{T_{0}}{T_{k}}$</li><li>实际加速比：$S = \frac{knΔt}{(k+n-1)Δt} = \frac{kn}{(k+n-1)}$</li><li>当连续输入的任务 n→∞ 时，最大加速比为 Smax=k。</li></ul></li><li>效率：流水线的设备利用率称为流水线的效率。<ul><li>在时空图上，流水线的效率定义为完成<strong>n个任务占用的时空区有效面积</strong>与<strong>n个任务所用的时间与k个流水段所围成的时空区总面积</strong>之比。</li><li>效率公式：$E = \frac{n个任务占用k时空区有效面积}{n个任务所用的时间与k个流水段所围成的时空区总面积}=\frac{T_{0}}{kT_{k}}$</li><li>当连续输入的任务 n→∞ 时，最高效率为 Emax=1。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025095147120.png" alt="image-20211025095147120"></li></ul></li></ul><h3 id="指令流水线的影响因素和分类"><a href="#指令流水线的影响因素和分类" class="headerlink" title="指令流水线的影响因素和分类"></a>指令流水线的影响因素和分类</h3><h4 id="影响流水线的因素（重要）"><a href="#影响流水线的因素（重要）" class="headerlink" title="影响流水线的因素（重要）"></a>影响流水线的因素（重要）</h4><ul><li>结构相关（资源冲突）<ul><li>由于多条指令在同一时刻争用同一资源而形成的冲突称为结构相关。</li><li>解决办法：<ul><li>后一相关指令暂停一周期</li><li>资源重复配置：数据存储器+指令存储器</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025102708962.png" alt="image-20211025102708962"></li></ul></li><li>数据相关（数据冲突）<ul><li>数据相关指在一个程序中，存在必须等前一条指令执行完才能执行后一条指令的情况，则这两条指令即为数据相关。</li><li>解决办法：<ul><li>把遇到数据相关的指令及其后续指令都暂停一至几个时钟周期，直到数据相关问题消失后再继续执行。可分为<strong>硬件阻塞（stall）</strong>和<strong>软件插 入（NOP）</strong>两种方法。</li><li>数据旁路技术（转发机制）：通过增加电路（数据旁路），将上一步的 ALU 结果直接送入下一步的 ALU</li><li>编译优化：通过编译器调整指令顺序来解决数据相关。</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025102758077.png" alt="image-20211025102758077"></li></ul></li><li>控制相关（控制冲突）<ul><li>当流水线遇到改变 PC 值的指令（如转移指令、CALL、中断）而造成断流时，会引起控制相关。</li><li>解决办法：（由转移指令发生）<ul><li>转移指令预测。简单预测（永远猜true或false）动态预测（根据历史情况动态调整）</li><li>预取转移成功和不成功两个控制流方向上的目标指令（需要增加至能够支持两个方向的寄存器数量）</li><li>加快和提前形成条件码（类似于全加器并行）</li><li>提高转移方向的猜准率</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025103223036.png" alt="image-20211025103223036"></li></ul></li></ul><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025103334530.png" alt="image-20211025103334530"></p><h4 id="流水线的分类"><a href="#流水线的分类" class="headerlink" title="流水线的分类"></a>流水线的分类</h4><ul><li>部件功能级、处理机级和处理机间级流水线（流水线使用的级别）<ul><li><strong>部件功能级流水</strong>就是将复杂的算术逻辑运算组成流水线工作方式。例如，可将浮点加法操作分成求阶差、对阶、尾数相加以及结果规格化等4个子过程。</li><li><strong>处理机级流水</strong>是把一条指令解释过程分成多个子过程，如前面提到的取指、译码、执行、访存及写回 5 个子过程。</li><li><strong>处理机间流水</strong>是一种宏流水，其中每一个处理机完成某一专门任务，各个处理机所得到的结果需存放在与下一个处理机所共享的存储器中。</li></ul></li><li>单功能流水线和多功能流水线（流水线可以完成的功能）<ul><li><strong>单功能流水线</strong>指只能实现一种固定的专门功能的流水线</li><li><strong>多功能流水线</strong>指通过各段间的不同连接方式可以同时或不同时地实现多种功能的流水线</li></ul></li><li>动态流水线和静态流水线（同一时间内各段之间的连接方式）<ul><li><strong>静态流水线</strong>指在同一时间内，流水线的各段只能按同一种功能的连接方式工作。</li><li><strong>动态流水线</strong>指在同一时间内，当某些段正在实现某种运算时，另一些段却正在进行另一种运算。这样对提高流水线的效率很有好处，但会使流水线控制变得很复杂。</li></ul></li><li>线性流水线和非线性流水线（各个功能段之间是否有反馈信号）<ul><li><strong>线性流水线</strong>中，从输入到输出，每个功能段只允许经过一次，不存在反馈回路。</li><li><strong>非线性流水线</strong>存在反馈回路，从输入到输出过程中，某些功能段将数次通过流水线，这种流水线适合进行线性递归的运算。</li></ul></li></ul><h4 id="流水线的多发技术"><a href="#流水线的多发技术" class="headerlink" title="流水线的多发技术"></a>流水线的多发技术</h4><ul><li><strong>超标量技术</strong>（空分复用）<ul><li>每个时钟周期内可<strong>并发多条独立指令</strong></li><li>要配置<strong>多个功能部件</strong>（多个 ALU、寄存器组）</li><li><strong>不能调整</strong>指令的<strong>执行顺序</strong>（乱序发射 CPU 可以调整顺序）</li><li>通过编译优化技术，把可并行执行的指令搭配起来</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025105329051.png" alt="image-20211025105329051"></li></ul></li><li><strong>超流水技术</strong>（时分复用）<ul><li>在<strong>一个时钟周期内再分段</strong>（3段）</li><li>在一个时钟周期（机器周期，理想情况下min机器周期-&gt;时钟周期）内<strong>一个功能部件使用多次</strong>（ 3 次）</li><li><strong>不能调整</strong>指令的<strong>执行顺序</strong></li><li>靠编译程序解决优化问题</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025105518773.png" alt="image-20211025105518773"></li></ul></li><li><strong>超长指令字</strong><ul><li>由<strong>编译程序</strong>挖掘出指令间<strong>潜在的并行性</strong></li><li>将<strong>多条能并行操作的指令组合</strong>成 一条</li><li>具有<strong>多个操作码字段</strong>的超长指令字（可达几百位）</li><li>采用<strong>多个处理部件</strong></li></ul></li></ul><h3 id="五段式指令流水线"><a href="#五段式指令流水线" class="headerlink" title="五段式指令流水线"></a>五段式指令流水线</h3><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part5/image-20211025100227733.png" alt="image-20211025100227733"></li><li>为方便流水线的设计，将每个阶段的耗时取成一样，以最长耗时为准。即此处应将机器周期设置为 100ns。</li><li>流水线<strong>每一个功能段</strong>部件后面都要有一个<strong>缓冲寄存器</strong>，或称为<strong>锁存器</strong>，其作用是<strong>保存本流水段的执行结果</strong>，提供给下一流水段使用。</li><li><strong>① IF 取指 -&gt; ② ID 译码&amp;取数 -&gt; ③ EX 执行 -&gt; ④ M 访存 -&gt; ⑤ WB 写回寄存器</strong></li><li>考试中常见的五类指令：<ul><li><strong>运算类指令</strong>、<strong>LOAD 指令</strong>、<strong>STORE 指令</strong>、<strong>条件转移指令</strong>、<strong>无条件转移指令</strong></li></ul></li></ul><h4 id="运算类指令的执行过程"><a href="#运算类指令的执行过程" class="headerlink" title="运算类指令的执行过程"></a>运算类指令的执行过程</h4><ul><li>IF：根据 PC 从指令 Cache 取指令至 IF 段的锁存器</li><li>ID：取出操作数至 ID 段锁存器</li><li>EX：运算，将结果存入 EX 段锁存器</li><li>M：空段（RISC 指令集不会在运算指令中访问主存，但是这段时间周期是需要消耗的）</li><li>WB：将运算结果写回指定寄存器</li></ul><table><thead><tr><th>运算类指令举例</th><th>指令的汇编格式</th><th>功能</th></tr></thead><tbody><tr><td>加法指令（两个寄存器相加）</td><td>ADD Rs, Rd</td><td>(Rs)+(Rd) -&gt; Rd</td></tr><tr><td>加法指令（寄存器与立即数相加）</td><td>ADD #996, Rd</td><td>996+(Rd) -&gt; Rd</td></tr><tr><td>算数左移指令</td><td>SHL Rd</td><td>(Rd)&lt;&lt;&lt;2 -&gt; Rd</td></tr></tbody></table><blockquote><ul><li>Rs 指源操作数（source）</li><li>Rd 指目的操作数（destination）</li></ul></blockquote><h4 id="LOAD-指令的执行过程"><a href="#LOAD-指令的执行过程" class="headerlink" title="LOAD 指令的执行过程"></a>LOAD 指令的执行过程</h4><ul><li>IF：根据 PC 从指令 Cache 取指令至 IF 段的锁存器</li><li>ID：将基址寄存器的值放到锁存器 A，将偏移量的值放到 Imm</li><li>EX：运算，得到有效地址</li><li>M：从数据 Cache 中取数并放入锁存器</li><li>WB：将取出的数写回寄存器</li><li>LOAD 也需要经过运算，才能得到有效地址</li></ul><table><thead><tr><th>指令的汇编格式</th><th>功能</th></tr></thead><tbody><tr><td>LOAD Rd, 996(Rs) 或 LOAD Rd, mem</td><td>(996+(Rs)) -&gt; Rd 或 (mem) -&gt; Rd</td></tr></tbody></table><blockquote><p>通常，RISC 处理器<strong>只有“取数 LOAD”和“存数 STORE”指令才能访问主存</strong></p></blockquote><h4 id="STORE-指令的执行过程"><a href="#STORE-指令的执行过程" class="headerlink" title="STORE 指令的执行过程"></a>STORE 指令的执行过程</h4><ul><li>IF：根据 PC 从指令 Cache 取指令至 IF 段的锁存器</li><li>ID：将基址寄存器的值放到锁存器 A，将偏移量的值放到 Imm。<strong>将要存的数放到 B</strong></li><li>EX：运算，得到有效地址。并将锁存器 B 的内容放到锁存器 Store。</li><li>M：写入数据 Cache</li><li>WB：空段</li></ul><table><thead><tr><th>指令的汇编格式</th><th>功能</th></tr></thead><tbody><tr><td>STORE Rs, 996(Rd) 或 STORE Rs, mem</td><td>Rs -&gt; (996+(Rd)) 或 Rs -&gt; (mem)</td></tr></tbody></table><h4 id="条件转移指令的执行过程"><a href="#条件转移指令的执行过程" class="headerlink" title="条件转移指令的执行过程"></a>条件转移指令的执行过程</h4><ul><li><p>IF：根据 PC 从指令 Cache 取指令至 IF 段的锁存器</p></li><li><p>ID：进行比较的两个数放入锁存器 A、B；偏移量放入 Imm</p></li><li><p>EX：运算，比较两个数</p></li><li><p>M：将目标 PC 值写回 PC（<strong>修改 PC 值不在写回阶段而是访存阶段</strong>）</p></li><li><p>WB：空段（<strong>写回阶段通常是修改通用寄存器</strong>）</p></li><li><p>转移类指令常采用<strong>相对寻址</strong></p></li><li><p>很多教材把写回 PC 的功能段称为“WrPC 段”，其耗时比 M 段更短，可安排在 M 段时间内完成。</p></li><li><p>通常在 IF 段结束止之后 PC 就会自动 +“1”</p></li></ul><table><thead><tr><th>指令的汇编格式</th><th>功能</th></tr></thead><tbody><tr><td>beq Rs, Rt, #偏移量</td><td>若(Rs)==(Rt)，则**(PC)+指令字长+(偏移量×指令字长) -&gt; PC<strong>；否则</strong>(PC)+指令字长 -&gt; PC**</td></tr><tr><td>bne Rs, Rt, #偏移量</td><td>若(Rs)!=(Rt)，则**(PC)+指令字长+(偏移量×指令字长) -&gt; PC<strong>；否则</strong>(PC)+指令字长 -&gt; PC**</td></tr></tbody></table><h4 id="无条件转移指令的执行过程"><a href="#无条件转移指令的执行过程" class="headerlink" title="无条件转移指令的执行过程"></a>无条件转移指令的执行过程</h4><ul><li>IF：根据 PC 从指令 Cache 取指令至IF段的锁存器</li><li>ID：偏移量放入 Imm</li><li>EX：将目标 PC 值写回 PC（<strong>这里修改 PC 值不在写回阶段而又是在执行阶段</strong>）</li><li>M：空段</li><li>WB：空段</li><li>“WrPC段”耗时比 EX 段更短，可安排在 EX 段时间内完成。<strong>尽早完成 WrPC 段、修改 PC 值，就越能避免控制冲突</strong>。当然，也有的地方会在 WB 段时间内才修改 PC 的值</li><li>偏移量通常采用补码表示</li></ul><table><thead><tr><th>指令的汇编格式</th><th>功能</th></tr></thead><tbody><tr><td>jmp #偏移量</td><td>(PC)+指令字长+(偏移量×指令字长) -&gt; PC</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">中央处理器</summary>
    
    
    
    <category term="计算机组成原理" scheme="https://nephrencake.gitee.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机组成原理-Part4</title>
    <link href="https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/"/>
    <id>https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/</id>
    <published>2021-10-02T02:53:13.000Z</published>
    <updated>2021-10-26T15:16:21.917Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算机组成原理-Part4——指令系统"><a href="#计算机组成原理-Part4——指令系统" class="headerlink" title="计算机组成原理-Part4——指令系统"></a>计算机组成原理-Part4——指令系统</h1><p>[TOC]</p><h2 id="指令格式"><a href="#指令格式" class="headerlink" title="指令格式"></a>指令格式</h2><h3 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h3><ul><li>指令（机器指令）：一台计算机的所有指令的集合构成该机的<strong>指令系统</strong>，也称为<strong>指令集</strong>。</li><li>一台计算机只能执行自己指令系统中的指令，不能执行其他系统的指令。Eg：x86 架构、ARM架构</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211021233141411.png" alt="image-20211021233141411"></li></ul><h3 id="根据地址码数目不同分类"><a href="#根据地址码数目不同分类" class="headerlink" title="根据地址码数目不同分类"></a>根据地址码数目不同分类</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211021235538544.png" alt="image-20211021235538544"></li><li>一条指令可能包含不同数量的地址码</li><li>零地址指令<ul><li>不需要操作数，如空操作、停机、关中断等指令</li><li>堆栈计算机，两个操作数隐含存放在栈顶和次栈顶，计算结果压回栈顶（后缀表达式）</li></ul></li><li>一地址指令<ul><li>只需要单操作数，如加1、减1、取反、求补等<ul><li>指令含义：OP(A1) -&gt; A1 ，完成一条指令需要 3 次访存：<strong>取指 -&gt; 读A1 -&gt; 写A1</strong></li></ul></li><li>需要两个操作数，但其中一个操作数隐含在某个寄存器（如隐含在ACC）<ul><li>指令含义： (ACC)OP(A1) -&gt; ACC，完成一条指令需要2次访存：<strong>取指 -&gt; 读A1</strong></li></ul></li><li>注：A1 指某个主存地址， (A1)表示 A1 所指向的地址中的内容</li></ul></li><li>二地址指令<ul><li>常用于需要两个操作数的算术运算、逻辑运算相关指令</li><li>指令含义：(A1)OP(A2) -&gt; A1</li><li>完成一条指令需要访存 4 次，<strong>取指 -&gt; 读A1 -&gt; 读A2 -&gt; 写A1</strong></li></ul></li><li>三地址指令<ul><li>常用于需要两个操作数的算术运算、逻辑运算相关指令</li><li>指令含义：(A1)OP(A2) -&gt; A3</li><li>完成一条指令需要访存 4 次，<strong>取指 -&gt; 读A1 -&gt; 读A2 -&gt; 写A3</strong></li></ul></li><li>四地址指令<ul><li>指令含义：(A1)OP(A2) -&gt; A3，A4=下一条将要执行指令的地址</li><li>完成一条指令需要访存 4 次，<strong>取指 -&gt; 读A1 -&gt; 读A2 -&gt; 写A3</strong></li></ul></li><li><strong>若指令总长度固定不变，则地址码数量越多，寻址能力越差</strong></li></ul><h3 id="根据指令长度分类"><a href="#根据指令长度分类" class="headerlink" title="根据指令长度分类"></a>根据指令长度分类</h3><ul><li><strong>指令字长</strong>：一条指令的总长度（可能会变）<ul><li>机器字长：CPU进行一次整数运算所能处理的二进制数据的位数（通常和ALU直接相关）</li><li>存储字长：一个存储单元中的二进制代码位数（通常和MDR位数相同）</li><li>半字长指令、单字长指令、双字长指令——指令长度是机器字长的 n 倍</li><li>指令字长会影响取指令所需时间。如：机器字长=存储字长=16bit，则取一条双字长指令需要两次访存</li></ul></li><li><strong>定长指令字结构</strong>：指令系统中所有指令的长度都相等</li><li><strong>变长指令字结构</strong>：指令系统中各种指令的长度不等</li></ul><h3 id="根据操作码长度分类"><a href="#根据操作码长度分类" class="headerlink" title="根据操作码长度分类"></a>根据操作码长度分类</h3><ul><li><strong>定长操作码</strong>：指令系统中所有指令的操作码长度都相同<ul><li>控制器的译码电路设计简单，但灵活性较低</li><li>n 位 -&gt; 2^n^ 条指令</li><li>优：定长操作码对于简化计算机硬件设计，提高指令译码和识别速度很有利；</li><li>缺：指令数量增加时会占用更多固定位，留给表示操作数地址的位数受限。</li></ul></li><li><strong>可变长操作码</strong>：指令系统中各指令的操作码长度可变<ul><li>控制器的译码电路设计复杂， 但灵活性较高</li><li>不同地址数的指令可以具有不同长度的操作码，从而在满足需要的前提下，有效地缩短指令字长。</li><li>优： 在指令字长有限的前提下仍保持比较丰富的指令种类；</li><li>缺 ：增加了指令译码和分析的难度，使控制器的设计复杂化。</li></ul></li><li><strong>定长指令字结构+可变长操作码</strong> -&gt; <strong>扩展操作码指令格式</strong><ul><li>不同地址数的指令使用不同长度的操作码</li></ul></li></ul><h3 id="根据操作类型分类"><a href="#根据操作类型分类" class="headerlink" title="根据操作类型分类"></a>根据操作类型分类</h3><ul><li>数据传送类：进行主存与CPU之间的数据传送<ol><li>数据传送<ul><li>LOAD：把<strong>存储器</strong>中的数据放到<strong>寄存器</strong>中</li><li>STORE：把<strong>寄存器</strong>中的数据放到<strong>存储器</strong>中</li></ul></li></ol></li><li>运算类<ol start="2"><li>算术逻辑操作<ul><li>算术：加、减、乘、除、增 1、减 1、求补、浮点运算、十进制运算</li><li>逻辑：与、或、非、异或、位操作、位测试、位清除、位求反</li></ul></li><li>移位操作<ul><li>算术移位、逻辑移位、循环移位(带进位和不带进位)</li></ul></li></ol></li><li>程序控制类：改变程序执行的顺序<ol start="4"><li>转移操作<ul><li>无条件转移 JMP</li><li>条件转移 JZ：结果为 0；JO：结果溢出；JC：结果有进位</li><li>调用和返回 CALL 和 RETURN</li><li>陷阱(Trap)与陷阱指令</li></ul></li></ol></li><li>输入输出类（I/O）：进行CPU和I/O设备之间的数据传送<ol start="5"><li>输入输出操作<ul><li>CPU 寄存器与 IO 端口之间的数据传送（端口即 IO 接口中的寄存器）</li></ul></li></ol></li></ul><h2 id="扩展操作码指令格式"><a href="#扩展操作码指令格式" class="headerlink" title="扩展操作码指令格式"></a>扩展操作码指令格式</h2><ul><li>扩展操作码：定长指令字结构+可变长操作码</li><li>设计扩展操作码指令格式的注意点：<ul><li>不允许短码是长码的前缀。</li><li>各指令的操作码一定不能重复。</li></ul></li><li>通常情况下，对使用频率较高的指令，分配较短的操作码；对使用频率较低的指令，分配较长的操作码。从而尽可能减少指令译码和分析的时间。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022102529044.png" alt="image-20211022102529044"></li><li>设地址长度为 n，上一层留出 m 种状态，则下一层可扩展出 m×2^n^ 种状态</li></ul><h2 id="指令寻址"><a href="#指令寻址" class="headerlink" title="指令寻址"></a>指令寻址</h2><h3 id="顺序寻址"><a href="#顺序寻址" class="headerlink" title="顺序寻址"></a>顺序寻址</h3><ul><li>指令寻址：下一条欲执行指令的地址（<strong>始终由程序计数器PC给出</strong>）</li><li>顺序寻址：( PC ) + “1” -&gt; PC</li><li><strong>这里的“1”理解为 1 个指令的字长</strong>，实际加的值会因<strong>指令长度</strong>、<strong>编址方式</strong>而不同<ul><li><strong>定长</strong>指令字结构 + 按<strong>字</strong>编址 + 指令字长=存储字长=16bit=2B：每次 PC + 1 </li><li><strong>定长</strong>指令字结构 + 按<strong>字节</strong>编址 + 指令字长=存储字长=16bit=2B：每次 PC + 2</li><li><strong>变长</strong>指令字结构 + 按<strong>字节</strong>编址 + 指令字长!=存储字长=16bit=2B：每次 PC + n<ul><li>根据指令的类型，CPU 可能还要进行多次访存，<strong>每次读入一个字</strong></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022130853433.png" alt="image-20211022130853433" style="zoom:80%;"></li></ul></li></ul></li></ul><h3 id="跳跃寻址"><a href="#跳跃寻址" class="headerlink" title="跳跃寻址"></a>跳跃寻址</h3><ul><li>跳跃寻址：由转移指令指出</li><li>CPU 与 PC 之间的顺序：<ul><li>CPU 取出 PC 中存储的指令</li><li>PC += 1</li><li>CPU 执行取出的指令<ul><li>如果是执行转移指令，则修改 PC 内容</li></ul></li><li>CPU 取出 PC 中存储的指令。Loop</li></ul></li><li>每一条指令的执行都分为 “取指令”、“执行指令” 两个阶段</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022131031340.png" alt="image-20211022131031340"></li></ul><h2 id="数据寻址"><a href="#数据寻址" class="headerlink" title="数据寻址"></a>数据寻址</h2><h3 id="指令寻址-v-s-数据寻址"><a href="#指令寻址-v-s-数据寻址" class="headerlink" title="指令寻址 v.s. 数据寻址"></a>指令寻址 v.s. 数据寻址</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022132436880.png" alt="image-20211022132436880"></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022132520543.png" alt="image-20211022132520543"></li><li>求出操作数的真实地址，称为有效地址（EA）。</li></ul><h3 id="直接与间接寻址"><a href="#直接与间接寻址" class="headerlink" title="直接与间接寻址"></a>直接与间接寻址</h3><h4 id="直接寻址"><a href="#直接寻址" class="headerlink" title="直接寻址"></a>直接寻址</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022133342005.png" alt="image-20211022133342005" style="zoom:80%;"></li><li>直接寻址：指令字中的<strong>形式地址 A 就是操作数的真实地址 EA(effective address)**，即 **EA=A</strong> 。</li><li>共访存 <strong>2</strong> 次：取指令访存1次；执行指令访存 1 次。</li><li>优点：<ul><li>简单，指令执行阶段仅访问一次主存，不需专门计算操作数的地址。</li></ul></li><li>缺点：<ul><li>A 的位数决定了该指令操作数的寻址范围。</li><li>操作数的地址不易修改。</li></ul></li></ul><h4 id="间接寻址"><a href="#间接寻址" class="headerlink" title="间接寻址"></a>间接寻址</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022134527227.png" alt="image-20211022134527227" style="zoom:80%;"></li><li>间接寻址：指令的地址字段给出的形式地址是<strong>操作数有效地址所在的存储单元的地址</strong>，也就是操作数地址的地址，即 <strong>EA=(A)</strong> 。</li><li>共访存 <strong>3</strong> 次：取指令访存 1 次；执行指令访存 2 次。</li><li>优点：<ul><li>可扩大寻址范围（有效地址 EA 的位数大于形式地址 A 的位数）。</li><li>便于编制程序（用间接寻址可以方便地完成子程序返回）。</li></ul></li><li>缺点：<ul><li>指令在执行阶段要多次访存（一次间址需两次访存，多次寻址需根据存储字的最高位确定几次访存）。</li></ul></li></ul><h4 id="寄存器寻址"><a href="#寄存器寻址" class="headerlink" title="寄存器寻址"></a>寄存器寻址</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022135027732.png" alt="image-20211022135027732" style="zoom:80%;"></li><li>寄存器寻址：在指令字中直接给出操作数所在的寄存器编号，即 **EA=R<del>i</del>**，其操作数在由 R<del>i</del> 所指的寄存器内。</li><li>共访存 <strong>1</strong> 次：取指令访存 1 次；执行指令访存 0 次。</li><li>优点：<ul><li>指令在<strong>执行阶段</strong>不访问主存，<strong>只访问寄存器</strong></li><li>指令字短且执行速度快，支持<strong>向量/矩阵运算</strong>。</li></ul></li><li>缺点：<ul><li>寄存器价格昂贵，计算机中寄存器个数有限 。</li></ul></li></ul><h4 id="寄存器间接寻址"><a href="#寄存器间接寻址" class="headerlink" title="寄存器间接寻址"></a>寄存器间接寻址</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022135538548.png" alt="image-20211022135538548" style="zoom:80%;"></li><li>寄存器间接寻址：寄存器 R<del>i</del> 中给出的不是一个操作数，而是<strong>操作数所在主存单元的地址</strong>，即 <strong>EA=(R<del>i</del>)</strong> 。</li><li>共访存 <strong>2</strong> 次：取指令访存 1 次；执行指令访存 1 次。</li><li>特点：<ul><li>与一般间接寻址相比速度更快，但指令的执行阶段需要访问主存（因为操作数在主存中）。</li></ul></li></ul><h4 id="隐含寻址"><a href="#隐含寻址" class="headerlink" title="隐含寻址"></a>隐含寻址</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022135948636.png" alt="image-20211022135948636" style="zoom:80%;"></li><li>隐含寻址：不是明显地给出操作数的地址，而是在指令中隐含着操作数的地址。</li><li>共访存 <strong>1</strong> 次：取指令访存 1 次；执行指令访存 0 次。</li><li>优点：有利于缩短指令字长。</li><li>缺点：需增加存储操作数或隐含地址的硬件。</li></ul><h4 id="立即寻址"><a href="#立即寻址" class="headerlink" title="立即寻址"></a>立即寻址</h4><ul><li>立即寻址：形式地址 A 就是<strong>操作数本身</strong>，又称为立即数，一般采用补码形式。“#”表示立即寻址特征。</li><li>共访存 <strong>1</strong> 次：取指令访存 1 次；执行指令访存 0 次。</li><li>优点：指令执行阶段不访问主存，<strong>指令执行时间最短</strong></li><li>缺点：A 的位数限制了立即数的范围。<ul><li>如 A 的位数为 n，且立即数采用补码时，可表示的数据范围为：−2^n-1^ ～ 2^n-1^−1</li></ul></li></ul><h3 id="偏移寻址"><a href="#偏移寻址" class="headerlink" title="偏移寻址"></a>偏移寻址</h3><h4 id="基址寻址（多道程序）"><a href="#基址寻址（多道程序）" class="headerlink" title="基址寻址（多道程序）"></a>基址寻址（多道程序）</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022142356006.png" alt="image-20211022142356006"></li><li>基址寻址：将 CPU 中基址寄存器（base address register，BR）的内容加上指令格式中的形式地址 A，而形成操作数的有效地址，即 <strong>EA=(BR)+A</strong>。<ul><li>操作系统第三章第一节学习，OS 课中的“重定位寄存器”就是“基址寄存器”</li><li>程序运行前，CPU 将 BR 的值修改为该程序的起始地址（存在操作系统 PCB 中）</li></ul></li><li>基址寄存器是<strong>面向操作系统</strong>的，<strong>其内容仅由操作系统或管理程序确定，对于程序员是透明的</strong>。<ul><li>基址寄存器的内容不变（作为基地址），形式地址可变（作为偏移量）。</li><li>当采用通用寄存器作为基址寄存器时，可由<strong>用户决定哪个寄存器作为基址寄存器</strong>，但其<strong>内容仍由操作系统确定</strong>。</li></ul></li><li>优点：<ul><li>便于程序在内存里的地址浮动，方便实现<strong>多道程序</strong>并发运行</li><li>可扩大寻址范围（基址寄存器的位数大于形式地址的位数）</li><li>用户<strong>不必考虑自己的程序存于主存的哪一空间区域</strong>，故<strong>有利于多道程序设计</strong></li></ul></li></ul><h4 id="变址寻址（循环程序）"><a href="#变址寻址（循环程序）" class="headerlink" title="变址寻址（循环程序）"></a>变址寻址（循环程序）</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022181842393.png" alt="image-20211022181842393"></li><li>变址寻址：有效地址 EA 等于指令字中的形式地址 A 与变址寄存器 IX 的内容相加之和，即 <strong>EA=(IX)+A</strong>。其中 <strong>IX 可为变址寄存器（专用）</strong>，也<strong>可用通用寄存器作为变址寄存器</strong>。</li><li>变址寄存器是<strong>面向用户</strong>的，<strong>变址寄存器的内容可由用户改变</strong><ul><li>基址寄存器的内容不变（作为偏移量），形式地址可变（作为基地址）。</li></ul></li><li>优点：<ul><li>在数组处理过程中，可设定 A 为数组的首地址，不断改变变址寄存器 IX(index register) 的内容，便可很容易地表示数组中任一数据的地址，<strong>适合编制循环程序</strong>。</li></ul></li><li>两者同样可以一起使用——基址&amp;变址复合寻址</li></ul><h4 id="相对寻址（转移指令）"><a href="#相对寻址（转移指令）" class="headerlink" title="相对寻址（转移指令）"></a>相对寻址（转移指令）</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part4/image-20211022185157913.png" alt="image-20211022185157913"></li><li>相对寻址：把程序计数器 PC 的内容加上指令格式中的形式地址 A 而形成操作数的有效地址，即 <strong>EA=(PC)+A</strong>。<ul><li><strong>A 是相对于下一条指令（PC 所指地址）的偏移量</strong>，可正可负，<strong>补码表示</strong> 。因为 PC 会在取出指令后立刻加一。</li></ul></li><li>优点：<ul><li>便于一段代码在程序内部的浮动。</li></ul></li><li>相对寻址广泛应<strong>用于转移指令</strong>。</li></ul><h3 id="堆栈寻址"><a href="#堆栈寻址" class="headerlink" title="堆栈寻址"></a>堆栈寻址</h3><h4 id="堆栈寻址-1"><a href="#堆栈寻址-1" class="headerlink" title="堆栈寻址"></a>堆栈寻址</h4><ul><li>堆栈寻址：操作数存放在堆栈中，隐含使用堆栈指针（Stack Pointer, SP）作为操作数地址。</li><li>堆栈是一块按“后进先出（LIFO）”原则管理的存储区，该存储区中被读/写单元的地址是用堆栈指针给出的。</li><li>堆栈可以由寄存器组（硬堆栈，成本高）或者主存中划分一块区域（软堆栈，成本低）实现。</li><li>有堆栈才可以方便地实现保护现场、调用函数。</li></ul><h3 id="数据寻址总结"><a href="#数据寻址总结" class="headerlink" title="数据寻址总结"></a>数据寻址总结</h3><table><thead><tr><th>寻址方式</th><th>有效地址</th><th>访存次数（指令执行期间）</th></tr></thead><tbody><tr><td>直接寻址</td><td>EA=A</td><td>1</td></tr><tr><td>一次间接寻址</td><td>EA=(A)</td><td>2</td></tr><tr><td>寄存器寻址</td><td>EA=R<del>i</del></td><td>0</td></tr><tr><td>寄存器间接一次寻址</td><td>EA=(R<del>i</del>)</td><td>1</td></tr><tr><td>隐含寻址</td><td>程序指定</td><td>0</td></tr><tr><td>立即寻址</td><td>A 即是操作数</td><td>0</td></tr><tr><td>基址寻址</td><td>EA=(BR)+A</td><td>1</td></tr><tr><td>变址寻址</td><td>EA=(IX)+A</td><td>1</td></tr><tr><td>相对寻址</td><td>EA=(PC)+A</td><td>1</td></tr><tr><td>堆栈寻址</td><td>入栈/出栈时 EA 的确定方式不同</td><td>硬堆栈不访存，软堆栈访存1次</td></tr></tbody></table><h2 id="CISC-和-RISC"><a href="#CISC-和-RISC" class="headerlink" title="CISC 和 RISC"></a>CISC 和 RISC</h2><ul><li>CISC：Complex Instruction Set Computer<ul><li>设计思路：一条指令完成一个复杂的基本功能。</li></ul></li><li>RISC：Reduced Instruction Set Computer<ul><li>设计思路：一条指令完成一个基本“动作”；多条指令组合完成一个复杂的基本功能。</li></ul></li></ul><table><thead><tr><th>对比项目</th><th>CISC</th><th>RISC</th></tr></thead><tbody><tr><td>指令系统</td><td>复杂</td><td>简单</td></tr><tr><td>代表</td><td>x86 架构</td><td>ARM 架构</td></tr><tr><td>指令数目</td><td>一般大于 200 条</td><td>一般小于 100 条</td></tr><tr><td>指令字长</td><td>不固定</td><td>定长</td></tr><tr><td>可访存指令</td><td>不加限制</td><td>有 Load/Store 指令</td></tr><tr><td>各指令执行时间</td><td>相差较大</td><td>大部分在一个周期内</td></tr><tr><td>各指令执行频率</td><td>相差较大</td><td>都比较常用</td></tr><tr><td>通用寄存器数量</td><td>较少</td><td>多</td></tr><tr><td>目标代码</td><td>难以用优化编译提高程序效率</td><td>可以用优化编译提高程序效率</td></tr><tr><td>控制方式</td><td>大部分为微程序控制（更慢）</td><td>大部分为组合逻辑控制（更快）</td></tr><tr><td>指令流水线</td><td>可以实现</td><td>必须实现</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">指令系统</summary>
    
    
    
    <category term="计算机组成原理" scheme="https://nephrencake.gitee.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机组成原理-Part3</title>
    <link href="https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/"/>
    <id>https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/</id>
    <published>2021-10-02T02:53:08.000Z</published>
    <updated>2021-10-26T15:16:29.302Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算机组成原理-Part3——存储系统"><a href="#计算机组成原理-Part3——存储系统" class="headerlink" title="计算机组成原理-Part3——存储系统"></a>计算机组成原理-Part3——存储系统</h1><p>[TOC]</p><h2 id="存储系统基本概念"><a href="#存储系统基本概念" class="headerlink" title="存储系统基本概念"></a>存储系统基本概念</h2><h3 id="存储器的层次结构"><a href="#存储器的层次结构" class="headerlink" title="存储器的层次结构"></a>存储器的层次结构</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211018200033029.png" alt="image-20211018200033029"></li><li>辅存中的数据要调入主存后才能被 CPU 访问</li><li>主存—辅存：实现虚拟存储系统，<strong>解决了主存容量不够的问题</strong></li><li>Cache—主存：<strong>解决了主存与 CPU 速度不匹配的问题</strong></li></ul><h3 id="存储器的分类"><a href="#存储器的分类" class="headerlink" title="存储器的分类"></a>存储器的分类</h3><ol><li>层次：<ul><li>高速缓存（Cache）</li><li>主存储器（主存、内存）</li><li>辅助存储器（辅存、外存）</li></ul></li><li>存储介质：<ul><li>半导体存储器（主存、Cache）</li><li>磁表面存储器（磁盘、磁带）</li><li>光存储器（光盘）</li></ul></li><li>存取方式：<ul><li><strong>随机存取存储器</strong>（Random Access Memory，RAM）（内存）</li><li>串行访问存储器：读写某个存储单元所需时间与存储单元的物理位置有关<ul><li>顺序存取存储器（Sequential Access Memory，SAM）（磁带）：读写一个存储单元所需时间取决于存储单元所在的物理位置</li><li>直接存取存储器（Direct Access Memory，DAM）（磁盘）：既有随机存取特性，也有顺序存取特性。先直接选取信息所在区域，然后按顺序方式存取。</li></ul></li><li>相联存储器（Associative Memory），即可按内容访问的存储器（Content Addressed Memory，CAM）：可以按照内容检索到存储位置进行读写，“快表”就是一种相联存储器。</li></ul></li><li>信息的可更改性：<ul><li>读写存储器（Read/Write Memory）：即可读、也可写（磁盘、内存、Cache）</li><li>只读存储器（Read Only Memory）：只能读，不能写（实体音乐专辑通常采用 CD-ROM、实体电影采用蓝光光碟、BIOS 通常写在 ROM 中）</li></ul></li><li>信息的可保存性：<ul><li>断电后，存储信息消失的存储器——易失性存储器（主存、Cache）</li><li>断电后，存储信息依然保持的存储器——非易失性存储器（磁盘、光盘）</li><li>信息读出后，原存储信息被破坏——破坏性读出（如 DRAM 芯片，读出数据后要进行重写）</li><li>信息读出后，原存储信息不被破坏——非破坏性读出（如 SRAM 芯片、磁盘、光盘）</li></ul></li></ol><h3 id="存储器的性能指标"><a href="#存储器的性能指标" class="headerlink" title="存储器的性能指标"></a>存储器的性能指标</h3><ol><li>存储容量：存储字数（MDR）×字长（MAR）。</li><li>单位成本：每位价格=总成本/总容量。（每个 bit 位的成本）</li><li>存储速度：数据传输率=数据的宽度/存储周期。<ul><li>数据的宽度=存储字长</li><li>数据传输率=主存带宽</li></ul></li></ol><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211018202338820.png" alt="image-20211018202338820"></p><ul><li><strong>存取时间</strong>（T<del>a</del>）：指从启动一次存储器操作到完成该操作所经历的时间，分为读出时间和写入时间。</li><li><strong>存取周期</strong>（T<del>m</del>）：又称<strong>读写周期</strong>或<strong>访问周期</strong>。它是指存储器进行一次完整的读写操作所需的全部时间，即连续两次独立地访问存储器操作（读或写操作）之间所需的最小时间间隔。<ul><li><strong>存取周期=存取时间+恢复时间</strong></li></ul></li><li><strong>主存带宽</strong>（B<del>m</del>）：又称<strong>数据传输率</strong>，表示每秒从主存进出信息的最大数量，单位为字/秒、字节/秒（B/s）或位/秒（b/s）。</li></ul><h2 id="主存储器的基本组成"><a href="#主存储器的基本组成" class="headerlink" title="主存储器的基本组成"></a>主存储器的基本组成</h2><h3 id="半导体元件的原理"><a href="#半导体元件的原理" class="headerlink" title="半导体元件的原理"></a>半导体元件的原理</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211018203602384.png" alt="image-20211018203602384"></li><li>MOS 管相当于一种电控开关，输入电压达到某个阈值时，MOS 管就可以接通</li><li>之所以是<strong>按存储字读写</strong>，是因为<strong>一个存储单元共用一条线来控制 MOS 管</strong>，只能同时读取或同时写入一个字</li></ul><h3 id="存储芯片的基本原理"><a href="#存储芯片的基本原理" class="headerlink" title="存储芯片的基本原理"></a>存储芯片的基本原理</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211018204801463.png" alt="image-20211018204801463"></li><li>头上划线表示该信号低电平有效</li><li>引脚与数据线<ul><li>地址线</li><li>数据线</li><li>片选线：一块内存条可能有多块存储芯片<ul><li>CS：Chip-Select；CE：Chip-Enable（两种表示方式皆可，当有上划线则表示低电平有效）</li><li>对选中存储芯片的片选线给予低电平，对未选中的给予高电平</li></ul></li><li>读写线<ul><li>当使用不同数量的读写线，则对外暴露的引脚是不一样的</li><li>使用 WE(Write Enable)、OE，则两根读写线</li><li>使用 WE或WR 则一根读写线（低电平写，高电平读）</li></ul></li></ul></li><li>译码器驱动电路：将地址信号转化成字选通线的高低电平</li><li>存储矩阵（存储体）：由多个存储单元构成，每个存储单元又由多个存储元构成</li></ul><h3 id="如何实现不同的寻址方式"><a href="#如何实现不同的寻址方式" class="headerlink" title="如何实现不同的寻址方式"></a>如何实现不同的寻址方式</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211018223923446.png" alt="image-20211018223923446"></li><li>一般以<strong>字节</strong>编址，即每个字节对应一个地址。同时也能够支持按半字、字、双字寻址</li><li>存储单元的长度并不影响按何种方式寻址，但是一次读写数据必须按字为单位</li><li>n 根地址线能够寻 2^n^ 个存储单元</li></ul><h2 id="SRAM-和-DRAM"><a href="#SRAM-和-DRAM" class="headerlink" title="SRAM 和 DRAM"></a>SRAM 和 DRAM</h2><ul><li>随机存取存储器（Random Access Memory，RAM）<ul><li>Dynamic Random Access Memory，即动态RAM</li><li>Static Random Access Memory，即静态RAM</li><li><strong>DRAM</strong> 用于<strong>主存</strong>；<strong>SRAM</strong> 用于 <strong>Cache</strong></li></ul></li><li>核心区别：<strong>存储元不一样</strong><ul><li>DRAM芯片：使用<strong>栅极电容</strong>存储信息</li><li>SRAM芯片：使用<strong>双稳态触发器</strong>存储信息</li></ul></li></ul><h3 id="存储元件不同导致的特性差异"><a href="#存储元件不同导致的特性差异" class="headerlink" title="存储元件不同导致的特性差异"></a>存储元件不同导致的特性差异</h3><h4 id="栅极电容"><a href="#栅极电容" class="headerlink" title="栅极电容"></a>栅极电容</h4><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211019205541116.png" alt="image-20211019205541116"></p><ul><li>存储状态<ul><li>1：电容内存储了电荷</li><li>0：电容内未存储电荷</li></ul></li><li>读出状态<ul><li>读出 1：MOS 管接通，电容放电，数据线上产生电流</li><li>读出 0：MOS 管接通后，数据线上无电流</li></ul></li><li>特点<ul><li>电容放电信息被破坏，是<strong>破坏性读出</strong>。所以，读出后应有<strong>重写</strong>操作，也称“再生”</li><li>读写速度<strong>更慢</strong></li><li><strong>制造成本更低，集成度高，功耗低</strong></li></ul></li></ul><h4 id="双稳态触发器"><a href="#双稳态触发器" class="headerlink" title="双稳态触发器"></a>双稳态触发器</h4><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211019205912531.png" alt="image-20211019205912531"></p><ul><li>存储状态<ul><li>1：A 高电平 B 低电平</li><li>0：A 低电平 B 高电平</li></ul></li><li>读出状态<ul><li>读出 0：BL 为低电平，BLX 无信号</li><li>读出 1：BLX 为低电平，BL 无信号</li></ul></li><li>特点<ul><li>读出数据，触发器状态保持稳定，是<strong>非破坏性读出</strong>，<strong>无需重写</strong></li><li>读写速度<strong>更快</strong></li><li><strong>制造成本更高，集成度低，功耗大</strong></li></ul></li></ul><h4 id="DRAM-和-SRAM-对比（高频）"><a href="#DRAM-和-SRAM-对比（高频）" class="headerlink" title="DRAM 和 SRAM 对比（高频）"></a>DRAM 和 SRAM 对比（高频）</h4><table><thead><tr><th>类型特点</th><th>SRAM（静态 RAM）</th><th>DRAM（动态 RAM）</th></tr></thead><tbody><tr><td>存储信息</td><td>触发器</td><td>电容</td></tr><tr><td>破坏性读出</td><td>否</td><td>是</td></tr><tr><td>读出后需要重写</td><td>否</td><td>是</td></tr><tr><td>运行速度</td><td>快</td><td>man</td></tr><tr><td>集成度</td><td>低</td><td>高</td></tr><tr><td>发热量</td><td>大</td><td>小</td></tr><tr><td>存储成本</td><td>高</td><td>低</td></tr><tr><td>易失/非易失性存储器</td><td>易失（断电后丢失信息）</td><td>易失（断电后丢失信息）</td></tr><tr><td>需要刷新</td><td>不需要</td><td>需要</td></tr><tr><td>送行列地址</td><td>同时送</td><td>分两次送</td></tr><tr><td>常用与制作</td><td>Cache</td><td>主存</td></tr></tbody></table><h3 id="DRAM-的刷新"><a href="#DRAM-的刷新" class="headerlink" title="DRAM 的刷新"></a>DRAM 的刷新</h3><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211019220838787.png" alt="image-20211019220838787"></p><ul><li><p>为什么刷新？</p><ul><li>双稳态（静态RAM，SRAM）只要不断电，触发器的状态就不会改变。所以<strong>不需要刷新</strong>。</li><li>栅极电容（动态RAM，DRAM）电容内的电荷只能维持 2ms。即便不断电，2ms 后信息也会消失。所以，2ms 之内必须“刷新”一次（给电容充电）</li></ul></li><li><p>多久需要刷新一次？刷新周期：若题目没有指明，则一般为 <strong>2ms</strong></p></li><li><p>每次刷新多少存储单元？以行为单位，<strong>每次刷新一行存储单元</strong></p><ul><li>为什么要用行列地址？<strong>减少选通线的数量</strong>。拆分为行列地址（<strong>DRAM行、列地址等长</strong>）</li></ul></li><li><p>如何刷新？有硬件支持，读出一行的信息后重新写入，<strong>占用1个读/写周期（存取周期）</strong></p></li><li><p>在什么时刻刷新？</p><ul><li>假设 DRAM 内部结构排列成 128×128 的形式，读/写周期 0.5us。2ms 共 <code>2ms / 0.5us = 4000</code> 个周期</li></ul></li><li><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211019215622587.png" alt="image-20211019215622587"></p></li><li><p>“刷新”由存储器独立完成，不需要 CPU 控制。</p></li></ul><h3 id="DRAM-的地址线复用技术"><a href="#DRAM-的地址线复用技术" class="headerlink" title="DRAM 的地址线复用技术"></a>DRAM 的地址线复用技术</h3><ul><li>SRAM 同时送行列地址；DRAM 分两次送行列地址。</li><li>同时送指的是将地址的前后两部分同时送给行地址译码器和列地址译码器。这样的话，<strong>地址有 n 位，就要设计 n 根地址线</strong></li><li>但是 DRAM 的存储容量较大。所以将<strong>地址线、引脚减半</strong>，先将<strong>前半部分送入行地址缓冲器</strong>，再将<strong>后半部分送入列地址缓冲器</strong></li></ul><h2 id="只读存储器ROM"><a href="#只读存储器ROM" class="headerlink" title="只读存储器ROM"></a>只读存储器ROM</h2><h3 id="各种-ROM"><a href="#各种-ROM" class="headerlink" title="各种 ROM"></a>各种 ROM</h3><ol><li><strong>MROM（Mask Read-Only Memory）</strong>——掩模式只读存储器<ul><li>厂家按照客户需求，在芯片生产过程中直接写入信息，之后<strong>任何人不可重写</strong>（只能读出）</li><li>可靠性高、灵活性差、生产周期长、只适合批量定制</li></ul></li><li><strong>PROM（Programmable Read-Only Memory）</strong>——可编程只读存储器<ul><li>用户可用专门的PROM写入器写入信息，<strong>写一次之后就不可更改</strong></li></ul></li><li><strong>EPROM（Erasable Programmable Read-Only Memory）</strong>——可擦除可编程只读存储器<ul><li>允许用户写入信息，之后用某种方法擦除数据，<strong>修改次数有限，写入时间很长</strong></li><li>UVEPROM（ultraviolet rays）——用紫外线照射8~20分钟，<strong>擦除所有信息</strong></li><li>EEPROM（也常记为E2 PROM，第一个E是Electrically）——可用“电擦除”的方式，<strong>擦除特定的字</strong></li></ul></li><li><strong>Flash Memory</strong> ——闪速存储器（注：U盘、SD卡就是闪存）<ul><li>在EEPROM 基础上发展而来，断电后也能保存信息，且<strong>可进行多次快速擦除重写</strong></li><li>注意：由于闪存需要先擦除在写入，因此<strong>闪存的“写”速度要比“读”速度更慢</strong>。</li><li>每个存储元只需单个MOS管，位密度比RAM高</li></ul></li><li><strong>SSD（Solid State Drives）</strong>—— 固态硬盘<ul><li>由控制单元+存储单元（Flash 芯片）构成，与闪速存储器的核心区别在于控制单元不一样，但存储介质都类似，<strong>可进行多次快速擦除重写</strong>。</li><li>SSD速度快、功耗低、价格高。</li><li>手机辅存也使用Flash 芯片，但相比SSD使用的芯片集成度高、功耗低、价格贵</li></ul></li></ol><ul><li>很多 ROM 芯片虽然名字是“Read-Only”，但<strong>很多 ROM 也可以“写”</strong></li><li>RAM（Random Access Memory）芯片是易失性的，ROM（Read-Only Memory）芯片是非易失性的。<strong>很多 ROM 也具有“随机存取”的特性</strong></li></ul><h3 id="计算机内的重要-ROM"><a href="#计算机内的重要-ROM" class="headerlink" title="计算机内的重要 ROM"></a>计算机内的重要 ROM</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211019223754947.png" alt="image-20211019223754947"></li><li>针对问题<ul><li>断电后，RAM内数据全部丢失</li><li>操作系统安装在辅存</li></ul></li><li><strong>主板上的</strong> BIOS 芯片（ROM），存储了“自举装入程序”，负责引导装入操作系统（开机）</li><li>逻辑上，<strong>主存由 RAM+ROM 组成</strong>，且二者常统一编址</li><li>我们常说“内存条”就是“主存”，但事实上，主板上的 ROM 芯片也是“主存”的一部分</li></ul><h2 id="主存储器与-CPU-的连接"><a href="#主存储器与-CPU-的连接" class="headerlink" title="主存储器与 CPU 的连接"></a>主存储器与 CPU 的连接</h2><h3 id="单块存储芯片与-CPU-的连接"><a href="#单块存储芯片与-CPU-的连接" class="headerlink" title="单块存储芯片与 CPU 的连接"></a>单块存储芯片与 CPU 的连接</h3><ul><li><p>针对问题：想要扩展主存<strong>字数</strong></p><ul><li>改进方向：<strong>字扩展</strong></li><li>连接多块存储芯片来扩展主存字数</li></ul></li><li><p>针对问题：想要扩展<strong>存储字长</strong></p><ul><li>改进方向：<strong>位扩展</strong></li><li>一个存储字长是 8bit，而数据总线一般有 64bit</li><li>为了尽可能发挥数据总线的性能，要保持<strong>数据总线宽度=存储字长</strong>，提高一次读写的数据量</li></ul></li><li><p>现在 MDR 和 MAR 常集成于 CPU 内部，存储芯片内只需要一个普通的寄存器（暂存输入输出数据）</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020121325377.png" alt="image-20211020121325377" style="zoom:80%;"></li><li>CPU 和主存之间通过<strong>数据总线</strong>、<strong>地址总线</strong>、<strong>控制总线</strong>连接</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020123151574.png" alt="image-20211020123151574" style="zoom:80%;"></li><li>对外暴露的引脚：地址线+数据线+片选线+读写控制线</li></ul></li></ul><h3 id="多块存储芯片与-CPU-的连接"><a href="#多块存储芯片与-CPU-的连接" class="headerlink" title="多块存储芯片与 CPU 的连接"></a>多块存储芯片与 CPU 的连接</h3><h4 id="位扩展法"><a href="#位扩展法" class="headerlink" title="位扩展法"></a>位扩展法</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020130901005.png" alt="image-20211020130901005" style="zoom:80%;"></li><li>将 <strong>n</strong> 片 <strong>k×m</strong> 位的存储芯片，集合成 <strong>k×(m×n)</strong> 的芯片</li></ul><h4 id="字扩展法"><a href="#字扩展法" class="headerlink" title="字扩展法"></a>字扩展法</h4><h5 id="线选法"><a href="#线选法" class="headerlink" title="线选法"></a>线选法</h5><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020131651225.png" alt="image-20211020131651225" style="zoom:80%;"></li><li><strong>n</strong> 条线可提供 <strong>n</strong> 个片选信号</li><li>将 <strong>n</strong> 片 <strong>k×m</strong> 位的存储芯片，集合成 <strong>(n×k)×m</strong> 的芯片</li></ul><h5 id="片选法"><a href="#片选法" class="headerlink" title="片选法"></a>片选法</h5><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020131755343.png" alt="image-20211020131755343"></li><li><strong>n</strong> 条线可提供 <strong>2^n^</strong> 个片选信号</li><li>将 <strong>n</strong> 片 <strong>k×m</strong> 位的存储芯片，集合成 <strong>(n^2^×k)×m</strong> 的芯片</li><li>要注意译码器的接线，若 A<del>15</del> 无接线，则 A<del>15</del> 取值与地址无关</li></ul><table><thead><tr><th>线选法</th><th>译码片选法</th></tr></thead><tbody><tr><td>n 条线 -&gt; n 个片选信号</td><td>n 条线 -&gt; 2^n^ 个片选信号</td></tr><tr><td>电路简单</td><td>电路复杂</td></tr><tr><td>地址空间不连续</td><td>地址空间连续</td></tr></tbody></table><h4 id="字位扩展法"><a href="#字位扩展法" class="headerlink" title="字位扩展法"></a>字位扩展法</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020133531141.png" alt="image-20211020133531141"></li><li>同时扩展字位</li></ul><h3 id="译码器知识补充"><a href="#译码器知识补充" class="headerlink" title="译码器知识补充"></a>译码器知识补充</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020135655208.png" alt="image-20211020135655208" style="zoom:80%;"></li><li>低电平有效还是高电平有效，在译码器与存储芯片之间是相互协调同步的，<strong>都带圈或都不带圈</strong>。带圈则低电平，否则高电平。</li><li>译码器会有<strong>一个或者多个</strong>使能端，带圈则低电平，否则高电平。<ul><li>CPU 可使用译码器的使能端控制片选信号的生效时间</li><li>保证地址的信号稳定之后，再由 MREQ 发出存储器请求信号，然后译码器发出片选信号</li></ul></li><li>RAM的读周期<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020135621066.png" alt="image-20211020135621066"></li></ul></li></ul><h2 id="双端口-RAM-和多模块存储器"><a href="#双端口-RAM-和多模块存储器" class="headerlink" title="双端口 RAM 和多模块存储器"></a>双端口 RAM 和多模块存储器</h2><h3 id="存取周期"><a href="#存取周期" class="headerlink" title="存取周期"></a>存取周期</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020140333261.png" alt="image-20211020140333261"></li><li>存取周期：可以连续读/写的最短时间间隔<ul><li><strong>存取周期T = 存取时间r + 恢复时间</strong></li></ul></li><li>DRAM 芯片的恢复时间比较长，有可能是存取时间的几倍（SRAM的恢复时间较短）<ul><li>如：存取时间为 r，存取周期为 T，T=4r</li><li>针对问题：多核CPU都要访存；CPU 读写速度比主存快很多，而主存恢复时间太长</li></ul></li></ul><h3 id="双端口-RAM"><a href="#双端口-RAM" class="headerlink" title="双端口 RAM"></a>双端口 RAM</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020140651218.png" alt="image-20211020140651218"></li><li>作用：<strong>优化多核 CPU 访问一根内存条的速度</strong></li><li><strong>需要有两组完全独立的数据线、地址线、控制线</strong>。</li><li><strong>CPU、RAM 中控制电路也更复杂</strong>。</li><li>两个端口对同一主存操作有以下 4 种情况：<ol><li>两个端口同时对不同的地址单元存取数据。 </li><li>两个端口同时对同一地址单元读出数据。 </li><li>两个端口同时对同一地址单元写入数据。 （禁止）</li><li>两个端口同时对同一地址单元，一个写入数据，另一个读出数据。（禁止）</li></ol></li><li>置“busy”信号为 0，由判断逻辑决定暂时关闭一个端口（即被延时）</li></ul><h3 id="多模块存储器"><a href="#多模块存储器" class="headerlink" title="多模块存储器"></a>多模块存储器</h3><h4 id="多体并行存储器"><a href="#多体并行存储器" class="headerlink" title="多体并行存储器"></a>多体并行存储器</h4><ul><li><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020151139894.png" alt="image-20211020151139894"></p></li><li><p><strong>高位交叉编址</strong>：<strong>更慢</strong>；<strong>低位交叉编址</strong>：<strong>更快</strong></p></li><li><p>为什么要探讨“连续访问”的情况？诸如数组、程序代码，都是在连续空间中存储的。</p></li><li><p>低位交叉编址</p><ul><li>采用“流水线”的方式并行存取（宏观上并行访问 m 个模块，微观上串行访问 m 个模块）</li><li>宏观上，一个存储周期内，m 体交叉存储器可以提供的数据量为单个模块的 m 倍。</li></ul></li><li><p>应该取多少个存储体？</p><ul><li>存取周期为 T，<strong>存取时间</strong>（<strong>总线传输周期</strong>）为 r，为了使流水线不间断，应保证模块数 <strong>m≥T/r</strong></li><li>存取时间：<strong>存储体的性能瓶颈</strong>。总线传输周期：<strong>将数据传给 CPU 所需要的时间</strong>。<ul><li>虽然意义不同，但<strong>都代表了 CPU 存取一次的时间都不可能低于 r</strong></li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020151753136.png" alt="image-20211020151753136"></li></ul></li><li><p>通过地址 x，确定为第几个存储体</p><ul><li>通过二进制 体内地址+体号 中的<strong>体号</strong>确定</li><li>通过十进制对地址取余 <strong>x%m</strong></li></ul></li></ul><h4 id="单体多字存储器"><a href="#单体多字存储器" class="headerlink" title="单体多字存储器"></a>单体多字存储器</h4><ul><li>每次并行读出 m 个连续的<strong>字</strong></li><li>总线宽度也要扩展成 m</li><li>速度效率因为是一次读出 m 个字，所以跟多体并行存储器是差不多的<ul><li>但是相比多体并行存储器，单体多字存储器更加不灵活</li></ul></li></ul><h2 id="Cache-的基本概念和原理（高频）"><a href="#Cache-的基本概念和原理（高频）" class="headerlink" title="Cache 的基本概念和原理（高频）"></a>Cache 的基本概念和原理（高频）</h2><h3 id="高速缓冲存储器-Cache"><a href="#高速缓冲存储器-Cache" class="headerlink" title="高速缓冲存储器 Cache"></a>高速缓冲存储器 Cache</h3><ul><li>针对问题：双端口 RAM、多模块存储器<strong>提高存储器的工作速度</strong>，但<strong>与 CPU 差距依然很大</strong></li><li>改进方向：更高速的存储单元 <strong>Cache</strong> 和存储体系的改善“<strong>Cache-主存</strong>”层次</li><li>依据：程序访问的<strong>局部性原理</strong></li><li>Cache 的工作原理：<ul><li><strong>Cache 被集成在CPU内部</strong>，Cache <strong>用 SRAM 实现</strong>，速度快，成本高。</li><li>将可能会被经常用到的代码/数据<strong>复制</strong>一份到 Cache</li></ul></li></ul><h3 id="局部性原理"><a href="#局部性原理" class="headerlink" title="局部性原理"></a>局部性原理</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020221430934.png" alt="image-20211020221430934"></li><li>空间局部性：在最近的未来要用到的信息(指令和数据)，很可能与现在正在使用的信息在<strong>存储空间</strong>上是邻近的<ul><li>Eg：数组元素、顺序执行的指令代码</li></ul></li><li>时间局部性：在最近的未来要用到的信息，很可能是现在<strong>正在使用的信息</strong><ul><li>Eg：循环结构的指令代码</li></ul></li><li>基于局部性原理，可以把 CPU 目前访问的地址“周围”的部分数据放到 Cache 中</li><li>程序 B 按“列优先”访问二维数组，空间局部性更差，更难利用 Cache 的优势</li></ul><h3 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h3><ul><li>设 t<del>c</del> 为访问一次 Cache 所需时间，t<del>m</del> 为访问一次主存所需时间</li><li><strong>命中率</strong> H：CPU 欲访问的信息已在 Cache 中存在的比率</li><li><strong>缺失（未命中）率</strong> M：1 - H</li><li>“Cache—主存”系统的<strong>平均访问时间</strong> t 为：<ul><li>先访问 Cache，若 Cache 未命中再访问主存：t = H * t<del>c</del> + (1 - H) * (t<del>c</del> + t<del>m</del>)</li><li>同时访问 Cache 和主存，若 Cache 命中则立即停止访问主存：t = H * t<del>c</del> + (1 - H) * t<del>m</del> </li></ul></li></ul><h3 id="存储空间分块"><a href="#存储空间分块" class="headerlink" title="存储空间分块"></a>存储空间分块</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211020223722710.png" alt="image-20211020223722710"></li><li>主存与 Cache 之间<strong>以“块”为单位</strong>进行数据交换。</li><li>主存地址可以看作为：<strong>块号 + 块内地址</strong></li><li><strong>操作系统</strong>中，通常将主存中的“一个<strong>块</strong>”也称为“一个<strong>页/页面/页框</strong>”</li><li>Cache 中的“<strong>块</strong>”也称为“<strong>行</strong>” </li><li><strong>每次被访问的主存块，一定会被立即调入 Cache</strong>。</li><li><strong>Cache 的块和主存的块大小相等</strong>。</li></ul><h2 id="Cache-和主存的映射方式"><a href="#Cache-和主存的映射方式" class="headerlink" title="Cache 和主存的映射方式"></a>Cache 和主存的映射方式</h2><h3 id="Cache-中存储的信息"><a href="#Cache-中存储的信息" class="headerlink" title="Cache 中存储的信息"></a>Cache 中存储的信息</h3><ul><li>针对问题：<strong>如何确定 Cache 与主存的数据块对应关系</strong></li><li>Cache 中存储的信息：<strong>有效位（0/1标记是否有效）</strong>+ <strong>标记（来自主存的块号）</strong>+ 块数据</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021090740397.png" alt="image-20211021090740397"></li></ul><h3 id="全相联映射"><a href="#全相联映射" class="headerlink" title="全相联映射"></a>全相联映射</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021092031684.png" alt="image-20211021092031684"></li><li>映射规则：主存块可以放在 Cache 的<strong>任意位置</strong> </li><li>优点：<strong>Cache 存储空间利用充分，命中率高</strong></li><li>缺点：<strong>查找标记慢，可能要遍历所有行</strong></li><li>CPU 访存：<ul><li>将想要访问的主存地址的<strong>前 22 位</strong>主存块号，对比 Cache 中所有块的<strong>标记</strong>；</li><li>若标记匹配且有效位=1，则 Cache 命中，访问块内地址为 001110 的单元。</li><li>若未命中或有效位=0，则正常访问主存</li></ul></li></ul><h3 id="直接映射"><a href="#直接映射" class="headerlink" title="直接映射"></a>直接映射</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021093735633.png" alt="image-20211021093735633"></li><li>映射规则：每个主存块只能放到一个特定的位置：<strong>Cache块号 = 主存块号 % Cache总块数</strong> </li><li>优点：对于任意地址<strong>只需要比对一遍标记，速度最快</strong></li><li>缺点：即使其他地方有空闲 Cache 块，但是不一定能使用。存储空间利用不充分，命中率低</li><li>优化标记：<ul><li>若 <strong>Cache总块数=2^n^</strong> 则主存块号末尾 n 位<strong>直接反映它在 Cache 中的位置</strong></li><li>其主存块号的其余位即为标记</li></ul></li><li>CPU 访存：<ul><li>根据主存块号的<strong>后 3 位确定 Cache 行</strong></li><li>若主存块号的<strong>前 19 位与 Cache 标记匹配</strong>且有效位=1，则 Cache 命中，访问块内地址为 001110 的单元。</li><li>若未命中或有效位=0，则正常访问主存</li></ul></li></ul><h3 id="组相连映射"><a href="#组相连映射" class="headerlink" title="组相连映射"></a>组相连映射</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021094545290.png" alt="image-20211021094545290"></li><li>映射规则：Cache 块分为若干组，每个主存块可放到特定分组中的任意一个位置：<strong>组号 = 主存块号 % 分组数</strong> </li><li>n 路组相联映射——每 n 个Cache 行为一组</li><li>优点：是前两种的折中方式</li><li>CPU 访存：<ul><li>根据主存块号的<strong>后 2 位确定所属分组号</strong></li><li>若主存块号的<strong>前 20 位与分组内的某个标记匹配</strong>且有效位=1， 则 Cache 命中，访问块内地址为 001110 的单元。</li><li>若未命中或有效位=0，则正常访问主存</li></ul></li></ul><h2 id="Cache-替换算法"><a href="#Cache-替换算法" class="headerlink" title="Cache 替换算法"></a>Cache 替换算法</h2><ul><li><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021204805492.png" alt="image-20211021204805492"></p></li><li><p>针对问题：Cache 很小，主存很大。如何替换满 Cache 的块数据</p></li><li><p><strong>抖动现象</strong>：<strong>频繁的换入换出现象（刚被替换的块很快又被调入）</strong></p></li><li><p><strong>随机算法</strong>（<strong>RAND</strong>, Random）—— 若 Cache 已满，则随机选择一块替换。</p><ul><li>评价：：实现简单，但完全没考虑局部性原理，命中率低，实际效果很不稳定</li></ul></li><li><p><strong>先进先出算法</strong>（<strong>FIFO</strong>, First In First Out）—— 若 Cache 已满，则替换最先被调入 Cache 的块</p><ul><li>评价：：实现简单，依然没考虑局部性原理，最先被调入 Cache 的块也有可能是被频繁访问的</li></ul></li><li><p><strong>近期最少使用算法</strong>（<strong>LRU</strong>, Least Recently Used ）—— 为每一个 Cache 块设置一个“<strong>计数器</strong>”，用于记录每个 Cache 块已经有多久没被访问了。当 Cache 满后<strong>替换“计数器”最大的</strong></p><ul><li>机器实现：<ul><li>命中时，<strong>所命中的行的计数器清零</strong>，<strong>比其低的计数器加1，比其高的不变</strong>；</li><li>未命中且还有空闲行时，新装入的行的计数器置0，其余非空闲行全加1；</li><li>未命中且无空闲行时，计数值最大的行的信息块被淘汰，新装行的块的计数器置0，其余全加1。</li></ul></li><li><strong>刷题策略</strong>：当需要确定替换谁时，<strong>在“依次访问主存块”列表向前数 n 位，最后一个即为淘汰项</strong></li><li><strong>Cache 块的总数=2^n^，则计数器只需 n 位</strong>。且 Cache 装满后所有计数器的值一定不重复</li><li>评价：<ul><li>基于“局部性原理”，近期被访问过的主存块，在不久的将来也很有可能被再次访问，因此淘汰最久没被访问过的块是合理的。LRU 算法的<strong>实际运行效果优秀，Cache 命中率高</strong>。</li><li>但若<strong>被频繁访问的主存块数量 &gt; Cache 行的数量</strong>，则有可能发生“抖动”，如：{1,2,3,4,5,1,2,3,4,5,1,2…}</li></ul></li></ul></li><li><p><strong>最不经常使用算法</strong>（<strong>LFU</strong>, Least Frequently Used ）—— 为每一个 Cache 块设置一个“<strong>计数器</strong>”，用于记录每个 Cache 块被访问过几次。当 Cache 满后<strong>替换“计数器”最小的</strong></p><ul><li>若有多个计数器最小的行，可按行号递增、或 FIFO 策略进行选择</li><li>计数器范围可能会很大</li><li>评价：曾经被经常访问的主存块<strong>在未来不一定会用到</strong>（如：微信视频聊天相关的块），并没有很好地遵循局部性原理，因此<strong>实际运行效果不如 LRU</strong></li></ul></li><li><p>最常考察的是近期最少使用算法 LRU，同时也不太会考察计数器，掌握刷题策略即可</p></li></ul><h2 id="Cache-写策略"><a href="#Cache-写策略" class="headerlink" title="Cache 写策略"></a>Cache 写策略</h2><ul><li>针对问题：CPU 修改了 Cache 中的数据副本，如何确保主存中数据母本的一致性？</li><li>读操作不会导致数据不一致性，所以只需要讨论写操作</li></ul><h3 id="写命中"><a href="#写命中" class="headerlink" title="写命中"></a>写命中</h3><h4 id="全写法"><a href="#全写法" class="headerlink" title="全写法"></a>全写法</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021212957934.png" alt="image-20211021212957934"></li><li>全写法(写直通法，write-through) —— 当CPU对Cache写命中时，必须把数据同时写入 Cache 和主存，一般使用写缓冲(write buffer)</li><li><strong>访存次数增加，速度变慢</strong>，但<strong>更能保证数据一致性</strong>。</li><li>使用写缓冲时：CPU 写的速度很快，若写操作不频繁，则效果很好。若写操作很频繁，可能会因为写缓冲饱和而发生阻塞。</li></ul><h4 id="写回法"><a href="#写回法" class="headerlink" title="写回法"></a>写回法</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021212704830.png" alt="image-20211021212704830"></li><li>写回法(write-back) —— 当 CPU 对 Cache 写命中时，<strong>只修改 Cache 的内容，而不立即写入主存，只有当此块被换出时才写回主存</strong> </li><li>要新增一个<strong>脏位</strong>，表示是否经过修改</li><li><strong>减少了访存次数</strong>，但<strong>存在数据不一致的隐患</strong>。</li></ul><h3 id="写不命中"><a href="#写不命中" class="headerlink" title="写不命中"></a>写不命中</h3><h4 id="写分配法"><a href="#写分配法" class="headerlink" title="写分配法"></a>写分配法</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021213711247.png" alt="image-20211021213711247"></li><li>写分配法(write-allocate)——当 CPU 对 Cache 写不命中时，把主存中的块调入 Cache，在 Cache 中修改。<strong>通常搭配写回法使用</strong>。</li></ul><h4 id="非写分配法"><a href="#非写分配法" class="headerlink" title="非写分配法"></a>非写分配法</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021213515741.png" alt="image-20211021213515741"></li><li>非写分配法(not-write-allocate)——当 CPU 对 Cache <strong>写不命中时</strong>只写入主存，不调入Cache。<strong>搭配全写法使用</strong>。<ul><li>全写法—— 当 CPU 对 Cache <strong>写命中时</strong>，必须把数据同时写入Cache和主存，一般使用写缓冲</li><li>只有“读”未命中时才会调入 Cache</li></ul></li></ul><h3 id="多级-Cache"><a href="#多级-Cache" class="headerlink" title="多级 Cache"></a>多级 Cache</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021213826421.png" alt="image-20211021213826421"></li><li>各级 Cache 之间常采用“全写法+非写分配法”。保证数据一致性。</li><li>Cache-主存 之间常采用“写回法+写分配法”。减少访存次数。</li></ul><h2 id="页式存储器"><a href="#页式存储器" class="headerlink" title="页式存储器"></a>页式存储器</h2><h3 id="页式存储"><a href="#页式存储" class="headerlink" title="页式存储"></a>页式存储</h3><ul><li>页式存储系统：<ul><li>一个程序(进程)<strong>在逻辑上被分为若干个大小相等的页面</strong>；</li><li><strong>页面大小与块的大小相同</strong>；</li><li><strong>每个页面可以离散地放入不同的主存块中</strong>，从而提高主存的利用率。</li></ul></li><li>CPU 执行的机器指令中，使用的是“逻辑地址”，因此<strong>需要通过“页表”将逻辑地址转为物理地址</strong>。<ul><li>逻辑地址（虚地址）：程序员视角看到的地址<ul><li>逻辑地址= 逻辑页号+页内地址（虚地址=虚页号+页内地址）</li></ul></li><li>物理地址（实地址）：实际在主存中的地址<ul><li>物理地址= 主存块号+页内地址（实地址=实页号+页内地址）</li></ul></li><li>页表的作用：记录了每个逻辑页面存放在哪个主存块中</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021220933259.png" alt="image-20211021220933259"></li></ul></li></ul><h3 id="地址变换过程"><a href="#地址变换过程" class="headerlink" title="地址变换过程"></a>地址变换过程</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021221504428.png" alt="image-20211021221504428"></li><li>快表是一种“相联存储器”，可以按内容寻访</li><li>快表中存储的是页表项的副本；Cache中存储的是主存块的副本。</li></ul><h2 id="虚拟存储器"><a href="#虚拟存储器" class="headerlink" title="虚拟存储器"></a>虚拟存储器</h2><h3 id="页式虚拟存储器"><a href="#页式虚拟存储器" class="headerlink" title="页式虚拟存储器"></a>页式虚拟存储器</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021231033684.png" alt="image-20211021231033684"></li><li>有效位：这个页面是否已调入主存</li><li>脏位：这个页面是否被修改过</li><li>引用位：用于“页面置换算法”，比如，可以用来统计这个页面被访问过多少次</li><li>物理页：即主存块号</li><li>磁盘地址：即这个页面的数据在磁盘中的存放位置</li></ul><h3 id="段-页-式虚拟存储器"><a href="#段-页-式虚拟存储器" class="headerlink" title="段(页)式虚拟存储器"></a>段(页)式虚拟存储器</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part3/image-20211021231601239.png" alt="image-20211021231601239"></li><li>页式虚拟存储器——拆分成大小相等的页面</li><li>段式虚拟存储器——按照功能模块拆分。如：#0 段是自己的代码，#1 段是库函数代码，#2 段是变量</li><li>段页式虚拟存储器——把程序按逻辑结构分段，每段再划分为固定大小的页，主存空间也划分为大小相等的页。<ul><li>程序对主存的调入、调出仍以页为基本传送单位。</li><li><strong>每个程序对应一个段表</strong>，每段对应一个页表。</li><li>虚拟地址：段号+段内页号+页内地址</li></ul></li></ul>]]></content>
    
    
    <summary type="html">存储系统</summary>
    
    
    
    <category term="计算机组成原理" scheme="https://nephrencake.gitee.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机组成原理-Part2</title>
    <link href="https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/"/>
    <id>https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/</id>
    <published>2021-10-02T02:53:03.000Z</published>
    <updated>2021-10-26T15:16:36.936Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算机组成原理-Part2——数据的表示和运算"><a href="#计算机组成原理-Part2——数据的表示和运算" class="headerlink" title="计算机组成原理-Part2——数据的表示和运算"></a>计算机组成原理-Part2——数据的表示和运算</h1><p>[TOC]</p><h2 id="数制与编码"><a href="#数制与编码" class="headerlink" title="数制与编码"></a>数制与编码</h2><h3 id="进位计数制及其相互转换"><a href="#进位计数制及其相互转换" class="headerlink" title="进位计数制及其相互转换"></a>进位计数制及其相互转换</h3><h4 id="r-进制计数法"><a href="#r-进制计数法" class="headerlink" title="r 进制计数法"></a>r 进制计数法</h4><ul><li><strong>基数</strong>：每个数码位所用到的不同符号的个数，r 进制的基数为 r</li><li><strong>位权</strong>：处于第 i 位的权重，值为 r^i^</li><li>每个位上的取值范围：0 ~ r-1</li></ul><h4 id="其他进制-gt-十进制"><a href="#其他进制-gt-十进制" class="headerlink" title="其他进制 -&gt; 十进制"></a>其他进制 -&gt; 十进制</h4><ul><li>r 进制数：K<del>n</del>K<del>n-1</del>……K<del>1</del>K<del>0</del>K<del>-1</del>……K<del>-m</del></li><li>十进制：  K<del>n</del> × r^n^ + K<del>n-1</del> × r^n-1^ + …… + K<del>1</del> × r^1^ + K<del>0</del> × r^0^ + K<del>-1</del> × r^-1^ + …… + K<del>-m</del> × r^m^ </li></ul><table><thead><tr><th>2^12^</th><th>2^11^</th><th>2^10^</th><th>2^9^</th><th>2^8^</th><th>2^7^</th><th>2^6^</th><th>2^5^</th><th>2^4^</th><th>2^3^</th><th>2^2^</th><th>2^1^</th><th>2^0^</th><th>2^-1^</th><th>2^-2^</th><th>2^-3^</th></tr></thead><tbody><tr><td>4096</td><td>2048</td><td>1024</td><td>512</td><td>256</td><td>128</td><td>64</td><td>32</td><td>16</td><td>8</td><td>4</td><td>2</td><td>1</td><td>0.5</td><td>0.25</td><td>0.125</td></tr></tbody></table><h4 id="二、八、十六进制之间相互转换"><a href="#二、八、十六进制之间相互转换" class="headerlink" title="二、八、十六进制之间相互转换"></a>二、八、十六进制之间相互转换</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211003084155324.png" alt="image-20211003084155324"></li><li>各种进制的常见书写方式<ul><li>二进制（Binary）：(1010001010010)<del>2</del>    或    1010001010010B</li><li>八进制：(1652)<del>8</del></li><li>十六进制（hex）：(1652)<del>16</del>    或     1652H    或    0x1652</li><li>十进制（dec）：(1652)<del>10</del>    或    1652D</li></ul></li><li>注意需要补位：整数向前补 0，小数向后补 0。</li></ul><h4 id="十进制-gt-其他进制"><a href="#十进制-gt-其他进制" class="headerlink" title="十进制 -&gt; 其他进制"></a>十进制 -&gt; 其他进制</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211003085557768.png" alt="image-20211003085557768"></li><li>整数部分除法：先除得的余数为低位（靠近0）</li><li>小数部分乘法：先进位的整数为高位（靠近0）</li><li>也可以使用拼凑法：枚举 r 进制数与十进制数的对应表</li><li>有的十进制小数无法使用二进制精确表示，如：0.3</li></ul><h4 id="真值和机器数"><a href="#真值和机器数" class="headerlink" title="真值和机器数"></a>真值和机器数</h4><ul><li><strong>真值</strong>：符合人类习惯的数字</li><li><strong>机器数</strong>：数字实际存到机器里的形式，正负号需要被“数字化”</li></ul><h3 id="BCD码（408大纲已删）"><a href="#BCD码（408大纲已删）" class="headerlink" title="BCD码（408大纲已删）"></a>BCD码（408大纲已删）</h3><ul><li>BCD ：Binary-Coded Decimal，用二进制编码的十进制<ul><li>8421 码（掌握加法）</li><li>余 3 码</li><li>2421 码</li></ul></li><li>针对问题：二进制方便计算机处理、十进制符合人类习惯，但是转换麻烦</li><li>改进方向：快速转换，一一对应</li><li><strong>以 4bit 二进制码表示 0~9</strong>。一共 16 种情况，6 种冗余。</li></ul><h4 id="8421-码"><a href="#8421-码" class="headerlink" title="8421 码"></a>8421 码</h4><ul><li>4 位二进制权值分别为：8 4 2 1</li><li>8421 码是有权码</li><li>8421 码的加法：<strong>先用十进制加法得出结果，再转 8421 码</strong></li><li>8421码中，1010～1111 没有定义</li><li>在机器的视角中，若发现结果<strong>非法定义</strong>（6+7）或<strong>出现进位</strong>（9+9），则需要继续 +6 进行修正。</li></ul><table><thead><tr><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>0000</td><td>0001</td><td>0010</td><td>0011</td><td>0100</td><td>0101</td><td>0110</td><td>0111</td><td>1000</td><td>1001</td></tr></tbody></table><h4 id="余-3-码"><a href="#余-3-码" class="headerlink" title="余 3 码"></a>余 3 码</h4><ul><li>余3码：8421码 + (0011)<del>2</del></li><li>余 3 码的每个权位不固定，所以是无权码</li></ul><table><thead><tr><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>0011</td><td>0100</td><td>0101</td><td>0110</td><td>0111</td><td>1000</td><td>1001</td><td>1010</td><td>1011</td><td>1100</td></tr></tbody></table><h4 id="2421-码"><a href="#2421-码" class="headerlink" title="2421 码"></a>2421 码</h4><ul><li>4 位二进制权值分别为：2 4 2 1</li><li>0<del>4 首位必须是 0，5</del>9 首位必须是 1</li></ul><table><thead><tr><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>0000</td><td>0001</td><td>0010</td><td>0011</td><td>0100</td><td>1011</td><td>1100</td><td>1101</td><td>1110</td><td>1111</td></tr></tbody></table><h3 id="字符与字符串"><a href="#字符与字符串" class="headerlink" title="字符与字符串"></a>字符与字符串</h3><h4 id="ASCII-码"><a href="#ASCII-码" class="headerlink" title="ASCII 码"></a>ASCII 码</h4><ul><li>键盘共 128 个字符，可以用 7 位二进制编码。<ul><li>为了存入计算机，通常在最高位补 0，凑足1B</li></ul></li><li>可印刷字符：32～126，其余为控制（如：127 DEL）、通信（如：6 ACK）字符</li><li>数字 0~9：<strong>48</strong>(0011 0000)～<strong>57</strong>(0011 1001)<ul><li>数字的 ASCII 码的<strong>前四位是 0011，后四位是 8421 码</strong></li></ul></li><li>大写字母 A~Z：<strong>65</strong>(0100 0001)～<strong>90</strong>(0101 1010)<ul><li>大写字母 ASCII 码<strong>前三位都是 010，后五位是 1~26</strong></li></ul></li><li>小写字母 a~z：<strong>97</strong>(0110 0001)～<strong>122</strong>(0111 1010)<ul><li>小写字母 ASCII 码<strong>前三位都是 011，后五位是 1~26</strong></li></ul></li><li>所有数字、大写字母、小写字母的编码都是连续的</li></ul><blockquote><p>求解某字符 ASCII 码时，要充分利用上述规律，避免十进制、二进制转换。</p></blockquote><h4 id="汉字的表示和编码"><a href="#汉字的表示和编码" class="headerlink" title="汉字的表示和编码"></a>汉字的表示和编码</h4><ul><li>GB 2312-100（19100年）：汉字+各种符号共7445个</li><li><strong>区位码：94 个区，每区 94 个位置</strong>。相当于把所有汉字存放在了 94*94 的方阵中，通过两个字节长度来定位汉字</li><li><strong>国标码：区位码 + 20H</strong>。防止信息交换时与“控制/通信字符”冲突</li><li><strong>汉字(机)内码：国标码 + 100H</strong>。保证高位为1，与ASCII码区分</li><li>输入法 -&gt; 国标码 -&gt; 汉字内码 -&gt;  (国标码 -&gt; )汉字字形码（像素方阵）</li></ul><h4 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211003101235176.png" alt="image-20211003101235176"></li><li><strong>按字节编址：每个地址对应 1B/1字节（存储单元大小为 1B/8b）</strong></li><li>很多语言中，“\0”作为字符串结尾标志</li><li>在所有计算机中，多字节数据都被存放在连续的字节序列中。根据数据中各字节的排列顺序不同，可能有“大端模式”、“小端模式”<ul><li>大端模式：将数据的<strong>最高有效字节存放在低地址单元</strong>中（低位到高位顺序读取）</li><li>小端模式：将数据的<strong>最高有效字节存放在高地址单元</strong>中（遇到多字节数据需要倒着读）</li></ul></li></ul><h3 id="奇偶校验码（计网要考）"><a href="#奇偶校验码（计网要考）" class="headerlink" title="奇偶校验码（计网要考）"></a>奇偶校验码（计网要考）</h3><h4 id="校验原理"><a href="#校验原理" class="headerlink" title="校验原理"></a>校验原理</h4><ul><li>位错误：在 bit 位上发生的突变。0 变 1，1 变 0</li><li><strong>码字</strong>：由若干位代码组成的一个字</li><li><strong>两个码字间的距离</strong>：将两个码字逐位进行对比，<strong>具有不同的位的个数</strong></li><li><strong>码距</strong>：一种编码方案中，合法码字之间的最小距离。<strong>最少变动多少位可以在各合法码字之间转换</strong></li><li>当 d=1 时，无检错能力；当 d=2 时，有检错能力；当 d≥3 时，若设计合理，可能具有检错、纠错能力</li></ul><h4 id="奇偶校验码"><a href="#奇偶校验码" class="headerlink" title="奇偶校验码"></a>奇偶校验码</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211004142023267.png" alt="image-20211004142023267" style="zoom:100%;"></li><li><strong>奇校验码</strong>：在有效信息位之外添加一位校验位，使得整个校验码中“1”的个数为<strong>奇数</strong>。</li><li><strong>偶校验码</strong>：在有效信息位之外添加一位校验位，使得整个校验码中“1”的个数为<strong>偶数</strong>。</li><li>本质：如果出现<strong>奇数次的位错误，则可以检测出错误</strong>；如果出现<strong>偶数次位错误，则检不出错误</strong>。</li><li><strong>偶校验的硬件实现</strong>：取各位的信息依次进行异或（模2加）运算，得到的结果即为偶校验位。<ul><li>进行偶校验且结果为 0 时，则通过校验。</li><li>进行奇校验且结果为 1 时，则通过校验。</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211004143356036.png" alt="image-20211004143356036" style="zoom:100%;"></li></ul><h3 id="海明校验码（计网要考）"><a href="#海明校验码（计网要考）" class="headerlink" title="海明校验码（计网要考）"></a>海明校验码（计网要考）</h3><h4 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h4><ul><li>针对问题：偶校验只能发现<strong>奇数位错误</strong>，且<strong>无法确定出错位置</strong>，想要获得正确信息则必须重新传输信息。</li><li>改进方向：将信息位分组进行偶校验 —&gt; 多个校验位 —&gt; 多个校验位标注出错位置<ul><li>1 个校验位之只能携带 2 种状态信息（对/错）</li><li>多个校验位能携带多种状态信息（对/错，错在哪）</li></ul></li><li>信息位 n 位 + 校验位 k 位 = 共 n+k 位<ul><li>k 位校验位代表最多能表示 2^k^ 种状态</li><li>校验位表示的状态应该要考虑到 n+k 的任何一位都有可能出错</li><li>重要公式：<strong>2^k^ ≥ n + k + 1</strong></li></ul></li><li>本质：分 k 组偶校验</li></ul><h4 id="海明码求解步骤（TODO）"><a href="#海明码求解步骤（TODO）" class="headerlink" title="海明码求解步骤（TODO）"></a>海明码求解步骤（TODO）</h4><p>Eg：信息位为 1010，求解海明码。</p><ol><li><p>确定海明码的位数：</p><ul><li>2^k^ ≥ n + k + 1</li><li>n=4 =&gt; k=3</li></ul></li><li><p>确定校验位的分布：</p><ul><li><p>设信息位 D<del>4</del>D<del>3</del>D<del>2</del>D<del>1</del>（1010），共4位；校验位 P<del>3</del>P<del>2</del>P<del>1</del>，共3位；对应的海明码为 H<del>7</del>H<del>6</del>H<del>5</del>H<del>4</del>H<del>3</del>H<del>2</del>H<del>1</del></p></li><li><p><strong>校验位 P<del>i</del> 放在海明位号为 2^i−1^ 的位置上</strong></p></li><li><p>校验位 P<del>i</del> 与位置序号第 i 位为 1 的信息位归为同一组，进行偶校验</p></li><li><table><thead><tr><th>H<del>7</del></th><th>H<del>6</del></th><th>H<del>5</del></th><th>H<del>4</del></th><th>H<del>3</del></th><th>H<del>2</del></th><th>H<del>1</del></th></tr></thead><tbody><tr><td>D<del>4</del></td><td>D<del>3</del></td><td>D<del>2</del></td><td>P<del>3</del></td><td>D<del>1</del></td><td>P<del>2</del></td><td>P<del>1</del></td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td></tr></tbody></table></li></ul></li><li><p>求校验位的值：</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211004151532962.png" alt="image-20211004151532962" style="zoom:100%;"></li><li>将海明码信息位的下标转二进制矩阵，一列为一组，对应 1 的信息拿出来做偶校验，结果就是海明码校验位的值。</li></ul></li><li><p>纠错：</p><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211004154938467.png" alt="image-20211004154938467" style="zoom:100%;"></li><li>题目的下标顺序不影响做题（TODO）</li></ul></li></ol><h4 id="海明码的检错、纠错能力"><a href="#海明码的检错、纠错能力" class="headerlink" title="海明码的检错、纠错能力"></a>海明码的检错、纠错能力</h4><ul><li>海明码的检错、纠错能力：<ul><li>纠错能力：1位（无法区分到底是 1 位错还是 2 位错）</li><li>检错能力：2位</li></ul></li><li>为了区分是 1 位错误还是 2 位错误，需加上“全校验位”，对整体进行偶校验。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211004155557262.png" alt="image-20211004155557262"></li></ul><h3 id="循环冗余校验码（计网要考）"><a href="#循环冗余校验码（计网要考）" class="headerlink" title="循环冗余校验码（计网要考）"></a>循环冗余校验码（计网要考）</h3><h4 id="基本思想-1"><a href="#基本思想-1" class="headerlink" title="基本思想"></a>基本思想</h4><ul><li>循环冗余校验（Cyclic Redundancy Check，CRC）</li><li>数据发送、接受方<strong>共同约定一个“除数”</strong></li><li>K个信息位+R个校验位 作为“被除数”，<strong>添加校验位以保证除法的余数为 0</strong><ul><li>收到数据后，进行除法检查余数是否为0</li><li>若余数非 0 说明出错，则进行重传或纠错</li></ul></li></ul><h4 id="构造、检错、纠错"><a href="#构造、检错、纠错" class="headerlink" title="构造、检错、纠错"></a>构造、检错、纠错</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211004164514722.png" alt="image-20211004164514722" style="zoom:100%;"></li><li>流程：<ol><li>题目给出生成多项式、信息码</li><li>K = 信息码的长度 = 6，R = 生成多项式最高次幂 = 3 =&gt; 校验码位数 N = K + R = 9</li><li>信息码左移 R 位，低位补 0。利用生成多项式系数进行模2除法，得余数为校验码。</li><li>将接受到的码字用生成多项式进行模2除法，余数为000则代表没有出错，否则不然。</li></ol></li><li>循环冗余校验码的检错、纠错特性：<ul><li>可检测出所有奇数个错误</li><li>可检测出所有双比特的错误</li><li>可检测出所有小于等于校验位长度的连续错误</li><li>对于确定的生成多项式，出错位与余数是相对应的</li><li>当码字长度超出检错码可表示的能力时，出错位与余数的对应关系将进行循环</li><li>K个信息位，R个校验位，若生成多项式选择得当，且 2^R^≥K+R+1，则 CRC 码可纠正1位错</li></ul></li></ul><h2 id="定点数的表示与运算"><a href="#定点数的表示与运算" class="headerlink" title="定点数的表示与运算"></a>定点数的表示与运算</h2><blockquote><ul><li>定点数：小数点的位置固定（定点数也包含小数）<ul><li>常规计数</li><li>Eg：996.007</li></ul></li><li>浮点数：小数点的位置不固定<ul><li>科学计数法</li><li>Eg：9.96007*102</li></ul></li></ul></blockquote><h3 id="定点数的表示"><a href="#定点数的表示" class="headerlink" title="定点数的表示"></a>定点数的表示</h3><h4 id="无符号数的定点表示"><a href="#无符号数的定点表示" class="headerlink" title="无符号数的定点表示"></a>无符号数的定点表示</h4><ul><li>无符号数：<strong>整个机器字长的全部二进制位均为数值位</strong>，没有符号位，相当于数的绝对值。</li><li>通常只有无符号整数，而没有无符号小数（unsigned int/long）</li><li>表示范围：<strong>n 位 bit 的无符号数表示范围为：0 ～ 2n-1</strong><ul><li>Eg：8位二进制数： 2^8^ 种不同的状态，表示 0000 0000 ～ 1111 1111，即 0 ~ 255</li></ul></li></ul><h4 id="有符号数的定点表示"><a href="#有符号数的定点表示" class="headerlink" title="有符号数的定点表示"></a>有符号数的定点表示</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211007120237263.png" alt="image-20211007120237263" style="zoom:100%;"></li><li>可用 <strong>原码、反码、补码</strong> 三种方式来表示定点整数和定点小数。还可用 <strong>移码</strong> 表示定点整数。</li><li>若真值为 x，则用 <strong>[x]原、[x]反、[x]补、[x]移</strong> 分别表示真值所对应的原码、反码、补码、移码</li><li>若机器字长为 n+1 位，则符号位占 1 位，尾数占 n 位</li><li>表示定点整数，默认小数点隐含在尾数后；表示定点小数，默认小数点隐含在符号位后尾数前。</li></ul><h5 id="原码表示定点整数和定点小数"><a href="#原码表示定点整数和定点小数" class="headerlink" title="原码表示定点整数和定点小数"></a>原码表示定点整数和定点小数</h5><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211007121910742.png" alt="image-20211007121910742" style="zoom:100%;"></li><li>未指明机器字长时，整数尾数前端的 0 可略去，小数尾数后端的 0 可略去。</li><li>要注意<strong>第一位是符号位</strong>，切不能当成数值。</li><li>整数原码常用 <strong>逗号</strong> 分割符号位和尾数，小数原码常用 <strong>小数点</strong> 分割符号位和尾数。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211007123125347.png" alt="image-20211007123125347" style="zoom:100%;"></li><li>在整数和小数的原码表示中，<strong>真值 0 有 +0 和 -0 两种形式</strong>，即两种二进制状态对应了同一种真值。<ul><li>因此即使机器字长为 n+1 位，理论上能够表示 2^n+1^ 种情况，但实际上值表示了 <strong>2^n+1^-1</strong> 种。</li></ul></li><li>原码整数表示范围：**-(2^n^-1)≤x≤2^n^-1<strong>；源码小数表示范围：</strong>-(1-2^n^)≤x≤1-2^n^**。</li></ul><h5 id="反码表示定点整数和定点小数"><a href="#反码表示定点整数和定点小数" class="headerlink" title="反码表示定点整数和定点小数"></a>反码表示定点整数和定点小数</h5><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211007124524338.png" alt="image-20211007124524338" style="zoom:100%;"></li><li>反码：<strong>若符号位为0，则反码与原码相同；若符号位为1，则数值位全部取反。</strong></li><li>表示范围和源码相同。</li></ul><blockquote><p>“反码”只是“原码”转变为“补码”的一个中间状态，实际中并没什么卵用——并没有解决“真值 0 有 +0 和 -0 两种形式”这个问题。</p></blockquote><h5 id="补码表示定点整数和定点小数"><a href="#补码表示定点整数和定点小数" class="headerlink" title="补码表示定点整数和定点小数"></a>补码表示定点整数和定点小数</h5><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211007131951644.png" alt="image-20211007131951644" style="zoom:100%;"></li><li>补码：正数的补码=原码；负数的补码=反码末位+1（要考虑进位）</li><li>补码的真值 0 只有一种表示形式：[+0]补= [-0]补= 00000000<ul><li>定点整数<ul><li>[-2^7^]补 = **1,**0000000</li><li>表示范围：−2^n^ ≤ x ≤ 2^n^−1</li></ul></li><li>定点小数<ul><li>[-1]补 = **1.**0000000 </li><li>表示范围：-1 ≤ x ≤ 1-2^-n^</li></ul></li></ul></li><li><strong>由 [x]补 快速求 [-x]补 的方法：符号位、数值位全部取反，末位+1</strong></li></ul><h5 id="移码表示定点整数"><a href="#移码表示定点整数" class="headerlink" title="移码表示定点整数"></a>移码表示定点整数</h5><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211007143059138.png" alt="image-20211007143059138" style="zoom:100%;"></li><li>移码：<strong>补码的基础上将符号位取反</strong>。注意：<strong>移码只能用于表示整数</strong>。</li><li>移码表示的整数很方便对比大小</li><li>在浮点数中将大量是使用源码</li></ul><h3 id="原码补码移码的作用"><a href="#原码补码移码的作用" class="headerlink" title="原码补码移码的作用"></a>原码补码移码的作用</h3><ul><li>无符号数：可以进行直接加减运算。</li><li>原码的加减运算：需要将符号位单独区分，但是这样将增加硬件成本、计算复杂度。<ul><li>因此需要用加法运算来代替减法运算 =&gt; 将减法操作转换成取模运算（例如：-3mod12=9）</li></ul></li><li>补码的本质：为了能够进行直接加运算，将系统设计成了循环。</li><li>反码的本质：将原码的负数部分进行了反转，是补码能够循环的铺垫。</li><li>移码的本质：使用偏置将补码的循环提前了半个周期。</li></ul><table><thead><tr><th>二进制机器数</th><th>二进制表示无符号数</th><th>二进制表示原码</th><th>二进制表示反码</th><th>二进制表示补码</th><th>二进制表示移码</th></tr></thead><tbody><tr><td>0000 0000</td><td><strong>0</strong></td><td><strong>+0</strong></td><td><strong>+0</strong></td><td><strong>±0</strong></td><td>-128</td></tr><tr><td>0000 0001</td><td>1</td><td>1</td><td>1</td><td>1</td><td>-127</td></tr><tr><td>……</td><td>……</td><td>……</td><td>……</td><td>……</td><td>……</td></tr><tr><td>0111 1111</td><td>127</td><td>127</td><td>127</td><td>127</td><td>-1</td></tr><tr><td>1000 0000</td><td>128</td><td><strong>-0</strong></td><td>-127</td><td>-128</td><td><strong>±0</strong></td></tr><tr><td>1000 0001</td><td>129</td><td>-1</td><td>-126</td><td>-127</td><td>1</td></tr><tr><td>……</td><td>……</td><td>……</td><td>……</td><td>……</td><td>……</td></tr><tr><td>1111 1111</td><td>255</td><td>-127</td><td><strong>-0</strong></td><td>-1</td><td>127</td></tr></tbody></table><ul><li>带余除法——设 x, m∈Z, m&gt;0 则存在唯一决定的整数 q 和 r，使得：x = q*m + r , 0 ≤ r &lt; m <ul><li>(mod 12) 把所有整数分为 12 类（余数为 0~11）</li><li>(mod 12) 余数相同的数，都是同一类，都是等价的</li><li>在 (mod m) 的条件下，若能<strong>找到负数的补数</strong>，就可以<strong>用正数的加法来等价替代减法</strong><ul><li>若二个数绝对值之和=模，则称这两个数互为补数</li></ul></li><li><strong>模 - a的绝对值 = a的补数</strong></li><li>↑ 这是补码的原生定义</li></ul></li><li>反码 + 原码 + 1 = 模</li><li>补码的作用： 使用补码可将减法操作转变为等价的加法，ALU 中无需集成减法器。执行加法操作时，符号位一起参与运算。</li></ul><h3 id="移位运算"><a href="#移位运算" class="headerlink" title="移位运算"></a>移位运算</h3><h4 id="算数移位"><a href="#算数移位" class="headerlink" title="算数移位"></a>算数移位</h4><ul><li><p>移位：通过改变各个数码位和小数点的相对位置，从而改变各数码位的位权。可用移位运算实现乘法、除法</p></li><li><p>原码的算数移位——<strong>符号位保持不变，仅对数值位进行移位</strong>。</p><ul><li>右移：高位补0，低位舍弃。若舍弃的位=0，<strong>则相当于÷2</strong>；若舍弃的位≠0，则会丢失<strong>精度</strong></li><li>左移：低位补0，高位舍弃。若舍弃的位=0，<strong>则相当于×2</strong>；若舍弃的位≠0，则会出现<strong>严重误差</strong></li></ul></li><li><p>反码的算数移位</p><ul><li>正数的反码与原码相同，因此对正数反码的移位运算也和原码相同。<ul><li>右移：<strong>高位补0</strong>，低位舍弃。</li><li>左移：<strong>低位补0</strong>，高位舍弃。</li></ul></li><li>负数的反码数值位与原码相反，因此负数反码的移位在补位上有所变化。<ul><li>右移：<strong>高位补1</strong>，低位舍弃。</li><li>左移：<strong>低位补1</strong>，高位舍弃。</li></ul></li></ul></li><li><p>补码的算数移位</p><ul><li>正数的补码与原码相同，因此对正数补码的移位运算也和原码相同。<ul><li>右移：<strong>高位补0</strong>，低位舍弃。</li><li>左移：<strong>低位补0</strong>，高位舍弃。</li></ul></li><li>负数补码=反码末位+1，导致反码最右边几个连续的1都因进位而变为0，直到进位碰到第一个0为止。<ul><li>负数补码中：最右边的1及其右边同原码；最右边的1的左边同反码负数补码。</li><li>右移（同反码）：<strong>高位补1</strong>，低位舍弃。</li><li>左移（同原码）：<strong>低位补0</strong>，高位舍弃。</li></ul></li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211008121454539.png" alt="image-20211008121454539" style="zoom:100%;"></li><li><p>Eg：-20×7 = -20×(2^0^+2^1^+2^2^) = (-20左移0位) + (-20左移1位) + (-20左移2位)</p></li></ul><h4 id="逻辑移位"><a href="#逻辑移位" class="headerlink" title="逻辑移位"></a>逻辑移位</h4><ul><li>逻辑右移：高位补0，低位舍弃。</li><li>逻辑左移：低位补0，高位舍弃。</li><li>可以把逻辑移位看作是对“无符号数”的算数移位</li><li>Eg：<img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211008122429002.png" alt="image-20211008122429002" style="zoom:100%;"></li></ul><h4 id="循环移位"><a href="#循环移位" class="headerlink" title="循环移位"></a>循环移位</h4><ul><li>不带进位位：用<strong>移出的位补上空缺</strong></li><li>带进位位：用<strong>进位位的值补上空缺</strong>，而<strong>移出的位放到进位位</strong></li><li>Eg：交换高低位字节时常用（大端存储&lt;=&gt;小端存储）</li></ul><h3 id="加减运算和溢出判断"><a href="#加减运算和溢出判断" class="headerlink" title="加减运算和溢出判断"></a>加减运算和溢出判断</h3><h4 id="原码的加减法"><a href="#原码的加减法" class="headerlink" title="原码的加减法"></a>原码的加减法</h4><ul><li>要考虑 2*2 种情况，并且同时实现加法器和减法器</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211008124354552.png" alt="image-20211008124354552"></li></ul><h4 id="补码的加减法"><a href="#补码的加减法" class="headerlink" title="补码的加减法"></a>补码的加减法</h4><ul><li><p>负数的补码转原码：</p><ul><li>数值位取反，+1</li><li>-1，数值位取反</li><li>负数补码中，最右边的1及其右边同原码，最右边的1的左边同反码</li></ul></li><li><p>[(负数)]补 &lt;=&gt; [(负数)]原：<strong>取负数补码最右边的一位1，其本身与其右侧不变，左侧数值位全部取反</strong>。（常用）</p></li><li><p>[x]补 =&gt; [-x]补：<strong>连同符号位一起取反加1</strong>。</p></li></ul><h4 id="溢出判断"><a href="#溢出判断" class="headerlink" title="溢出判断"></a>溢出判断</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211008151859101.png" alt="image-20211008151859101"></li><li>溢出情况：<ul><li>只有“正数+正数 ”才会上溢 —— 正+正=负</li><li>只有“负数+负数 ”才会下溢 —— 负+负=正</li></ul></li><li>判断方法：<ul><li>法一：采用一位符号位设 A 的符号为 A<del>S</del>，B 的符号为 B<del>S</del>，运算结果的符号为 S<del>S</del>，则溢出逻辑表达式为：<ul><li>$V = A_{s}B_{s}\overline{S_{s}} + \overline{A_{s}}\overline{B_{s}}S_{s}$</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211008163931615.png" alt="image-20211008163931615" style="zoom:100%;"></li><li>若 V=0，表示无溢出；若 V=1，表示有溢出。</li><li><strong>A<del>S</del> 为 1 且 B<del>S</del> 为 1 且 S<del>S</del> 为 0</strong> 或 <strong>A<del>S</del> 为 0 且 B<del>S</del> 为 0 且 S<del>S</del> 为 1</strong>。</li></ul></li><li>法二：采用一位符号位，根据数据位进位情况判断溢出符号位的进位 C<del>S</del>，最高数值位的进位 C<del>1</del><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211008163814192.png" alt="image-20211008163814192" style="zoom:100%;"></li><li>$V=C_{S}⊕ C_{1}$</li><li>若 V=0，表示无溢出；若 V=1，表示有溢出。</li><li><strong>最高数值位进位且符号位不进位</strong> 或 <strong>符号位进位而最高数值位不进位</strong>。</li></ul></li><li>法三：<ul><li>采用双符号位：正数符号为 00，负数符号为 11</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211008163848872.png" alt="image-20211008163848872" style="zoom:100%;"></li><li>记两个符号位为 S<del>S1</del>S<del>S2</del> ，则 $V=S_{S1}\otimes S_{S2}$。若 V=0，表示无溢出；若 V=1，表示有溢出。</li><li><strong>第一个符号位表明应该得到的符号，第二个符号位表明实际得到的符号。</strong></li><li>双符号位补码又称：模 4 补码；单符号位补码又称：模 2 补码。</li><li>实际存储时只存储一个符号位，运算时会复制一个符号位</li></ul></li></ul></li><li>同+同=异  =&gt;  发生溢出</li></ul><h4 id="符号扩展"><a href="#符号扩展" class="headerlink" title="符号扩展"></a>符号扩展</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211008164330993.png" alt="image-20211008164330993" style="zoom:100%;"></li><li>定点整数的符号扩展：在原符号位和数值位<strong>中间</strong>添加新位，正数都添0；负数<strong>原码添0，反码、补码添1</strong></li><li>定点小数的符号扩展：在原符号位和数值位<strong>后面</strong>添加新位，正数都添0；负数<strong>原码、补码添0，反码添1</strong></li></ul><h3 id="乘法运算"><a href="#乘法运算" class="headerlink" title="乘法运算"></a>乘法运算</h3><h4 id="乘法运算的实现思想"><a href="#乘法运算的实现思想" class="headerlink" title="乘法运算的实现思想"></a>乘法运算的实现思想</h4><ul><li>用移位实现乘数累加</li><li>Eg：0.1101×0.1011 = (1101×1×2^-8^ ) + (1101×1×2^-7^) + (1101×0×2^-6^) + (1101×1×2^-5^)</li></ul><h4 id="原码的一位乘法"><a href="#原码的一位乘法" class="headerlink" title="原码的一位乘法"></a>原码的一位乘法</h4><ul><li><strong>符号单独处理</strong>：符号位 = x<del>s</del>⊕y<del>s</del></li><li><strong>数值位取绝对值进行乘法</strong>计算</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211009130101904.png" alt="image-20211009130101904" style="zoom:100%;"></li><li>在机器实现乘法中：<ul><li>ACC 存放<strong>乘积高位</strong>，MQ 存放<strong>乘数与乘积低位</strong>，X 存放<strong>被乘数</strong></li><li>在正式进行乘法之前，<strong>ACC 置零</strong></li><li>先将 X 中被乘数与 MQ 最低位的乘积结果<strong>加到 ACC 中</strong></li><li>再将 ACC、MQ 整体<strong>逻辑右移</strong>一位<ul><li>逻辑右移，高位补零</li><li>ACC 的低位移到 MQ 的高位</li><li>MQ 的低位用完之后直接丢弃</li><li>此时，AC、MQ 中运算得到的位上的结果称作<strong>部分积</strong></li></ul></li><li>数值位一共 n 位，则<strong>循环 n 次</strong><ul><li>乘数的符号位不用参与运算。ACC+MQ 是一个整体，而 MQ 由取绝对值，所以最多只有 4 为有效，原本的符号位不会对结果造成影响。</li><li>小数点隐含位置在 ACC 的第二位</li></ul></li><li><strong>修改 ACC 的符号位</strong>：x<del>s</del>⊕y<del>s</del>=1</li></ul></li><li>之所以称为“一位乘法”是因为每次都使用 MQ 的最后一位进行乘积相加，还有更快的“二位乘法”但不做要求。</li><li>Tips：<ul><li>乘数的符号位不参与运算，可以省略</li><li>原码一位乘可以只用单符号位，也可以用双符号位</li><li>答题时最终结果最好写为原码机器数</li></ul></li></ul><h4 id="补码的一位乘法（Booth算法）"><a href="#补码的一位乘法（Booth算法）" class="headerlink" title="补码的一位乘法（Booth算法）"></a>补码的一位乘法（Booth算法）</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211009134039359.png" alt="image-20211009134039359" style="zoom:100%;"></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211009143142389.png" alt="image-20211009143142389" style="zoom:100%;"></li><li>机器实现补码乘法时：<ul><li>MQ 相比原码乘法需要在最低位后面加一位辅助位。同时 CPU 中寄存器大小应当一致，所以<ul><li>ACC、X 使用<strong>双符号位</strong>，MQ 使用<strong>单符号位以及一位辅助位</strong>，部分积也是使用<strong>双符号位</strong></li></ul></li><li>与原码乘法不同，每次加法是：<code>ACC + [(辅助位-MQ最低位)x]补</code></li><li>与原码乘法不同，每次移位是：<strong>补码的算数移位</strong><ul><li>符号位不动，数值位右移，正数右移补0，负数右移补1（符号位是啥就补啥）</li></ul></li><li>加法与移位的循环分别是 n+1 和 n 次，即<strong>乘数的单符号位也会参与计算</strong></li><li>与原码乘法不同，最后的符号确定已在计算过程中完成，无需再次校验。</li></ul></li></ul><h3 id="除法运算"><a href="#除法运算" class="headerlink" title="除法运算"></a>除法运算</h3><h4 id="除法运算的实现思想"><a href="#除法运算的实现思想" class="headerlink" title="除法运算的实现思想"></a>除法运算的实现思想</h4><ul><li>被除数、余数本质相同，是我们还需要去拼凑的数</li><li>商是我们尽可能去接近需要拼凑的数但不能超过</li></ul><h4 id="原码的除法运算"><a href="#原码的除法运算" class="headerlink" title="原码的除法运算"></a>原码的除法运算</h4><h5 id="恢复余数法"><a href="#恢复余数法" class="headerlink" title="恢复余数法"></a>恢复余数法</h5><ul><li>ACC 放被除数和余数，MQ 放商，X 放除数</li><li><strong>符号单独处理</strong>：符号位 = x<del>s</del>⊕y<del>s</del></li><li><strong>数值位取绝对值进行除法</strong>计算</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211011153443169.png" alt="image-20211011153443169" style="zoom:100%;"></li><li>在机器实现除法中：<ul><li>在正式进行乘法之前，<strong>MQ 置零</strong></li><li>首先<strong>默认商/MQ最低位为 1</strong>，将除数的相反数的补码与被除数/余数（ ACC ）相加（将 ACC 减去除数）<ul><li>(ACC)+ [−|y|]补 -&gt; ACC</li><li><strong>若相减之后余数符号位为 1</strong>（小于 0），则表明商不应该为 1，而是 0</li><li><strong>则将 ACC 的值再次加上除数</strong>，恢复成原样</li><li>并<strong>将商改成 0</strong></li><li>若最后一步余数是负数，则一样要进行上面的纠正</li></ul></li><li>ACC、MQ 整体逻辑左移<ul><li>MQ 低位补零</li><li>ACC 高位丢弃</li></ul></li><li>机器字长有 n+1 位，就要算 n+1 位的商，左移 n 次</li><li><strong>修改 MQ 的符号位</strong>：x<del>s</del>⊕y<del>s</del>=0</li></ul></li></ul><h5 id="不恢复余数法（加减交替法）"><a href="#不恢复余数法（加减交替法）" class="headerlink" title="不恢复余数法（加减交替法）"></a>不恢复余数法（加减交替法）</h5><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211011155159488.png" alt="image-20211011155159488" style="zoom:100%;"></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211011154706460.png" alt="image-20211011154706460" style="zoom:100%;"></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211011155635905.png" alt="image-20211011155635905" style="zoom:100%;"></li><li>当余数（ACC）为负时，商 0<ul><li>恢复余数法：+|除数（X）|，再左移，再-|除数（X）|</li><li>加减交替法：左移，再+|除数（X）|</li></ul></li><li>在加减交替法中<ul><li>余数（ACC）的正负性与商相同</li><li>符号位同样需要额外判定</li><li>若最后的余数（ACC）为负，需商0，并+[|y|]补得到正确余数</li></ul></li></ul><h4 id="补码的除法运算（加减交替法）"><a href="#补码的除法运算（加减交替法）" class="headerlink" title="补码的除法运算（加减交替法）"></a>补码的除法运算（加减交替法）</h4><ul><li>符号位参与运算</li><li>被除数/余数、除数采用双符号位</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211014133724204.png" alt="image-20211014133724204" style="zoom:100%;"></li><li>第一步，比较<strong>被除数</strong>（ACC）和除数（X）符号<ul><li>同号，则被除数-除数</li><li>异号，则被除数+除数</li></ul></li><li>然后，比较<strong>余数</strong>（ACC）和除数（X）符号（每次循环保证两者相加符号不同，结果与被除）<ul><li>同号，<strong>商1</strong>，余数左移，减去除数</li><li>异号，<strong>商0</strong>，余数左移，加上除数</li></ul></li><li>重复 n 次</li><li>MQ 的最后一位恒置 1（精度误差不超过 2^-n^）</li></ul><h4 id="除法运算总结"><a href="#除法运算总结" class="headerlink" title="除法运算总结"></a>除法运算总结</h4><table><thead><tr><th>除法类型</th><th>符号位参与运算</th><th>加减次数</th><th>移位方向</th><th>移位次数</th><th>上商、加减原则</th><th>说明</th></tr></thead><tbody><tr><td>原码加减交替法</td><td>否</td><td>N+1 或 N+2</td><td>左</td><td>N</td><td>余数的正负</td><td>若最终余数为负，则恢复余数</td></tr><tr><td>补码加减交替法</td><td>是</td><td>N+1</td><td>左</td><td>N</td><td>余数和除数是否同号</td><td>商末位恒置 1</td></tr></tbody></table><h3 id="强制类型转换"><a href="#强制类型转换" class="headerlink" title="强制类型转换"></a>强制类型转换</h3><ul><li>C 语言中定点整数（int、long、short）是用<strong>补码</strong>存储的。</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211015192650002.png" alt="image-20211015192650002" style="zoom:100%;"></li></ul><h3 id="数据的存储和排列"><a href="#数据的存储和排列" class="headerlink" title="数据的存储和排列"></a>数据的存储和排列</h3><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211015195134394.png" alt="image-20211015195134394" style="zoom:100%;"></li><li>最高有效字节（<strong>MSB</strong>）最低有效字节（<strong>LSB</strong>）</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211015200927531.png" alt="image-20211015200927531"></li><li>现代计算机<strong>通常是按字节编址</strong>，即每个字节对应1个地址<ul><li>通常也支持按字、按半字、按字节寻址</li></ul></li><li>假设存储字长为32位，则1个字=32bit，半字=16bit<ul><li>要将按字寻址转成按字节寻址，则将值左移2位</li></ul></li><li><strong>每次访存只能读/写1个字</strong></li><li>c语言的数据类型长度<ul><li>char：1字节</li><li>short：2字节</li><li>int：4字节</li><li>long：8字节</li></ul></li><li>使用<strong>边界对齐方式</strong>可以<strong>提高读取效率</strong>，而使用<strong>边界不对齐方式</strong>可以<strong>减少存储开销</strong>。</li></ul><h2 id="浮点数的表示与运算"><a href="#浮点数的表示与运算" class="headerlink" title="浮点数的表示与运算"></a>浮点数的表示与运算</h2><h3 id="浮点数的表示"><a href="#浮点数的表示" class="headerlink" title="浮点数的表示"></a>浮点数的表示</h3><h4 id="浮点数的作用和基本原理"><a href="#浮点数的作用和基本原理" class="headerlink" title="浮点数的作用和基本原理"></a>浮点数的作用和基本原理</h4><ul><li>针对问题：定点数可表示的数字范围有限，但我们不能无限制地增加数据的长度</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211016153947660.png" alt="image-20211016153947660"></li><li>阶码：常用补码或移码表示的定点整数</li><li>尾数：常用原码或补码表示的定点小数</li><li>浮点数的真值：N=r^E^×M<ul><li>阶码的底 r，通常为 2</li><li>阶码 E 反映浮点数的<strong>表示范围</strong>及小数点的实际位置</li><li>尾数 M 的数值部分的位数 n 反映浮点数的<strong>精度</strong></li><li>尾数给出一个小数，阶码正负与数值指明了尾数左移/右移(小数点向后/向前移动)几位。</li></ul></li></ul><h4 id="浮点数规格化"><a href="#浮点数规格化" class="headerlink" title="浮点数规格化"></a>浮点数规格化</h4><ul><li>规格化浮点数：规定尾数的最高数值位必须是一个有效值 。</li><li>左规：当浮点数运算的结果为非规格化（尾数的最高数值位为 0）时，将尾数算数左移 1 位，阶码减 1。<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211016194214228.png" alt="image-20211016194214228"></li></ul></li><li>右规：当浮点数运算的结果尾数出现溢出（双符号位为01或10）时，将尾数算数右移 1 位，阶码加 1。<ul><li>第一位符号位是正确的符号位</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211016194118837.png" alt="image-20211016194118837"></li></ul></li><li>规格化浮点数的特点：<ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211016204436786.png" alt="image-20211016204436786"></li><li>规格化的原码尾数，最高数值位一定是 1</li><li>规格化的补码尾数，符号位与最高数值位一定相反</li></ul></li></ul><h4 id="浮点数表示范围（大纲外）"><a href="#浮点数表示范围（大纲外）" class="headerlink" title="浮点数表示范围（大纲外）"></a>浮点数表示范围（大纲外）</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211016204413711.png" alt="image-20211016204413711"></li><li>对于下溢，只需要当做0处理</li><li>对于上溢，必须作为异常</li></ul><h3 id="浮点数标准IEEE75"><a href="#浮点数标准IEEE75" class="headerlink" title="浮点数标准IEEE75"></a>浮点数标准IEEE75</h3><ul><li><p>读音：I triple E——IEEE</p></li><li><p>移码的定义：<strong>移码 = 真值 + 偏置值</strong>（偏置值并不固定）</p><ul><li>在之前所学的移码中：偏置值为 128D=1000 0000B，即 2^n-1^</li><li>在 IEEE75 中，偏置值为 <strong>127D=0111 1111B</strong>，即 <strong>2^n-1^-1</strong></li></ul></li><li><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018121445755.png" alt="image-20211018121445755"></p></li><li></li><li><p>阶码全1、全0时用作特殊用途。因此阶码的正常范围：**-126~127**</p><ul><li>规格化短浮点真值：(-1)^s^×1.M×2^E-127^</li><li>规格化长浮点真值：(-1)^s^×1.M×2^E-1023^</li><li>计算<strong>移码/阶码的真值</strong>时，不用在二进制上面计算，可以<strong>先把看见的移码当做无符号数计算值</strong>，然后<strong>减去偏置值127</strong></li></ul></li></ul><table><thead><tr><th>类型</th><th>数符</th><th>阶码</th><th>尾数</th><th>总位数</th><th>偏置值（十六进制）</th><th>偏置值（十进制）</th></tr></thead><tbody><tr><td>短浮点数(float)</td><td>1</td><td>8</td><td>23</td><td>32</td><td>7FH</td><td>127</td></tr><tr><td>长浮点数(double)</td><td>1</td><td>11</td><td>52</td><td>64</td><td>3FFH</td><td>1023</td></tr><tr><td>临时浮点数(long double)</td><td>1</td><td>15</td><td>64</td><td>80</td><td>3FFFH</td><td>16383</td></tr></tbody></table><ul><li><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018115935229.png" alt="image-20211018115935229"></p></li><li><p>IEEE 754 单精度浮点型能表示的最小绝对值、最大绝对值</p><ul><li>最小绝对值：尾数全为0，阶码真值最小-126，对应移码机器数 0000 0001。此时整体的真值为 (1.0)<del>2</del>×2^-126^</li><li>最大绝对值：尾数全为1，阶码真值最大 127，对应移码机器数 1111 1110。此时整体的真值为 (1.111…11)<del>2</del>×2^127^</li></ul></li></ul><table><thead><tr><th>格式</th><th>格式化的最小绝对值</th><th>格式化的最大绝对值</th></tr></thead><tbody><tr><td>单精度</td><td>E=1，M=0：1.0×2^1−127^=2^-126^</td><td>E=254，M=.11…1：1.11…1×2^254−127^=2^127^×(2−2^−23^)</td></tr><tr><td>双精度</td><td>E=1，M=0：1.0×2^1−1023^=2^-1022^</td><td>E=254，M=.11…1：1.11…1×2^2046−1023^=2^1023^×(2−2^−52^)</td></tr></tbody></table><ul><li><strong>只有 1≤E≤254时，真值 = (−1)^s^×1.M×2^E−127^</strong></li><li>当阶码E全为0<ul><li>尾数M不全为0时，表示非规格化小数 ±(0.xx…x)<del>2</del>×2^-126^<ul><li>隐含最高位变为 0</li><li>阶码真值固定视为 -126</li></ul></li><li>尾数M全为0时，表示真值 ±0</li></ul></li><li>当阶码E全为1<ul><li>尾数M不全为0时，表示非数值 “NaN” (Not a Number)<ul><li>如：0/0、∞-∞ 等非法运算的结果就是 NaN</li></ul></li><li>尾数M全为0时，表示无穷大 ±∞</li></ul></li></ul><blockquote><p>由浮点数确定真值（阶码不是全0、也不是全1）：</p><ol><li>根据“某浮点数”确定数符、阶码、尾数的分布</li><li>确定尾数 1.M（注意补充最高的隐含位1）</li><li>确定阶码的真值 = 移码 - 偏置值 （可将移码看作无符号数，用无符号数的值减去偏置值）</li><li>(−1)^s^×1.M×2^E−偏置值^</li></ol></blockquote><h3 id="浮点数的运算"><a href="#浮点数的运算" class="headerlink" title="浮点数的运算"></a>浮点数的运算</h3><h4 id="浮点数的加减运算"><a href="#浮点数的加减运算" class="headerlink" title="浮点数的加减运算"></a>浮点数的加减运算</h4><ul><li>浮点数加减运算步骤<ul><li>转换格式（真值D-&gt;机器数B；注意审题：用补/移码表示阶码和尾数）</li><li>对阶（使两个数的阶码相等，小阶向大阶看齐，尾数毎右移一位，阶码加1）</li><li>尾数加减</li><li>规格化</li><li>舍入（可以有不同的舍入规则）<ul><li>0舍1入：移去为0则舍，为1则入（可能会使尾数又溢出，此时需再做一次右规。）</li><li>恒置1：无论移去0还是1，都使右移后的尾数末位置1（这种方法同样有使尾数变大和变小的两种可能。）</li></ul></li><li>判溢出（尾数溢出未必导致整体溢出，只有阶数溢出才是真正的溢出）</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018135039513.png" alt="image-20211018135039513"></li></ul><h4 id="强制类型转换-1"><a href="#强制类型转换-1" class="headerlink" title="强制类型转换"></a>强制类型转换</h4><table><thead><tr><th>类型</th><th>16位机器</th><th>32位机器</th><th>64位机器</th></tr></thead><tbody><tr><td>char</td><td>8</td><td>8</td><td>8</td></tr><tr><td>short</td><td>16</td><td>16</td><td>16</td></tr><tr><td>int</td><td>16</td><td>32</td><td>32</td></tr><tr><td>long</td><td>32</td><td>32</td><td>64</td></tr><tr><td>long long</td><td>64</td><td>64</td><td>64</td></tr><tr><td>float</td><td>16</td><td>32（23+1）</td><td>32</td></tr><tr><td>double</td><td>64</td><td>64（52+1）</td><td>64</td></tr></tbody></table><ul><li>没有特殊说明，则默认 32 位机器</li><li>无损转换：char -&gt; int -&gt; long -&gt; double；float -&gt; double。<ul><li>在 32 位中，这些转换过程没有损失</li><li>在 64 位中，long -&gt; double 会出现精度损失</li></ul></li><li>有损转换：int -&gt; float；float -&gt; int<ul><li>判断是否会有精度损失，要从有效数字方面考虑</li><li>int：表示整数，范围 -2^31^ ～ 2^31^-1 ，有效数字 <strong>32</strong> 位</li><li>float：表示整数及小数，范围 ±[2^-126^ ～ 2^127^×(2−2^−23^)]，有效数字 <strong>23+1=24</strong> 位</li><li>int -&gt; float：可能损失精度（如 2^24^~2^31^-1 中无法被 float 表示的）</li><li>float -&gt; int：可能溢出（范围过大）及损失精度（表示小数）</li></ul></li></ul><h2 id="算术逻辑单元（ALU）"><a href="#算术逻辑单元（ALU）" class="headerlink" title="算术逻辑单元（ALU）"></a>算术逻辑单元（ALU）</h2><h3 id="电路的基本原理、加法器设计"><a href="#电路的基本原理、加法器设计" class="headerlink" title="电路的基本原理、加法器设计"></a>电路的基本原理、加法器设计</h3><h4 id="作用与原理"><a href="#作用与原理" class="headerlink" title="作用与原理"></a>作用与原理</h4><ul><li>机器字长是 CPU（ALU）一次能够处理的长度，一般等于寄存器的长度。因为输入输出数据存放的寄存器也要和 CPU 处理长度一致。</li><li>ALU 的功能。由控制单元 CU 发出的控制信号决定<ul><li>算术运算：加、减、乘、除等</li><li>逻辑运算：与、或、非、异或等</li><li>辅助功能：移位、求补等</li></ul></li></ul><h4 id="电路基础知识"><a href="#电路基础知识" class="headerlink" title="电路基础知识"></a>电路基础知识</h4><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018152119179.png" alt="image-20211018152119179"></p><h4 id="加法器的实现"><a href="#加法器的实现" class="headerlink" title="加法器的实现"></a>加法器的实现</h4><h5 id="一位全加器"><a href="#一位全加器" class="headerlink" title="一位全加器"></a>一位全加器</h5><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018153352014.png" alt="image-20211018153352014">、</p><h5 id="串行加法器"><a href="#串行加法器" class="headerlink" title="串行加法器"></a>串行加法器</h5><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018153857365.png" alt="image-20211018153857365"></p><ul><li>串行加法器：只有一个全加器，数据逐位串行送入加法器中进行运算。<ul><li>进位触发器用来寄存进位信号，以便参与下一次运算。</li></ul></li><li>如果操作数长 n 位，加法就要分 n 次进行，每次产生一位和，并且串行逐位地送回寄存器。</li></ul><h5 id="并行加法器"><a href="#并行加法器" class="headerlink" title="并行加法器"></a>并行加法器</h5><p><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018153931233.png" alt="image-20211018153931233"></p><ul><li>串行进位的并行加法器：把 n 个全加器串接起来，就可进行两个 n 位数的相加。</li><li>串行进位又称为行波进位，每一级进位直接依赖于前一级的进位，即进位信号是逐级形成的。</li><li>其性能很大程度依赖于 <strong>FA 产生进位</strong>和<strong>传递进位</strong>的速度</li></ul><h3 id="加法器、ALU的改进"><a href="#加法器、ALU的改进" class="headerlink" title="加法器、ALU的改进"></a>加法器、ALU的改进</h3><ul><li>第 i 位向更高位的进位 C<del>i</del> 可根据 <strong>被加数、加数的第 1~i 位</strong>、<strong>C<del>0</del></strong> 即可确定</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018171510387.png" alt="image-20211018171510387"></li><li>并行进位的<strong>并行加法器</strong>：各级进位信号同时形成，又称为<strong>先行进位</strong>、<strong>同时进位</strong></li><li>为了避免电路过于复杂，设计为 4 个 FA 为一组，组成一个 CLA</li><li>G<del>i</del>：进位产生函数。（由本位确定的是否进位）</li><li>P<del>i</del>：进位传递函数。（本位会控制/屏蔽前一位的进位信息）</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018172125934.png" alt="image-20211018172125934"></li><li>完成组内并行之后，各组之间依然是串行的结构<ul><li><strong>单级先行进位方式</strong>，又称为<strong>组内并行、组间串行</strong>进位方式。</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018172358547.png" alt="image-20211018172358547"></li><li>通过 CLA 电路，同时产生各组之间的进位信息<ul><li><strong>多级先行进位</strong>方式，又称为<strong>组内并行、组间并行</strong>进位方式</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part2/image-20211018173324132.png" alt="image-20211018173324132"></li></ul>]]></content>
    
    
    <summary type="html">数据的表示和运算</summary>
    
    
    
    <category term="计算机组成原理" scheme="https://nephrencake.gitee.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机组成原理-Part1</title>
    <link href="https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/"/>
    <id>https://nephrencake.gitee.io/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/</id>
    <published>2021-10-02T02:52:28.000Z</published>
    <updated>2021-10-26T15:16:43.190Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算机组成原理-Part1——计算机系统概述"><a href="#计算机组成原理-Part1——计算机系统概述" class="headerlink" title="计算机组成原理-Part1——计算机系统概述"></a>计算机组成原理-Part1——计算机系统概述</h1><p>[TOC]</p><h2 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h2><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211002105534456.png" alt="image-20211002105534456" style="zoom:80%;"><ul><li><p>CPU、内存、硬盘上都有针脚。这些针脚就是用来接收高低电平的电信号的。</p></li><li><p>传递电信号就需要电脑主板上的印刷电路，这是可以导电的。</p></li><li><p>计算机体系结构 vs 计算机组成原理</p><ul><li><p>计算机体系结构：机器语言程序员所见到的计算机系统的属性概念性的结构与功能特性（指令系统、数据类型、寻址技术、I/O机理）</p><p>如何设计硬件与软件之间的接口。如：有无乘法指令</p></li><li><p>计算机组成原理：实现计算机体系结构所体现的属性，对程序员“透明”（具体指令的实现）</p><p>如何用硬件实现所定义的接口。如：如何实现乘法指令</p></li></ul></li></ul><h2 id="计算机发展历程（408大纲已删）"><a href="#计算机发展历程（408大纲已删）" class="headerlink" title="计算机发展历程（408大纲已删）"></a>计算机发展历程（408大纲已删）</h2><ul><li><p>计算机系统</p><ul><li><p>软件</p><ul><li><p>系统软件：用来管理整个计算机系统。</p><p>例如：操作系统、数据库管理系统(DBMS)、标准程序库(编程使用库)、网络软件(实现网络协议的模块)、语言处理程序(将高级语言转换成低级语言)、服务程序(调试程序)等。</p></li><li><p>应用软件：按任务需要编织成的各种程序。</p></li></ul></li><li><p>硬件</p></li></ul><p>计算机性能的好坏取决于“软”、“硬”件功能的总和。</p></li><li><p>硬件发展历程</p></li></ul><table><thead><tr><th>发展阶段</th><th>时间</th><th>逻辑元件</th><th>速度(次/秒)</th><th>内存</th><th>外存</th></tr></thead><tbody><tr><td>第一代</td><td>1946-1957</td><td><strong>电子管</strong></td><td>几千-几万</td><td>汞延迟线、磁鼓</td><td>穿孔卡片、纸带</td></tr><tr><td>第二代</td><td>1958-1964</td><td><strong>晶体管</strong></td><td>几万-几十万</td><td>磁芯存储器</td><td>磁带</td></tr><tr><td>第三代</td><td>1964-1971</td><td>中小规模<strong>集成电路</strong></td><td>几十万-几百万</td><td>半导体存储器</td><td>磁带、磁盘</td></tr><tr><td>第四代</td><td>1972-现在</td><td>大规模、超大规模集成电路</td><td>上千万-万亿</td><td>半导体存储器</td><td>磁盘、磁带、光盘、半导体存储器</td></tr></tbody></table><ul><li>发展趋势<ul><li>更微型、多用途</li><li>更巨型、超高速</li></ul></li></ul><h2 id="计算机系统层次结构"><a href="#计算机系统层次结构" class="headerlink" title="计算机系统层次结构"></a>计算机系统层次结构</h2><h3 id="计算机硬件的基本组成"><a href="#计算机硬件的基本组成" class="headerlink" title="计算机硬件的基本组成"></a>计算机硬件的基本组成</h3><h4 id="早期冯诺依曼结构"><a href="#早期冯诺依曼结构" class="headerlink" title="早期冯诺依曼结构"></a>早期冯诺依曼结构</h4><ul><li><strong>存储程序</strong>：将指令以二进制代码的形式事先输入计算机的主存储器，从首地址开始顺序执行指令至结束。<ul><li>针对问题：虽然早期冯诺依曼机的计算速度很快，但是 ENIAC 需要手动接线来控制计算，因此速度并不理想。</li></ul></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211002184657874.png" alt="image-20211002184657874" style="zoom:80%;"></li><li>在计算机系统中，软件和硬件在逻辑上是等效的。</li><li>冯·诺依曼计算机的特点：<ul><li><strong>计算机由五大部件组成：输入设备、输出设备、主存储器、运算器、控制器</strong></li><li>指令和数据以<strong>同等地位</strong>存于存储器，可<strong>按地址寻访</strong></li><li>指令和数据用二进制表示</li><li><strong>指令由操作码和地址码组成</strong></li><li>存储程序被首次提出</li><li><strong>以运算器为中心</strong>（输入/输出设备与存储器之间的数据传送通过运算器完成）</li></ul></li></ul><h4 id="现代计算机结构"><a href="#现代计算机结构" class="headerlink" title="现代计算机结构"></a>现代计算机结构</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211002185341049.png" alt="image-20211002185341049" style="zoom: 80%;"></li><li>现代计算机：<strong>以存储器为中心</strong></li><li><strong>CPU=运算器+控制器</strong></li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211002185424956.png" alt="image-20211002185424956" style="zoom: 80%;"></li><li>计组中的主机只包括：<strong>CPU 和主存</strong>。硬盘等不属于主机。</li></ul><h3 id="认识各个硬件部件"><a href="#认识各个硬件部件" class="headerlink" title="认识各个硬件部件"></a>认识各个硬件部件</h3><h4 id="主存储器"><a href="#主存储器" class="headerlink" title="主存储器"></a>主存储器</h4><ul><li>MAR（Memory Address Register）：<strong>存储地址寄存器</strong>，专门存放单次操作的数据地址</li><li>MDR（Memory Data Register）：<strong>存储数据寄存器</strong>，专门存放单次操作的数据内容</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211002191934325.png" alt="image-20211002191934325" style="zoom:80%;"></li><li>存储单元：每个存储单元用来存放一串二进制代码。每个存储单元对应一个地址信息（即 MAR 中应当指明的信息）</li><li>存储字(<strong>word</strong>)：每个存储单元中的一串二进制代码，就是一个存储字</li><li>存储字长：存储单元/每个存储字中，包含的二进制代码的位数。*<em>长度一般为：k * 8 bit*</em></li><li>存储元：即存储二进制的电子元件（电容），每个存储元可存 1bit</li><li><strong>数据在存储体内按地址存储</strong></li><li><strong>MAR 位数反映存储单元的个数</strong>：MAR=4位 -&gt; 总共有 24 个存储单元</li><li><strong>MDR 位数 = 存储字长</strong>：MDR = 16位 -&gt; 每个存储单元可存放 16bit，1个字(word) = 16bit</li><li>易混淆：<ul><li><strong>1个字节（Byte）= 8bit</strong></li><li>1B = 1个字节，1b = 1个bit</li><li><strong>字(word)的长度由机器本身决定</strong></li></ul></li></ul><h4 id="运算器"><a href="#运算器" class="headerlink" title="运算器"></a>运算器</h4><ul><li>运算器：用于实现算术运算（如：加减乘除）、逻辑运算（如：与或非）</li><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211002193854953.png" alt="image-20211002193854953" style="zoom:80%;"></li><li>ACC：累加器，用于存放操作数，或运算结果</li><li>MQ：乘商寄存器，在乘、除运算时，用于存放操作数或运算结果</li><li>X：通用的操作数寄存器/通用寄存器，用于存放操作数。通常有多个，不过只需要一个就可以完成大部分运算</li><li>ALU：（※核心元件）算术逻辑单元，通过内部复杂的电路实现算数运算、逻辑运算</li></ul><h4 id="控制器"><a href="#控制器" class="headerlink" title="控制器"></a>控制器</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211002194312723.png" alt="image-20211002194312723" style="zoom:80%;"></li><li>CU（Control Unit）：（※核心元件）控制单元，分析指令，给出控制信号</li><li>IR（Instruction Register）：指令寄存器，存放<strong>当前执行的指令</strong></li><li>PC（Program Counter）：程序计数器，存放<strong>下一条指令地址</strong>，有自动加1功能</li></ul><h4 id="计算机工作流程"><a href="#计算机工作流程" class="headerlink" title="计算机工作流程"></a>计算机工作流程</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211002200948818.png" alt="image-20211002200948818" style="zoom:80%;"></li><li>工作过程：<ul><li>初始状态：指令、数据存入主存，PC 指向第一条指令</li><li>PC -&gt; MAR -&gt; 从存储体中取指令放入 MDR -&gt; 由于处于取指令周期，将指令数据放入 IR -&gt; 由 CU 分析指令（前半段二进制码） -&gt; 变量地址（后半段二进制）放入 MAR -&gt; 从存储体中取指令放入 MDR -&gt; 由于执行指令周期，将变量数据放入运算器寄存器中 -&gt; 执行运算并将结果放入 ACC -&gt; …… </li></ul></li><li>注：现在的计算机通常把 MAR、MDR 也集成在 CPU 内</li></ul><h3 id="计算机系统的多级层次结构"><a href="#计算机系统的多级层次结构" class="headerlink" title="计算机系统的多级层次结构"></a>计算机系统的多级层次结构</h3><h4 id="五层机器结构"><a href="#五层机器结构" class="headerlink" title="五层机器结构"></a>五层机器结构</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211003001409155.png" alt="image-20211003001409155" style="zoom:80%;"></li><li>下层是上层的基础，上层是下层的扩展<ul><li>高级语言机器</li><li>汇编语言机器</li><li>操作系统机器</li><li>传统机器</li><li>微程序机器</li></ul></li></ul><h4 id="三个级别语言"><a href="#三个级别语言" class="headerlink" title="三个级别语言"></a>三个级别语言</h4><ul><li><img src="/2021/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-Part1/image-20211003002429374.png" alt="image-20211003002429374" style="zoom:80%;"></li><li>编译、汇编、解释程序，可统称“翻译程序”<ul><li>编译程序：将高级语言的源程序<strong>一次性</strong>翻译成汇编语言，或者直接翻译为机器语言的执行程序（只需翻译一次）</li><li>解释程序：将高级语言的源程序<strong>逐语句</strong>翻译成机器语言，并立即执行。（每次执行都要翻译）</li><li>汇编程序：将汇编语言翻译成机器语言。</li></ul></li></ul><h2 id="计算机的性能指标"><a href="#计算机的性能指标" class="headerlink" title="计算机的性能指标"></a>计算机的性能指标</h2><h3 id="存储器"><a href="#存储器" class="headerlink" title="存储器"></a>存储器</h3><ul><li><strong>MAR位数</strong>反映<strong>存储单元的个数</strong>（最多支持多少个）</li><li><strong>MDR位数</strong>=存储字长=<strong>每个存储单元的大小</strong></li><li><strong>总容量 = 存储单元个数×存储字长 bit = 存储单元个数×存储字长/8 Byte</strong> (1Byte=8bit)<ul><li>Eg：MAR为 32 位，MDR 为 8 位；总容量 = 2^32^ * 8 bit = 4 GB</li></ul></li></ul><blockquote><p>注：此处描述文件、容量大小等所用的 K、M、G、T </p><p>2^10^B：KB        2^20^B：MB        2^30^B：GB        2^40^B：TB</p></blockquote><h3 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h3><ul><li><strong>CPU 主频</strong>（时钟频率）：CPU内数字脉冲信号振荡的频率。<ul><li>1s 内进行的数字脉冲信号次数</li><li>CPU主频 = 1/CPU时钟周期</li><li>单位：赫兹，Hz</li></ul></li><li><strong>CPU 时钟周期</strong>：一次脉冲信号的周期。<ul><li>单位：微秒、纳秒</li></ul></li><li><strong>CPI</strong>（Clock cycle Per Instruction）：执行一条指令所需的时钟周期数<ul><li>不同的指令，CPI 有可能不同</li><li>相同的指令，根据不同 CPU 的实现方式不同、当前内存负载不同等等一系列状态，CPI 也可能不同</li><li>可以影响 CPI 的因素太多，一般只讨论平均 CPI</li></ul></li><li><strong>执行一条指令的耗时</strong> = CPI × CPU时钟周期</li><li><strong>CPU 执行时间</strong>（整个程序的耗时）= CPU时钟周期数 / 主频 =（指令条数 * CPI）/ 主频</li><li><strong>IPS</strong>（ Instructions Per Second ）：每秒执行多少条指令<ul><li>IPS =  主频 / 平均CPI</li><li>度量指标时经常使用 KIPS、MIPS</li></ul></li><li><strong>FLOPS</strong>（Floating-point Operations Per Second）：每秒执行多少次浮点运算<ul><li>度量指标时经常使用 KFLOPS、MFLOPS、GFLOPS、TFLOPS、PFLOPS、EFLOPS、ZFLOPS</li></ul></li></ul><blockquote><p>注：此处频率、速率常用的 K、M、G、T 为数量单位</p><p>K=Kilo=千=10^3^，M=Million=百万=10^6^，G=Giga=十亿=10^9^ ，T=Tera=万亿= 10^12^，P=10^3^T，E=10^3^P，Z=10^3^E</p></blockquote><h3 id="系统整体"><a href="#系统整体" class="headerlink" title="系统整体"></a>系统整体</h3><ul><li><p>数据通路带宽：数据总线一次所能并行传送信息的位数（各硬件部件通过数据总线传输数据）</p><ul><li>CPU &lt;=&gt; 主存储器 &lt;=&gt; I/O 设备。这之间的数据传输都需要依靠数据总线传输。</li></ul></li><li><p>吞吐量：指系统在单位时间内处理请求的数量。</p><ul><li>取决于信息能多快地输入内存，CPU能多快地取指令，数据能多快地从内存取出或存入，以及所得结果能多快地从内存送给一台外部设备。</li><li>这些步骤中的每一步都关系到主存，因此，系统吞吐量主要取决于主存的存取周期。</li></ul></li><li><p>响应时间：指从用户向计算机发送一个请求，到系统对该请求做出响应并获得它所需要的结果的等待时间。</p><ul><li>通常包括CPU时间（运行一个程序所花费的时间）与等待时间（用于磁盘访问、存储器访问、I/O操作、操作系统开销等时间）。</li></ul></li><li><p>基准程序：是用来测量计算机处理速度的一种实用程序，以便于被测量的计算机性能可以与运行相同程序的其它计算机性能进行比较。</p></li><li><p>思考：</p><ul><li>Q：主频高的 CPU 一定比主频低的CPU快吗？</li><li>A：不一定，如两个 CPU，A 的主频为 2GHz，平均 CPI=10；B 的主频 1GHz，平均 CPI=1…</li><li>Q：若 A、B 两个 CPU 的平均 CPI 相同，那么 A 一定更快吗？</li><li>A：也不一定，还要看指令系统。如：A 不支持乘法指令，只能用多次加法实现乘法；而 B 支持乘法指令。</li><li>Q：基准程序执行得越快说明机器性能越好吗？</li><li>A：基准程序中的语句存在频度差异，运行结果也不能完全说明问题，要看硬件具体的运用场景</li></ul></li></ul>]]></content>
    
    
    <summary type="html">计算机系统概述</summary>
    
    
    
    <category term="计算机组成原理" scheme="https://nephrencake.gitee.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>推荐系统-完结目录</title>
    <link href="https://nephrencake.gitee.io/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
    <id>https://nephrencake.gitee.io/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</id>
    <published>2021-09-27T09:11:49.000Z</published>
    <updated>2021-10-31T01:43:22.176Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推荐系统——完结目录"><a href="#推荐系统——完结目录" class="headerlink" title="推荐系统——完结目录"></a>推荐系统——完结目录</h1><p>教程传送门：</p><ul><li><p>老弓的学习日记：<a href="https://space.bilibili.com/34230158/search/video?keyword=%E6%8E%A8%E8%8D%90">https://space.bilibili.com/34230158/search/video?keyword=%E6%8E%A8%E8%8D%90</a></p></li><li><p>CSDN博文：<a href="https://blog.csdn.net/wuzhongqiang/article/details/107891787">https://blog.csdn.net/wuzhongqiang/article/details/107891787</a></p></li><li><p>王喆 – 《深度学习推荐系统》 </p></li><li><p><a href="https://github.com/RUCAIBox/RecBole">https://github.com/RUCAIBox/RecBole</a></p></li></ul><p>RecBole 项目救命，快速复现，我愿称之为神。</p><table><thead><tr><th align="center"><a href="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part1/">推荐系统-Part1——概述</a></th></tr></thead><tbody><tr><td align="center"><strong><a href="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/">推荐系统-Part2——传统推荐模型</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/">推荐系统-Part3——深度学习推荐模型</a></strong></td></tr></tbody></table>]]></content>
    
    
    <summary type="html">完结目录</summary>
    
    
    
    <category term="推荐系统" scheme="https://nephrencake.gitee.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="AI" scheme="https://nephrencake.gitee.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统-Part3</title>
    <link href="https://nephrencake.gitee.io/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/"/>
    <id>https://nephrencake.gitee.io/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/</id>
    <published>2021-09-27T08:58:57.000Z</published>
    <updated>2021-10-26T15:12:49.454Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推荐系统-Part3——深度学习推荐模型"><a href="#推荐系统-Part3——深度学习推荐模型" class="headerlink" title="推荐系统-Part3——深度学习推荐模型"></a>推荐系统-Part3——深度学习推荐模型</h1><p>[TOC]</p><h2 id="AutoRec-模型"><a href="#AutoRec-模型" class="headerlink" title="AutoRec 模型"></a>AutoRec 模型</h2><ul><li>AutoRec 是将自编码器的思想和协同过滤结合起来的一种单隐层的神经网络推荐模型。</li><li>基本原理：利用协同过滤中的共现矩阵，完成物品向量或者用户向量的自编码，再利用自编码的结果得到用户对物品的预估评分。</li><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210927202124750.png" alt="image-20210927202124750"></li><li>AutoRec 模型由于结构比较简单，表达能力不足，并且往往共现矩阵非常稀疏，更加加大了模型的预测难度。</li><li>所以，AutoRec 只是将深度学习思想应用于推荐系统的初步尝试，没有真正的投入到实践。</li></ul><h2 id="DeepCrossing-模型"><a href="#DeepCrossing-模型" class="headerlink" title="DeepCrossing 模型"></a>DeepCrossing 模型</h2><h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>输入一般会有类别型特征(如广告id)和数值型特征(如广告预算)两种情况。对于类别型特征，我们需要进行one-hot编码处理，而数值型特征，一般需要进行归一化处理，这样算是把数据进行了一个简单清洗。DeepCrossing 模型就是利用这些特征向量进行CRT预估。</p><p>为了完成端到端的训练，DeepCrossing 要在内部网络结构中解决如下问题：</p><ol><li>离散类特征编码后过于稀疏，不利于直接输入神经网络训练，需要解决稀疏特征向量稠密化的问题</li><li>如何解决特征自动交叉组合的问题</li><li>如何在输出层中达成问题设定的优化目标</li></ol><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><ul><li>DeepCrossing 都是常规的结构，但具有革命意义，因为没有任何人工特征工程的参与。</li><li>相比于 FM、FFM 只具备二阶特征交叉能力的模型，DeepCrossing 可以通过调整神经网络的深度进行特征之间的深度交叉，这也是 Deep Crossing 名称的由来。</li></ul><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210927223004876.png" alt="image-20210927223004876"></p><ol><li>Embedding 层：<ul><li>Embedding 的目的是将高维稀疏特征转化为低维稠密特征。</li><li>Feature#1 表示one-hot编码后非常稀疏的类别特征；Feature#2是数值型特征，不需要 embedding，直接进入 Stacking 层。</li><li>以字典形式存储每个类别特征域对应的 Embedding 矩阵，可以将 LabelEncoder 编码转化成一个数值型的特征向量。</li><li>每个类别特征域对应的 Embedding 矩阵的具体参数需要通过训练获得。</li></ul></li><li>Stacking 层：把不同的Embedding特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量，该层通常也称为连接层</li><li>Multiple Residual Units 层：采用了多层残差网络的多层感知机。这样对特征向量各个维度进行充分的交叉组合，使得模型能够抓取更多的非线性特征和组合特征信息。</li><li>Scoring 层：输出层。对于 CTR 预估二分类问题，采用逻辑回归；对于多分类，采用 Softmax 模型。</li></ol><h2 id="NeuralCF-模型"><a href="#NeuralCF-模型" class="headerlink" title="NeuralCF 模型"></a>NeuralCF 模型</h2><h3 id="推荐算法中的两个根本性思路"><a href="#推荐算法中的两个根本性思路" class="headerlink" title="推荐算法中的两个根本性思路"></a>推荐算法中的两个根本性思路</h3><ul><li><p>用户和物品的表征：即如何更好地把用户特征和物品特征信息表示出来。隐语义模型(MF)，使用了 embedding 的思路去表示用户和物品，从而用向量的乘积表示用户对物品的喜好程度。</p></li><li><p>特征交叉：即考虑特征之间的交互信息以丰富数据的表达能力。因子分解机(FM)系列，就将特征两两交叉。</p></li></ul><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928101611434.png" alt="image-20210928101611434"></p><h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><ol><li>矩阵分解算法：试图在协同过滤共现矩阵的基础上，使用更稠密的隐向量表示用户和物品，挖掘用户和物品的隐含兴趣和隐含特征，在一定程度上弥补了协同过滤模型处理稀疏矩阵能力不足的问题。</li><li>隐向量的学习过程，在深度学习中就可以看成是一个简单的神经网络表示；用户向量和物品向量可以看成 embedding 方法。而最终的评分值(预测值)，就是用户向量和物品向量内积后的相似度。</li><li>而问题就在相似度计算上：相似度计算一般采用余弦相似度。其中必然包含两个向量做<strong>内积</strong>的过程，用向量之间的夹角大小来衡量相似度。</li><li>按理说相似用户，其隐向量也应该相似。但是研究表明单靠夹角衡量相似度不一定可靠。</li></ol><blockquote><p>这是因为，隐向量内积获得夹角关系，是降维至二维的操作。这一步造成了不可逆的信息损失。</p></blockquote><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928101521711.png" alt="image-20210928101521711" style="zoom:67%;"><h3 id="原始-NeuralCF"><a href="#原始-NeuralCF" class="headerlink" title="原始 NeuralCF"></a>原始 NeuralCF</h3><p>Neural CF 模型是从传统的协同过滤方法(MF)的基础上进行的改进：</p><ol><li>把MF里面用户向量和物品向量的点积操作换成了<strong>多层神经网络</strong>，使得两个向量可以做更充分的交叉，得到更多有价值的特征组合信息；</li><li>另外一个就是神经网络的激活函数可以引入更多的非线性，让模型的表达能力更强。</li></ol><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928102102351.png" alt="image-20210928102102351"></p><h3 id="GMF-广义矩阵分解"><a href="#GMF-广义矩阵分解" class="headerlink" title="GMF 广义矩阵分解"></a>GMF 广义矩阵分解</h3><ul><li><p>原始的矩阵分解：</p><ol><li>用户向量与物品向量进行内积。</li></ol></li><li><p>GMF 广义矩阵分解：</p><ol><li>用户向量与物品向量进行元素积。</li><li>神经网络拟合互操作结果。</li></ol></li><li><p>GMF 步骤：</p><ol><li>将 NeuralCF 的 Neural CF Layers 替换成 user 和 item 的 Embedding 做点积（对应元素相乘），得到一个和 Embedding 等长的向量</li><li>隐形量进行对应元素相乘之后，给每个元素分配权重（也就是各个隐向量维度的重要性不一定相同），等价于一个 Linear 层。（如果 h 是一个全1向量的话，那模型实际上就是 MF 了。）</li><li>使用 sigmoid 预测分数。</li></ol></li><li><p>GMF 评价：虽然赋予了权重，但依然没有摆脱降维带来的信息损失。</p></li></ul><h3 id="NeuralCF-混合模型-MLP-GMF"><a href="#NeuralCF-混合模型-MLP-GMF" class="headerlink" title="NeuralCF 混合模型(MLP + GMF)"></a>NeuralCF 混合模型(MLP + GMF)</h3><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928111455990.png" alt="image-20210928111455990"></p><ul><li>混合了 MLP 和 GMF，需要注意的是两个墨西哥的 embedding 是分开的。</li><li>无论是 MLP 还是 GMF，都采用 Embedding 的方式生成隐向量，而不是传统的矩阵分解 MF。</li><li>根据评分矩阵生成的隐向量（User 和 Item 的 Embedding 向量），其中每一个维度都可以看做是一种抽象特征，而特征交叉就是针对 Embedding 向量的每个维度进行交叉。</li><li>特征交叉的方式：<ol><li>GMF：线性方式——向量点积</li><li>MLP：非线性方式——多层神经网络</li><li>NeuralCF ：混合上述两种方式</li></ol></li></ul><h2 id="PNN-模型"><a href="#PNN-模型" class="headerlink" title="PNN 模型"></a>PNN 模型</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><ul><li>NeuralCF 不足：只用到了用户向量和物品向量，可以看成一个MF的加强版，但是没有考虑其他特征信息，这无疑是一种浪费。</li><li>Product-based Neural Network(PNN)：在加入多组特征的基础上研究的特征交叉，这个模型和 Deep Crossing 模型的架构类似，只不过把 Deep Crossing 的 stacking 层换成了 Product 层，也就是不同特征的 embedding 向量不再是简单的堆叠，而是两两交互，更有针对性的获取特征之间的交叉信息。</li><li>在这里面研究了两两特征之间的内积和外积两种交叉方式，提出了具体的两种模型 IPNN 和 OPNN。</li></ul><h3 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928135958913.png" alt="image-20210928135958913"></p><ol><li>输入层：模型输入由 N 个特征域（Field）组成，都是离散稀疏的分类特征。数值型特征要在类别性特征交叉完之后再合并。</li><li>Embedding 层：和 DeepCrossing 一样，对每一个离散稀疏的 Field 特征域进行 Embedding 操作，参数是通过神经网络的反向传播进行学习。</li><li>Product 层：<ul><li>线性操作部分和乘积操作部分。丰富了特征交叉的能力。</li><li>z 区域表示单独特征，是单个特征与参数 w 的运算；p 区域表示交叉特征，是特征两两内积后的特征与参数 w 的计算。</li><li>p 部分又分为内积(IPNN)和外积(OPNN)</li></ul></li><li>L1、L2 层：两层全连接，输入包含了离散型特征 l<del>p</del> 和 l<del>z</del> 以及数值型特征。</li><li>输出层：二分类预测问题，使用 sigmoid 激活输出。</li></ol><h2 id="Wide-amp-Deep模型"><a href="#Wide-amp-Deep模型" class="headerlink" title="Wide&amp;Deep模型"></a>Wide&amp;Deep模型</h2><h3 id="记忆能力和泛化能力"><a href="#记忆能力和泛化能力" class="headerlink" title="记忆能力和泛化能力"></a>记忆能力和泛化能力</h3><ul><li><p>记忆能力：</p><ul><li>模型直接学习并利用历史数据中物品和特征的“共现频率”的能力。</li><li>记住了历史数据的分布特点，由原始数据直接影响结果。</li></ul></li><li><p>泛化能力：</p><ul><li>模型传递特征的相关性，以及发掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力。</li><li>让数据稀少的用户或者物品也能生成隐性特征，获得全局数据的泛化支撑，深度发掘数据中的潜在模式。</li></ul></li></ul><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><ul><li>在CTR预估任务中，利用手工构造的交叉组合特征来使线性模型具有“记忆性”，使模型记住共现频率较高的特征组合，往往能达到一个不错的baseline，且可解释性强。但这种方式有较为明显的缺点：1. 特征工程需要耗费太多精力。2. 对于未曾出现过的特征组合，权重系数为0，无法进行泛化。</li><li>为了加强模型的泛化能力，研究者引入了DNN结构、将高维稀疏特征编码为低维稠密的 Embedding vector，有效提高模型的泛化能力。但是，基于 Embedding 的方式遇到高度稀疏且高秩的共现矩阵时，长尾的一些特征值无法被充分学习，即使两者没有任何关系，也会由于 dense embedding 导致预测值非0，这便会造成模型泛化过度。</li><li>因此，现在主要的两种模型：<ol><li>协同过滤、逻辑回归等：能够从历史数据中学习到高频共现的特征组合能力，但是泛化能力不足；</li><li>矩阵分解、深度学习等：能够利用相关性的传递去探索历史数据中未出现的特征组合，挖掘数据潜在的关联模式，但是对于特定的场景（数据分布长尾，共现矩阵稀疏高秩）很难有效学习低纬度的表示，造成推荐的过渡泛化。</li></ol></li><li>Wide&amp;Deep 模型，将线性模型与 DNN  结合起来，在提高模型泛化能力的同时，兼顾模型的记忆性。</li><li>Wide&amp;Deep 后来成为推荐领域的经典模式，奠定了后面深度学习模型的基础。这个是一个里程碑式的改变，模型架构并没有多复杂，重点在于思想。</li></ul><h3 id="网络结构-2"><a href="#网络结构-2" class="headerlink" title="网络结构"></a>网络结构</h3><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928143913425.png" alt="image-20210928143913425" style="zoom:150%;"><p>W&amp;D 模型把单输入层的 Wide 部分和 Embedding + 多层全连接的部分连接起来，一起输入最终的输出层得到预测结果。单层的 wide 层善于处理大量的稀疏的 id 类特征，Deep 部分利用深层的特征交叉，挖掘在特征背后的数据模式。</p><h4 id="Wide"><a href="#Wide" class="headerlink" title="Wide"></a>Wide</h4><p>Wide 部分是一个广义的线性模型，用来处理神经网络并不擅长的离散特征。</p><ul><li>特征转换：cross-product transformation(原始特征的交互特征)，如果两个特征同时为1的时候，这个特征就是1，否则就是0，这是一种特征组合。</li><li>优化器：带 L1 正则的 FTRL 算法(Follow-the-regularized-leader)，该算法非常注重模型稀疏性质，可以使特征更加稀疏，即大部分参数都为0，就大大压缩了模型权重及特征向量的维度。</li></ul><blockquote><p>Wide 部分模型训练完之后留下来的特征都是非常重要的，那么模型的“记忆能力”就可以理解为发现”直接的”，“暴力的”，“显然的”关联规则的能力。</p></blockquote><h4 id="Deep"><a href="#Deep" class="headerlink" title="Deep"></a>Deep</h4><p>Deep 部分是 Embedding+MLP 的神经网络模型，用来挖掘藏在特征背后的数据模式。</p><p>输入的特征有两类：1. 数值型特征；2. 类别型特征(会经过 embedding)。</p><blockquote><p>大规模稀疏特征通过 Embedding 转化为低维密集型特征。</p><p>DNN 模型随着层数的增加，中间的特征就越抽象，也就提高了模型的泛化能力。</p></blockquote><h3 id="谷歌推荐系统的工业经验"><a href="#谷歌推荐系统的工业经验" class="headerlink" title="谷歌推荐系统的工业经验"></a>谷歌推荐系统的工业经验</h3><p>Wide&amp;Deep 模型本身的结构非常简单，但如何根据自己的场景去选择哪些特征放在 Wide 部分，哪些特征放在 Deep 部分是用好该模型的前提。</p><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928150837339.png" alt="image-20210928150837339"></p><p>Google Pay 通过检索来实现推荐的召回，将大规模的 App 应用缩小到小规模（例如100）的相关应用。然后在通过用户特征、上下文特征、用户行为特征等来建立推荐模型，估计用户点击每个 App 的概率分数进行排序，推荐 Top K 个 App。</p><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928151001327.png" alt="image-20210928151001327"></p><ol><li>Deep部分：全量的特征向量，包括用户年龄(age)、已安装应用数量(#app installs)、设备类型(device class)、已安装应用(installed app)、曝光应用impression app)等特征。其中，已安装应用，曝光应用等类别型特征，需要经过embedding层输入连接层，而数值型的特征和前面的特征拼接起来直接输入连接层，经过3层的Relu全连接层。</li><li>Wide部分：输入仅仅是已安装应用和曝光应用两类特征。其中已安装应用代表用户的历史行为，而曝光应用代表当前待推荐应用。选择这两部分是想发现当前曝光 APP 和用户已安装 APP 之间的关联，以充分发挥Wide的记忆能力，影响最终的得分。这部分是 L1 正则化的 FTRL 优化器，可能是因为这两个 id 类特征向量组合，在维度爆炸的同时，会让原本已经非常稀疏的 multihot 特征向量变得更加稀疏。因此采用FTRL过滤掉那些稀疏特征是非常好的工程经验。</li><li>两者结合：最后将两部分的特征再进行一个拼接，输出到 logistics Loss 层进行输出。</li></ol><h2 id="Deep-amp-Cross-模型"><a href="#Deep-amp-Cross-模型" class="headerlink" title="Deep&amp;Cross 模型"></a>Deep&amp;Cross 模型</h2><h3 id="基本思路-1"><a href="#基本思路-1" class="headerlink" title="基本思路"></a>基本思路</h3><ul><li>针对问题：Wide 部分有一个不足就是需要人工进行特征的组合筛选，过程繁琐且需要经验；2阶的FM模型在线性的时间复杂度中自动进行特征交互，但是这些特征交互的表现能力并不够，并且随着阶数的上升，模型复杂度会大幅度提高。</li><li>改进方向：用一个 Cross Network 替换掉 Wide 部分，来自动进行特征之间的交叉，并且网络的时间和空间复杂度都是线性的。</li></ul><h3 id="网络结构-3"><a href="#网络结构-3" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928152058933.png" alt="image-20210928152058933"></p><ol><li>Embedding and stacking layer：作用依然是把稀疏离散的类别型特征变成低维密集型。</li><li>Cross Network：目的是增加特征之间的交互力度。<ul><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928154001138.png" alt="image-20210928154001138"></li><li>交叉层的操作的二阶部分非常类似 PNN 提到的外积操作。</li><li>思想上类似于残差连接。</li><li>x1 中包含了所有的 x0 的 1、2 阶特征的交互，x2 包含了所有的 x1、x0 的 1、2、3 阶特征的交互。因此，交叉网络层的叉乘阶数是有限的。第 l 层特征对应的最高叉乘阶数为 l+1。</li><li>每一层特征权重共享。</li><li>交叉网络的时间和空间复杂度是线性的。因为每一层都只有w和b，没有激活函数的存在，相对于深度学习网络，交叉网络的复杂性可以忽略不计。</li><li>Cross网络是FM的泛化形式</li><li>当两个稀疏特征不全为 1 时，就不会训练权重，也就不会进行交互。</li></ul></li><li>Deep Network：与 D&amp;W 相同。</li><li>组合层：将两个网络的输出拼接，并且 Logistics 回归</li></ol><h2 id="FNN-模型"><a href="#FNN-模型" class="headerlink" title="FNN 模型"></a>FNN 模型</h2><h3 id="基本思路-2"><a href="#基本思路-2" class="headerlink" title="基本思路"></a>基本思路</h3><ul><li><p>FNN 是在 W&amp;D 之前提出的</p></li><li><p>针对问题：1. Embedding 层收敛速度过慢；2. FM 虽然考虑了二阶交叉，但无法延伸到高阶交叉信息的学习。</p></li><li><p>Embedding 层收敛速度慢的原因：1. 输入极端稀疏化，众多的 0 导致 w 无法更新；2. 参数量往往占整个神经网络的大半以上。</p></li><li><p>解决方案：</p><ol><li>提前预训练好一个 FM 模型，该模型可以用来得到各特征隐向量。</li><li>然后用 FM 训练好的特征隐向量对正式训练的模型进行 Embedding 层的权重初始化。</li></ol></li></ul><h3 id="网络结构与训练细节"><a href="#网络结构与训练细节" class="headerlink" title="网络结构与训练细节"></a>网络结构与训练细节</h3><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928181946861.png" alt="image-20210928181946861"></p><ul><li>类似于 Deep Crossing 模型的经典深度神经网络，从稀疏输入向量到稠密向量的转换依然是embedding结构。</li><li>FNN 模型针对 Embedding 层收敛速度慢的问题，用 FM 模型替换了下面的 Embedding 层，并且在模型的正式训练之前，先提前训练好 FM，然后用 FM 训练好的特征隐向量对正式训练的模型进行 Embedding 层的初始化操作。</li><li>采用两阶段训练方式，是为了将 FM 作为有价值先验知识加入到模型中。防止因为数据稀疏带来的歧义造成模型参数偏差、加速模型收敛、充分利用 FM 的特征表达。</li><li>类似于迁移学习。</li></ul><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ul><li>优点：<ul><li>采用预训练的方式</li><li>采用了组合模型的思想，引入 DNN，可以进行特征的高阶组合，减少特征工程。</li></ul></li><li>缺点：<ul><li>两阶段的训练模型，应用过程不方便，且模型能力受限于 FM 表征能力的上限。</li><li>只关注于高阶特征的组合交叉，容易丢失记忆能力。</li><li>全连接层将向量的所有元素加权求和，不会对Field进行区分，本质又回到了 Deep Crossing 的问题。</li><li>特征的学习没有针对性。</li><li>两阶段的训练方式给神经网络调参带来难题。</li></ul></li></ul><h2 id="DeepFM-模型"><a href="#DeepFM-模型" class="headerlink" title="DeepFM 模型"></a>DeepFM 模型</h2><h3 id="基本思路-3"><a href="#基本思路-3" class="headerlink" title="基本思路"></a>基本思路</h3><ul><li>存在问题：<ul><li>简单的线性模型虽然简单，同样这样是它的不足，就是限制了模型的表达能力，随着数据的大且复杂，这种模型并不能充分挖掘数据中的隐含信息，且忽略了特征间的交互，如果想交互，需要复杂的特征工程。</li><li>FM模型考虑了特征的二阶交叉，但是这种交叉仅停留在了二阶层次，虽然说能够进行高阶，但是计算量和复杂性一下子随着阶数的增加一下子就上来了。所以二阶是最常见的情况，会忽略高阶特征交叉的信息</li><li>DNN，适合天然的高阶交叉信息的学习，但是低阶的交叉会忽略掉</li><li>W&amp;D 把简单的LR模型和DNN模型进行了组合，使得模型既能够学习高阶组合特征，又能够学习低阶的特征模式。但是W&amp;D的wide部分是用了LR模型，这一块依然是需要一些经验性的特征工程，且 Wide 部分和 Deep 部分需要两种不同的输入模式，这个在具体实际应用中需要很强的业务经验。</li></ul></li></ul><h3 id="网络结构-4"><a href="#网络结构-4" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928185114897.png" alt="image-20210928185114897"></p><ul><li>DeepFM 也是两部分组成，左边的 FM + 右边的 DNN。</li><li>与 W&amp;D 和 DCN 相比，DNN 部分是相同的，区别在于 Wide 部分：1. W&amp;D 采用的是 LR；2. Deep&amp;Cross 采用 Cross 交叉网络；3. DeepFM 采用了 FM。后两者都是针对于 W&amp;D 的 wide 不具备自动特征组合能力的缺陷进行改进的。</li><li>Input、Embedding：与 Wide&amp;Deep 不同的是，DeepFM 中的 Wide 部分与 Deep 部分共享了输入特征，即 Embedding 向量。</li><li>FM：与 FNN 不同，FM 的隐向量参数也是当做学习参数一块学习的，以端到端方式训练整个网络。作者发现，通过高阶和低阶交互特征一块来进行反向传播更新参数反而会使得模型表现更佳，当然这也依赖于共享Embedding输入的策略。</li><li>Deep：与 Wide 使用相同的特征 Embedding。</li></ul><h3 id="论文细节"><a href="#论文细节" class="headerlink" title="论文细节"></a>论文细节</h3><ul><li><p>特征交互本质：</p><ul><li>二阶特征交互：通过对主流应用市场的研究，我们发现人们经常在用餐时间下载送餐的应用程序，这就表明应用类别和时间戳之间的（阶数-2）交互作用是 CTR 预测的一个信号。</li><li>三阶或者高阶特征交互：我们还发现男性青少年喜欢射击游戏和 RPG 游戏，这意味着应用类别、用户性别和年龄的(阶数-3)交互是 CTR 的另一个信号。</li><li>同时考虑低阶和高阶的交互特征，比单独考虑其中之一有更多的提升。</li></ul></li><li><p>人工特征工程的挑战性：</p><ul><li>一些特征工程比较容易理解，就比如上面提到的那两个，这时候往往我们都能很容易的设计或者组合那样的特征。 然而，其他大部分特征交互都隐藏在数据中，难以先验识别（比如经典的关联规则”尿布和啤酒“就是从数据中挖掘出来的，而不是由专家发现的），只能由机器学习自动捕捉。即使是对于容易理解的交互，专家们似乎也不可能详尽地对它们进行建模，特别是当特征的数量很大的时候。</li><li>所以，尽量的避免人工特征工程，构建端到端的推荐系统时作者研究该篇论文的另一动机所在（改进了W&amp;D）</li></ul></li><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928192807353.png" alt="image-20210928192807353" style="zoom:150%;"></li><li><p>FNN、PNN、W&amp;D 比较：</p><ul><li>FNN 模型：预训练的方式增加了开销，模型能力受限于 FM 表征能力的上限，且只考虑了高阶交互</li><li>PNN 模型：IPNN 的内积计算非常复杂，OPNN 的外积近似计算损失了很多信息，结果不稳定，且同样忽视了低阶交互</li><li>W&amp;D 模型：虽然是考虑到了低阶和高阶交互，兼顾了模型的泛化和记忆，但是 Wide 部分输入需要专业的特征工程经验，作者这里还举了个例子，比如用户安装应用和应用推荐中曝光应用的交叉，这个需要一些强的业务经验。</li></ul></li></ul><h2 id="NFM-模型"><a href="#NFM-模型" class="headerlink" title="NFM 模型"></a>NFM 模型</h2><h3 id="基本思路-4"><a href="#基本思路-4" class="headerlink" title="基本思路"></a>基本思路</h3><ul><li>NFM(Neural Factorization Machines)</li><li>FM 的问题：真实世界中的数据往往是非线性且内部结构复杂，而 FM 还是个线性模型，且交互仅仅限于二阶交互，所以作者认为 FM 在处理真实数据的时候，表达能力并不好。</li><li>改进方向：用一个表达能力更强的函数（神经网络）来替代原 FM 中二阶隐向量内积的部分。</li><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928213549920.png" alt="image-20210928213549920"></li></ul><h3 id="网络结构-5"><a href="#网络结构-5" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928213748093.png" alt="image-20210928213748093"></p><p>类似于 PNN，由 product_layer 换成了 Bi-Interaction Pooling，这个也是NFM的核心结构了。</p><ul><li>Input、Embedding、DNN、Output 层：同之前。</li><li>Bi-Interaction Pooling layer：在 Embedding 和 DNN 之间加入的特征交叉池化层。<ul><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928214540185.png" alt="image-20210928214540185"></li><li>可以认为是特征域和特征域之间做交叉</li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>NFM 相比较于其他模型的核心创新点是特征交叉池化层，实现了 FM 和 DNN 的无缝连接，NN 可以在 low level 就学习到包含更多信息的组合特征。集合了 FM 二阶交叉线性和 DNN 高阶交叉非线性的优势，非常适合处理稀疏数据的场景任务</li><li>在特征交叉层和隐藏层加入 dropout 技术，有利于缓解过拟合，dropout也是线性隐向量模型过拟合的策略</li><li>在 NFM 中，使用 BN+Dropout 的组合会使得学习的稳定性下降， 具体使用的时候要注意</li><li>特征交叉池化层能够较好的对二阶特征信息的交互进行学习编码，这时候，就会减少 DNN 的很多负担，只需要很少的隐藏层就可以学习到高阶特征信息，也就是 NFM 相比之前的 DNN，模型结构更浅，更简单，但是性能更好，训练和调参更容易</li><li>NFM 对参数初始化相对不敏感，也就是不会过度依赖于预训练，模型的鲁棒性较强</li><li>深度学习模型的层数不总是越深越好，太深了会产生过拟合的问题，且优化起来也会困难</li></ul><h2 id="AFM-模型"><a href="#AFM-模型" class="headerlink" title="AFM 模型"></a>AFM 模型</h2><h3 id="基本思路-5"><a href="#基本思路-5" class="headerlink" title="基本思路"></a>基本思路</h3><ul><li>AFM(Attentional Factorization Machines)</li><li>针对问题：NFM 的特征交叉池化层对于各个交叉特征向量进行加和，没有考虑不同特征对结果的影响程度。这可能会影响最后的预测效果，因为不是所有的交互特征都能够对最后的预测起作，而没有用的交互特征就可能会产生噪声。</li><li>改进方向：引入注意力机制，学习不同交叉特征对于结果的不同影响程度。</li></ul><h3 id="网络结构-6"><a href="#网络结构-6" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928220446143.png" alt="image-20210928220446143"></p><ul><li><p>Input、embedding、Output 层：同之前。</p></li><li><p>Pair-wise Interaction Layer：和 NFM 是一样的，采用的也是每对 Embedding 向量进行各个元素对应相乘（element-wise product）交互。</p></li><li><p>Attention based Pooling layer：在 Pair-wise Interaction Layer 和 Output layer 中间加入的 Attention 注意力网络。</p><ul><li><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928221437267.png" alt="image-20210928221437267"></p></li><li><p>主要思路：不同的特征交互向量在将它们压缩为单个表示时，根据对预测结果的影响程度给其加上不同权重。</p></li><li><p>其中aij表示v<del>i</del>⊙v<del>j</del>对的注意力分数，表示该交互特征对于预测目标的重要性程度。这个注意力分数可以作为参数然后通过最小化预测损失来进行学习，但是对于从未在训练数据中共同出现的特征，就无法估计其交互作用的注意力得分。所以为了解决泛化问题，这里才使用了一个多层感知器（MLP）将注意力得分参数化，就是上面的那个 Attention Net。</p></li><li><p>该注意力网络的结构是一个简单的单全连接层加 softmax 输出层的结构</p></li><li><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928221953070.png" alt="image-20210928221953070"></p></li></ul></li></ul><h2 id="DIN-模型"><a href="#DIN-模型" class="headerlink" title="DIN 模型"></a>DIN 模型</h2><h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><ul><li>Deep Interest Network(DIN) 模型非常重要，是基于业务的观察，从实际应用的角度进行的模型改进，完全符合以需求为导向的创新原则。</li><li>DIN 的论文写得非常精彩，建议读原文，里面不仅提出了DIN模型，并基于真实场景下大规模数据集的模型训练问题，提出了两种重要的训练技术。</li><li>DIN 模型的创新点或者解决的问题就是使用了注意力机制来对用户的兴趣动态模拟，非常注重用户的历史行为特征（历史购买过的商品或者类别信息）。</li></ul><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul><li>拥有大量的用户历史行为信息。</li><li>用户兴趣多种多样，并且变化多端。</li><li>用户的兴趣往往蕴含在历史行为当中。</li><li>用户是否点击商品，很大程度上依赖于他的历史行为。</li><li>广告推荐两个主要的阶段：召回和排序。<ul><li>召回(matching stage): 通过协同过滤等方法生成与访问用户相关的候选广告列表</li><li>排序：就是通过排序模型来预测用户对于候选广告的点击概率，根据概率生成推荐列表</li></ul></li><li>大部分的推荐场景下都会有这两大步骤， 而商品广告推荐和一些其他推荐有些区别的是很注重用户的历史行为，因为这个直接与用户的兴趣相关， 而用户兴趣又反过来和商品挂钩。</li></ul><h3 id="基本思路-6"><a href="#基本思路-6" class="headerlink" title="基本思路"></a>基本思路</h3><ul><li>Embeding&amp;MLP 模型的存在问题：1. 无法表达用户广泛的兴趣；2. 并不是用户所有的历史行为特征都会对某个商品广告点击预测起到作用。</li><li>改进方向：1. 应该考虑到用户的历史行为商品与当前商品广告的一个关联性（局部兴趣），模拟出用户对商品的兴趣程度（注意力得分）。</li></ul><h3 id="特征表示"><a href="#特征表示" class="headerlink" title="特征表示"></a>特征表示</h3><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928231725615.png" alt="image-20210928231725615" style="zoom: 80%;"><p>工业上的CTR预测数据集一般都是 multi-group categorial form 的形式，就是类别型特征最为常见。</p><h3 id="网络结构-7"><a href="#网络结构-7" class="headerlink" title="网络结构"></a>网络结构</h3><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210929082029917.png" alt="image-20210929082029917"><p>base 模型：</p><ol><li>Embedding layer：把高维稀疏的输入转成低维稠密向量。multi-hot 编码会得到一个 embedding 向量的列表。</li><li>pooling layer：pooling 层的作用是将用户的历史行为，即不定长的 embedding 向量，最终变成一个定长的向量。</li><li>Concat layer：把所有特征 embedding 向量，从特征维度拼接，作为MLP的输入。</li><li>MLP：普通全连接。</li><li>Loss：二分类的问题。<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210929080649441.png" alt="image-20210929080649441"></li></ol><p>存在问题：</p><ol><li>用户的历史行为特征和当前的候选广告特征拼接之前，一点交互过程都没有。</li><li>拼接之后给 MLP，虽然有交互，但是既丢失了部分信息，也引入了部分噪声<ul><li>丢失信息：已经无法看出到底用户历史行为中的哪个商品与当前商品比较相关。</li><li>引入噪声：因为当前候选广告商品交互的是池化之后的历史特征 embedding，其包含了所有的历史商品信息。</li></ul></li></ol><p>改进思路：</p><ol><li>对于给定的候选广告，DIN 通过考虑历史行为与其相关性，自适应地计算用户兴趣的表示向量，该向量随不同广告而变化。</li></ol><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210928231838285.png" alt="image-20210928231838285"><p>DIN 模型的 Activation Unit：</p><ul><li>位于 embedding 和 pooling 层之间。</li><li>用于计算用户历史行为商品和当前候选商品之间的相关性，即每个历史商品的权重。</li><li>不进行 softmax，目的是为了保留用户的兴趣强度。否则可能会因为过长的序列和 0-1 映射，使得权重差距不明显。</li></ul><h3 id="论文细节-1"><a href="#论文细节-1" class="headerlink" title="论文细节"></a>论文细节</h3><ol><li><p>Mini-batch Aware Regularization：为了防止模型过拟合，一般会加入正则化，而L2正则化加入的时候，是对于所有的参数都会起作用。像这种真实数据集中，每个mini-batch的样本特征是非常稀疏的，这种情况下根本就没有必要考虑所有的参数进去，这个复杂度会非常大。仅约束那些在当前 mini-batch 样本中出现的特征(不为0的那些特征) embedding 就可以了。</p></li><li><p>Data Adaptive Activation Function：随着数据分布而动态调整的自适应激活函数。</p><ul><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210929084620252.png" alt="image-20210929084620252"></li><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210929084636564.png" alt="image-20210929084636564"></li><li>这里的 E(s) 和 Var(s) 是每个 mini-batch 里面样本的均值和方差，当然这是训练集部分，测试集的时候采用的是在数据上平滑的均值和方差。由于把均值和方差考虑进去了，那么这个函数的调整就可以根据数据的分布进行自适应，这样会更加的灵活且合理。</li></ul></li><li><p>衡量模型改进程度的评价标准 RelaImpr：</p><ul><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210929084951620.png" alt="image-20210929084951620"></li><li>基于 base model 评价模型的提高程度</li><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210929085036339.png" alt="image-20210929085036339"></li><li>#inpressiong<del>i</del> 和 AUC<del>i</del> 分别是 i 用户的喜好和 AUC。</li><li>AUC = P(P<del>正样本</del> &gt; P<del>负样本</del>)</li></ul></li></ol><h2 id="DIEN-模型"><a href="#DIEN-模型" class="headerlink" title="DIEN 模型"></a>DIEN 模型</h2><h3 id="基本思路-7"><a href="#基本思路-7" class="headerlink" title="基本思路"></a>基本思路</h3><ol><li><p>DIN有两点不足：</p><ul><li>直接将用户过去的历史行为当做了用户的兴趣，缺乏对具体行为背后的潜在兴趣进行专门的建模。只计算了当前候选广告与过去历史行为的相关性，根据这个做出推荐，但历史行为之间的依赖关系并没有很好的模拟出来。历史行为其实也是一个随时间排序的序列，既然是时间相关的序列，就一定存在或深或浅的前后依赖关系，而这样的序列信息或者说依赖关系对推荐过程是非常有价值的(能够反映用户背后的潜在兴趣变化)，大量研究发现这种信息能够用于构建更丰富的用户模型并发现附加的行为模式，而DIN模型包括之前的MLP系列模型都无法学习到这样的序列依赖关系。</li><li>DIN 模型没法捕捉到用户的兴趣变化过程，作者在论文中用到了一个词叫做”兴趣漂移”，即在相邻的访问中，用户的意图可能非常不同，用户的一个行为可能依赖于很久以前的行为。而一个用户对不同目标项的点击行为受到不同兴趣部分的影响，如果没法学习用户的兴趣演化，就很容易基于用户所有购买历史行为综合推荐，而不是针对“下一次购买”推荐，DIN虽然是能够更加注重与当前候选物品相关的历史行为，但是这些行为并不能表示出用户的兴趣变化过程，所以序列信息是非常之重要的。</li><li>总结：DIN 忽略了序列信息，容易基于历史行为综合推荐，而不是针对下一次购买进行推荐。</li></ul></li><li><p>序列的重要性：</p><p>例如：上周一位用户在挑选一双篮球鞋，这位用户上周的行为序列都会集中在篮球鞋这个品类的商品上，但是完成购买之后，本周的购物兴趣可能变成买机械键盘。</p><ul><li>加强了最近行为对下次行为预测的影响。比如上面的例子，用户近期购买机械键盘的概率会明显高于再买篮球鞋的概率。</li><li>序列模型能够学习到购买趋势的信息。这个感觉就是在建模用户的兴趣演化，在上面例子中，序列模型能在一定程度上建立“篮球鞋”到“机械键盘”的转移概率，如果这个转移概率在全局统计意义上足够高，那么用户购买篮球鞋时，推荐机械键盘也会是一个不错的选项。直观上，两者的用户群体很有可能一致。</li></ul></li><li><p>兴趣的实际状况：</p><ul><li>同一时刻下，可能会拥有多种不同的兴趣，应该用兴趣状态描述</li><li>兴趣是动态变化的，拥有各自的演化过程</li><li>兴趣发展是有一定前因后果的</li><li>当前时刻的兴趣，都有可能由上个时刻的兴趣概率转移</li></ul></li><li><p>改进方向：将 DIN 的 embedding 到 concat 之间的部分替换成兴趣进化网络。</p></li></ol><h3 id="网络结构-8"><a href="#网络结构-8" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part3/image-20210929091931368.png" alt="image-20210929091931368"></p><ol><li>行为序列层(Behavior Layer)：主要作用是把原始的 id 类行为序列转成 Embedding 行为序列， 和 DIN 下面的 Embedding 层一样。</li><li>兴趣抽取层(Interest Extractor Layer): 主要作用是通过模拟用户的兴趣迁移过程，挖掘提取用户行为背后潜藏的状态，DIN 模型没有这个东西。这个也就是 DIN 的不足①的改进，能够学习到历史序列行为之间的序列依赖关系。<ul><li>auxiliary loss：帮助监督更新这一层的 GRU 参数。让当前时刻输出的隐藏状态 h<del>t</del> 尽量的与下一个时刻用户点击的行为 embedding 相似，与下一个时刻里面用户没有点击过的行为 embedding 越远。</li><li>原来的 GRU 是只在最后一个时间步进行输出，然后与真实 label 进行了一个交叉熵的计算，而改进的这个核心点是每个时间步都会有一个输出，然后都会与一个 label 进行一个交叉熵计算，所以这里就会多出了时间步 t 维度上的加和损失，还有点不同就是 label 这里，当前时间步用的 label 是下一个时间步的输入值。</li><li>引入 auxiliary loss 的原因：目标商品的点击行为是由最后的兴趣触发，而 label 只能监督最后的兴趣，对隐藏层的状态不能有效监督。</li></ul></li><li>兴趣进化层(Interest Evolving Layer)：主要作用是通过兴趣抽取层在兴趣抽取层的基础上加入注意力机制，模拟与目标广告相关的兴趣进化过程。这个和 DIN 引入注意力的思路其实是一脉相承的， 注意力的计算得分都是一致的。只不过这里这里不是把这个注意力计算的得分与兴趣抽取层的 h 简单加权组合了，而是把这个注意力操作嵌入到了 GRU 更新门里面去，形成了一个 AUGRU 的结构，用这个层来更有针对性的模拟与目标广告相关的兴趣进化路径。</li></ol>]]></content>
    
    
    <summary type="html">深度学习推荐模型</summary>
    
    
    
    <category term="推荐系统" scheme="https://nephrencake.gitee.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>推荐系统-Part2</title>
    <link href="https://nephrencake.gitee.io/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/"/>
    <id>https://nephrencake.gitee.io/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/</id>
    <published>2021-09-26T14:49:08.000Z</published>
    <updated>2021-10-26T15:12:46.766Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推荐系统-Part2——传统推荐模型"><a href="#推荐系统-Part2——传统推荐模型" class="headerlink" title="推荐系统-Part2——传统推荐模型"></a>推荐系统-Part2——传统推荐模型</h1><p>[TOC]</p><h2 id="协同过滤-CF"><a href="#协同过滤-CF" class="headerlink" title="协同过滤(CF)"></a>协同过滤(CF)</h2><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><blockquote><p>协同过滤是推荐系统的必看内容，因为该算法是后来推荐算法的基本思想。</p></blockquote><ul><li>基本思想：根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品(基于对用户历史行为数据的挖掘发现用户的喜好偏向，并预测用户可能喜好的产品进行推荐)。</li><li>一般是仅仅基于用户的行为数据（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）。</li><li>目前应用比较广泛的协同过滤算法是基于邻域的方法， 而这种方法主要有下面两种算法：<ul><li>基于用户的协同过滤算法(UserCF): 给用户推荐和他兴趣相似的其他用户喜欢的产品</li><li>基于物品的协同过滤算法(ItemCF): 给用户推荐和他之前喜欢的物品相似的物品</li></ul></li></ul><h3 id="目标场景"><a href="#目标场景" class="headerlink" title="目标场景"></a>目标场景</h3><ul><li><p>存在一系列数据：行为用户记录，列为用户行为目标(包括点击、点赞、购买哪些商品/视频等一些列行为)。</p><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927170905175.png" alt="image-20210927170905175"></p></li></ul><h3 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h3><ol><li><p>杰卡德相似度：衡量两个集合的相似度，也就是交并比。</p><ul><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926195359526.png" alt="image-20210926195359526"></li></ul></li><li><p>余弦相似度：衡量了两个用户向量(矩阵中的行向量) i 和 j 之间的向量夹角的大小，夹角越小，说明相似度越大，两个用户越相似。</p><ul><li><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926195502822.png" alt="image-20210926195502822"></p></li><li><p>局限性：对于不规范的评分数据敏感。例如：A 用户比较喜欢打高分(以95分为优秀)，而 B 用户比较喜欢打低分(以80分为优秀)。</p></li><li><p>```python<br>from sklearn.metrics.pairwise import cosine_similarity</p><p>i = [1, 0, 0, 0]<br>j = [1, 0.5, 0.5, 0]<br>consine_similarity([a, b])</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">3. 皮尔逊相关系数：先将两个向量都减去分别的均值，消除偏置的影响，然后再计算向量角度 consine 值。</span><br><span class="line"></span><br><span class="line">   - ![image-20210926202642997](推荐系统-Part2&#x2F;image-20210926202642997.png)</span><br><span class="line"></span><br><span class="line">   - &#96;&#96;&#96;python</span><br><span class="line">     from scipy.stats import pearsonr</span><br><span class="line">     </span><br><span class="line">     i &#x3D; [1, 0, 0, 0]</span><br><span class="line">     j &#x3D; [1, 0.5, 0.5, 0]</span><br><span class="line">     pearsonr(i, j)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>其他：欧式距离、曼哈顿距离、马氏距离。</p><ul><li>欧式距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。</li><li>如果要分析两个用户对于不同视频的偏好：以观看比例作为特征时，用户A观看向量(0,1)，用户B观看向量(1,0)，此时二者的余弦距离很大，而欧式距离很小。因此，更关注相对差异，应当用余弦距离。</li><li>如果要分析用户活跃度：以登录次数和平均观看时长作为特征时，余弦距离会认为(1,10)和(10,100)两个用户距离很近，但显然这两个用户活跃度是有着极大差异的。此时我们关注的是数值绝对差异，应当使用欧式距离。</li></ul></li></ol><h3 id="最终结果预测"><a href="#最终结果预测" class="headerlink" title="最终结果预测"></a>最终结果预测</h3><ol><li>利用 用户相似度(Wu,s) 和 相似用户的评价(Rs,p) 加权平均获得用户的评价预测：<ul><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926211009685.png" alt="image-20210926211009685"></li></ul></li><li>在用户相似度和相似用户的评价加权平均的基础上，将用户的评价都减去此用户的所有评分的均值：<ul><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926211027717.png" alt="image-20210926211027717"></li></ul></li></ol><h3 id="基于用户的协同过滤"><a href="#基于用户的协同过滤" class="headerlink" title="基于用户的协同过滤"></a>基于用户的协同过滤</h3><ul><li><p>对于需要个性化推荐的用户A，可以先找到和用户A有相似兴趣的其他用户，然后把那些用户喜欢的且用户A没有听说过的物品推荐给A。</p><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927170929803.png" alt="image-20210927170929803"></p></li></ul><p>步骤：</p><ol><li>根据已有的用户向量，计算与目标用户的相似度，找到最相似的 n 个用户。</li><li>根据这 n 个相似用户对目标行为的分数，加权平均来预测目标用户对目标行为的分数。</li></ol><p>缺点：</p><ol><li>数据稀疏性：在大型的电子商务中物品繁多，不同用户之间买的物品重叠性低，难以找到偏好相似的用户。<strong>UserCF 不适用于那些正反馈获取较困难的应用场景</strong>。</li><li>算法扩展性：需要维护用户相似度矩阵，该矩阵的存储开销大，且随着用户数量的增加而增加。<strong>不适合用户数据量大的情况使用</strong>。</li></ol><h3 id="基于物品的协同过滤"><a href="#基于物品的协同过滤" class="headerlink" title="基于物品的协同过滤"></a>基于物品的协同过滤</h3><ul><li><p>对于需要推广的物品A，可以先找到和物品A有相似用户群体的其他物品，然后把物品A推荐给相似物品的用户群体。</p></li><li><p>减缓 UserCF 的数据稀疏性和算法扩展性问题。</p><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927171027279.png" alt="image-20210927171027279"></p></li></ul><p>步骤：</p><ol><li>根据已有的物品向量，计算与目标物品之间的相似度，找到最相似的 n 个物品。</li><li>根据物品的相似度和用户的历史行为给用户生成推荐列表（购买了该商品的用户也经常购买的其他商品）</li></ol><p>优点：</p><ol><li>Item-based 算法的预测结果比 User-based 算法的质量要高一点。</li><li>由于 Item-based 算法可以预先计算好物品的相似度，所以在线的预测性能比 User-based 算法高。</li></ol><p>缺点：</p><ol><li>依然存在数据稀疏性。</li><li>用户的增长也会导致物品相似度矩阵维护难度变大。</li></ol><h3 id="应用场景区别"><a href="#应用场景区别" class="headerlink" title="应用场景区别"></a>应用场景区别</h3><table><thead><tr><th></th><th>UserCF</th><th>ItemCF</th></tr></thead><tbody><tr><td>性能</td><td>适合用户较少的场合，否则维护用户相似度矩阵代价较大</td><td>适合物品数远小于用户数的场合。如果物品太多，则维护物品相似度矩阵代价大</td></tr><tr><td>领域</td><td>时效性较强、具备强社交性、用户个性化弱的场合（热点新闻、发觉潜在喜好）</td><td>兴趣变化较为稳定、物品更新慢、用户个性化强烈的领域（推荐艺术品， 音乐， 电影）</td></tr><tr><td>实时性</td><td>用户的新行为不一定对推荐结果造成立即变化</td><td>用户的新行为一定会导致推荐结果的实时变化</td></tr><tr><td>推荐理由</td><td>很难提供令用户信服的推荐结识</td><td>利用用户的历史行为为用户推荐解释，具有信服力</td></tr></tbody></table><h3 id="存在问题分析"><a href="#存在问题分析" class="headerlink" title="存在问题分析"></a>存在问题分析</h3><ol><li>无法利用更多的信息。完全没有利用到物品或者用户的自身属性，仅利用用户与物品的交互信息就可以实现推荐，是一个可解释性强，直观、但也存在问题的模型。</li><li>较差的稀疏向量处理能力，泛化能力弱。热门物品具有很强的头部效应，容易跟大量物品产生相似，而尾部物品由于特征向量稀疏，导致很少被推荐。</li></ol><h2 id="矩阵分解-MF"><a href="#矩阵分解-MF" class="headerlink" title="矩阵分解(MF)"></a>矩阵分解(MF)</h2><h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><ul><li>矩阵分解算法通过引入了隐向量的概念，加强了模型处理稀疏矩阵的能力，也为后续深度学习推荐系统算法中 Embedding 的使用打下了基础。</li><li>也就是加入隐含层，抽象化用户和物品之间的特征。这个特征信息需要去挖掘。</li><li>针对问题：1. 协同过滤处理稀疏向量能力差；2. 维护相似矩阵难度大</li></ul><h3 id="隐语义模型"><a href="#隐语义模型" class="headerlink" title="隐语义模型"></a>隐语义模型</h3><ul><li><p>核心思想：通过隐含特征（latent factor）联系用户兴趣和物品（item），基于用户的行为找出潜在的主题和分类，然后对 item 进行自动聚类，划分到不同类别/主题（用户的兴趣）。</p><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926223713170.png" alt="image-20210926223713170"></p></li><li><p>矩阵分解算法将 m×n 维的共享矩阵 R分解成 m×k 维的用户矩阵 U 和 k×n 维的物品矩阵 V 相乘的形式。其中 m 是用户数量，n 是物品数量，k是隐向量维度（隐含特征个数），只不过这里的隐含特征变得不可解释了。k 的大小决定了隐向量表达能力的强弱，k 越大，表达信息就越强。</p></li><li><p>抽象的特征使得用户和物品向量更加稠密，解决了处理稀疏向量能力差的问题。</p></li><li><p>维护相似矩阵由用户相似矩阵的 m×m 和物品矩阵的 n×n 降低到了 (m+n)×k，解决了维护相似矩阵难度大的问题。</p></li><li><p>预测评分公式：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926224159177.png" alt="image-20210926224159177"></p></li></ul><h3 id="MF的几种方式"><a href="#MF的几种方式" class="headerlink" title="MF的几种方式"></a>MF的几种方式</h3><ul><li>特征值分解(EVD)：1. 要求矩阵是方阵，是不符合实际场景的。</li><li>奇异值分解(SVD)：1. 要求矩阵稠密，而实际场景非常稀疏。2. 一旦补全会导致空间复杂度非常高，且补得不一定正确。3. 计算复杂度非常高，基本无法使用。</li></ul><h4 id="Basic-SVD-LFM、Funk-SVD"><a href="#Basic-SVD-LFM、Funk-SVD" class="headerlink" title="Basic SVD(LFM、Funk-SVD)"></a>Basic SVD(LFM、Funk-SVD)</h4><ul><li>把矩阵分解问题转换成最优化问题，通过梯度下降进行优化。</li><li>预测函数：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926230607417.png" alt="image-20210926230607417"></li><li>损失函数：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926230642256.png" alt="image-20210926230642256"></li><li>优化目标：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926230705295.png" alt="image-20210926230705295"></li><li>求梯度：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926230925915.png" alt="image-20210926230925915"></li><li>梯度更新：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926231028214.png" alt="image-20210926231028214"></li></ul><h4 id="RSVD"><a href="#RSVD" class="headerlink" title="RSVD"></a>RSVD</h4><ul><li>在 Basic SVD 的基础上，在目标函数中加入正则化参数（加入惩罚项），对于目标函数来说，Q矩阵和V矩阵中的所有值都是变量，这些变量在不知道哪个变量会带来过拟合的情况下，对所有变量都进行惩罚：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926233756786.png" alt="image-20210926233756786"></li><li>梯度的更新公式就变成了：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926233828417.png" alt="image-20210926233828417"></li></ul><p>进一步优化：</p><ul><li><p>Netfix Prize中提出了另一种LFM，在原来的基础上加了偏置项，来消除用户和物品打分的偏差，预测公式：</p><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210926233946536.png" alt="image-20210926233946536"></p></li><li><p>这个预测公式加入了3项偏置μ,b<del>u</del>,b<del>i</del>：</p><ul><li>μ：训练集中所有记录的评分的全局平均数。在不同网站平台中， 因为网站定位和销售物品不同，网站的整体评分分布也会显示差异。比如有的网站中用户就喜欢打高分，有的网站中用户就喜欢打低分。而全局平均数可以表示网站本身对用户评分的影响。</li><li>b<del>u</del>：用户偏差系数，可以使用定值(用户 u 给出的所有评分的均值)，也可以当做训练参数。这一项表示了用户的评分习惯中和物品没有关系的那种因素。比如有些用户比较苛刻，对什么东西要求很高，那么他评分就会偏低；而有些用户比较宽容，对什么东西都觉得不错，那么评分就偏高。</li><li>b<del>i</del>：物品偏差系数，可以使用定值(物品 i 收到的所有评分的均值)，也可以当做训练参数。这一项表示了物品接受的评分中和用户没有关系的因素。比如有些物品本身质量就很高，因此获得的评分相对比较高，有的物品本身质量很差，因此获得的评分相对较低。</li></ul></li></ul><h4 id="SVD"><a href="#SVD" class="headerlink" title="SVD++"></a>SVD++</h4><ul><li>之前都是从用户和用户之间的相似度、其他用户对物品的评分进行预测推荐，而缺少考虑用户本身的历史行为对评分预测的影响。</li><li>即，之前只考虑分解当前的共现矩阵，注意：此时并没有考虑该用户评分的历史物品。</li><li>经过一系列的演化，最终预测公式为：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927094726696.png" alt="image-20210927094726696"></li><li>其中，1/√|N(u)| 是为了保证方差一致性。</li></ul><h3 id="优点-amp-局限性"><a href="#优点-amp-局限性" class="headerlink" title="优点&amp;局限性"></a>优点&amp;局限性</h3><p>优点：</p><ol><li>泛化能力强，一定程度上解决了数据稀疏的问题，通过为用户和物品的隐语义标签抽象化了特征。</li><li>空间复杂度低。UserCF(m×m)、ItemCF(n×n) -&gt; 用户隐向量(m×k) + 物品隐向量(n×k)</li><li>更好的扩展性和灵活性。可以和其他特征组合拼接，与深度学习网络无缝结合。</li></ol><p>局限性：</p><ol><li>与协同过滤一样，无法利用用户特征、物品特征、上下文特征。</li><li>缺乏用户历史行为时，无法推荐。</li></ol><h2 id="逻辑回归-LR"><a href="#逻辑回归-LR" class="headerlink" title="逻辑回归(LR)"></a>逻辑回归(LR)</h2><h3 id="基本思想-1"><a href="#基本思想-1" class="headerlink" title="基本思想"></a>基本思想</h3><ul><li>基础知识：逻辑回归、极大似然估计。<ul><li>逻辑回归做为神经网络中的最基础单一神经元</li><li>逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</li></ul></li><li>针对问题：协同过滤无法有效利用用户、物品、上下文等多种不同的特征。</li><li>改进方向：将推荐问题转化成了一个二分类问题。</li></ul><h3 id="推荐过程"><a href="#推荐过程" class="headerlink" title="推荐过程"></a>推荐过程</h3><ol><li><p>将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转成数值型向量。</p></li><li><p>确定逻辑回归的优化目标，比如把点击率预测转换成二分类问题，这样就可以得到分类问题常用的损失作为目标，训练模型。</p></li><li><p>在预测的时候，将特征向量输入模型产生预测，得到用户“点击”物品的概率。</p></li><li><p>利用点击概率对候选物品排序，得到推荐列表。</p><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927171345229.png" alt="image-20210927171345229"></p></li></ol><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>LR模型形式简单，可解释性好，从特征的权重可以看到不同的特征对最后结果的影响。</li><li>训练时便于并行化，在预测时只需要对特征进行线性加权，性能好，适合处理海量id类特征。用id类特征有一个很重要的好处，就是防止信息损失（相对于范化的 CTR 特征），对于头部资源会有更细致的描述。</li><li>资源占用小，尤其是内存。在实际的工程应用中只需要存储权重比较大的特征及特征对应的权重。</li><li>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值。</li><li>工程化需要。在深度学习技术之前，逻辑回归凭借易于并行化、模型简单、训练开销小等特点，占领工程领域的主流。因为即使工程团队发现了复杂模型会提升效果，但一般如果没有把握击败逻辑回归的话仍然不敢尝试或者升级。</li></ol><h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ol><li>表达能力不强，无法进行特征交叉、特征筛选等一系列高级操作，因此可能造成信息的损失。</li><li>准确率并不是很高。因为这毕竟是一个线性模型加了个 sigmoid，形式非常的简单，很难去拟合数据的真实分布。</li><li>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，如果想处理非线性，首先对连续特征的处理需要先进行离散化，而人工分桶的方式会引入多种问题。</li><li>LR 需要进行人工特征组合，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。</li></ol><h2 id="自动特征交叉"><a href="#自动特征交叉" class="headerlink" title="自动特征交叉"></a>自动特征交叉</h2><h3 id="基本思想-2"><a href="#基本思想-2" class="headerlink" title="基本思想"></a>基本思想</h3><ul><li><p>该算法属于对逻辑回归(LR)算法应用在推荐系统上的一个改进，在LR模型的基础上加上了特征交叉项，该思想不仅在传统的推荐算法中继续使用过，在深度学习推荐算法中也对其进行了改进与应用。</p></li><li><p>针对问题：逻辑回归只对单一特征做简单加权，不具备特征交叉生成组合特征的能力，因此表达能力受到了限制。</p></li><li><p>如：“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好……</p></li></ul><h3 id="POLY2——特征交叉的开始"><a href="#POLY2——特征交叉的开始" class="headerlink" title="POLY2——特征交叉的开始"></a>POLY2——特征交叉的开始</h3><ul><li>POLY2是二阶多项式模型，对所有特征进行两两交叉、暴力组合。</li><li>针对问题：在逻辑回归里面，如果想得到组合特征，往往需要人工在特征工程的时候手动的组合特征，然后再进行筛选，低效且玄学。</li><li>数学形式：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927165630280.png" alt="image-20210927165630280"></li><li>优点：<ol><li>保留了逻辑回归的优点：充分利用用户特征、物品特征、上下文特征。</li><li>一定程度上解决了特征组合的问题。</li></ol></li><li>存在问题：<ol><li>推荐系统中的数据稀疏性是实际问题中不可避免的挑战(类别型数据经过独热），经过特征较差之后特征向量更加稀疏，使得大部分特征交叉的权重缺乏有效训练，无法收敛。</li><li>训练复杂度由 O(n) 上升到 O(n^2^)</li></ol></li></ul><h3 id="FM——隐向量特征交叉"><a href="#FM——隐向量特征交叉" class="headerlink" title="FM——隐向量特征交叉"></a>FM——隐向量特征交叉</h3><ul><li>因子分解机(Factorization Machine, FM)</li><li>针对问题：1. 在面对稀疏数据向量时，PLOY2特征交叉项无法收敛。2. PLOY2 计算复杂度过高。</li><li>数学模型：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927172557690.png" alt="image-20210927172557690" style="zoom:67%;"></li><li>优点：<ol><li>极大降低了训练开销 O(n^2^) -&gt; O(kn)</li><li>引入隐向量，更好地解决数据稀疏性的问题</li><li>FM 模型是利用两个特征的 Embedding 做内积得到二阶特征交叉的权重。可以将训练好的 FM 特征离线训练保存，以便其他拓展。</li></ol></li></ul><h3 id="FFM——引入特征域"><a href="#FFM——引入特征域" class="headerlink" title="FFM——引入特征域"></a>FFM——引入特征域</h3><ul><li><p>域感知因子分解机(Field-aware Factorization Machine, FFM)</p></li><li><p>先对特征根据性质的不同进行了一个分类，不同的分类就是不同的域，域内特征一般都是同一个 categorical 特征经过 One-Hot 编码生成的数值特征。</p><p>对于连续特征，一个特征就对应一个域；或者可以对连续特征离散化，一个分箱成为一个特征，总的分箱是一个域。</p><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927185945062.png" alt="image-20210927185945062"></p></li><li><p>改进方向：针对不同的交叉域要学习不同的隐向量特征</p></li><li><p>优点：引入了更多有价值的信息，表达能力更强。</p></li><li><p>缺点：</p><ol><li>参数量：FM: k×n -&gt; FFM: f×k×n</li><li>时间复杂度：FM: O(kn) -&gt; FFM: O(kn^2^)</li><li>局限在二阶特征交叉</li></ol></li><li><p>工业改进：</p><ol><li>模型上：双线性FFM(新浪微博张俊林团队)，减少参数量的一种优化思路，共享W矩阵。</li><li>业务上：对特征域再进行抽象，减少域的数量。比如说，对 100 个域进行再分类，分为上下文特征、用户特征、item特征、交互特征和匹配特征五大类，实际应用时只考虑这 5 个域即可。</li></ol></li></ul><h2 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT+LR"></a>GBDT+LR</h2><h3 id="基本思想-3"><a href="#基本思想-3" class="headerlink" title="基本思想"></a>基本思想</h3><ul><li>该模型仍然是对LR模型的改进，使用树模型做特征交叉，相比于 FM 的二阶特征交叉，树模型可以对特征进行深度的特征交叉，充分利用了特征之间的相关性。</li><li>针对问题：<ol><li>LR 模型无法进行特征交叉、特征筛选等一些列操作。</li><li>FM、FFM只能做二姐特征交叉，如果继续提高特征交叉的维度，会不可避免地出现组合爆炸和计算复杂度过高的问题。</li></ol></li></ul><h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><ul><li><p>GBDT 全称梯度提升决策树，在传统机器学习算法里面是对真实分布拟合的最好的几种算法之一。</p><ul><li>GBDT 每轮的训练是在上一轮的训练的残差基础之上进行训练的。</li><li>GBDT 无论用于分类还是回归一直都是使用的 CART 回归树。</li></ul><p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927191924299.png" alt="image-20210927191924299"></p></li><li><p>优点：可以自动进行多维度特征的发掘与有效组合。</p></li><li><p>局限性：</p><ol><li>对于海量的 id 类特征，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储</li><li>另外海量特征在也会存在性能瓶颈，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内存。</li></ol><p>所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有信息损失，对于头部资源不能有效的表达。</p></li></ul><h3 id="GBDT-LR-1"><a href="#GBDT-LR-1" class="headerlink" title="GBDT+LR"></a>GBDT+LR</h3><ul><li>GBDT和LR的优缺点可以进行互补</li><li><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927192931637.png" alt="image-20210927192931637"></li><li>训练：GBDT 建树的过程相当于自动进行的特征组合和离散化，从根结点到叶子节点的这条路径就可以看成是不同特征进行的特征组合。用叶子节点可以唯一地表示这条路径，并作为一个离散特征传入 LR 进行二次训练。（GBDT 和 LR 是分两步进行训练的，所以不存在 LR 梯度回传至 GBDT）</li><li>预测：会先走 GBDT 的每棵树，得到某个叶子节点对应的一个离散特征(即一组特征组合)，然后把该特征以 one-hot 形式传入 LR 进行线性加权预测。</li><li>优点：<ol><li>大大推进了特征工程模型化这一重要趋势。意味着特征工程可以完全交由一个模型独立完成，模型的输入可以是原始的特征向量，不必在特征工程上投入过多的人工筛选和模型设计的精力。</li><li>在一定程度上解决了高阶特征交叉的问题。</li></ol></li><li>缺点：容易过拟合。</li></ul><h2 id="LS-PLM-MLR"><a href="#LS-PLM-MLR" class="headerlink" title="LS-PLM(MLR)"></a>LS-PLM(MLR)</h2><ul><li><p>针对问题：LR 表达能力差</p></li><li><p>解决思路：</p><ul><li>通过聚类，构造带权 LR。</li><li>在逻辑回归的基础上，采用分而治之的思路：先对样本进行聚类，在对样本分片中应用逻辑回归进行 CTR 预估。</li><li>目的：让 CTR 预估模型对不同的用户群体、不同的应用场景更有针对性。</li></ul></li><li><p>预测函数：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927194722571.png" alt="image-20210927194722571"></p></li><li><p>优点：</p><ol><li>端到端的非线性学习能力</li><li>稀疏性强，部署更加轻量级</li></ol></li><li><p>提升模型性能的 trick：</p><ol><li><p>结构化先验增量训练（用户特征进行训练，广告特征进行分类）</p><img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part2/image-20210927195859754.png" alt="image-20210927195859754" style="zoom: 80%;"></li><li><p>线性偏置</p></li><li><p>模型级联</p></li></ol></li></ul>]]></content>
    
    
    <summary type="html">传统推荐模型</summary>
    
    
    
    <category term="推荐系统" scheme="https://nephrencake.gitee.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>推荐系统-Part1</title>
    <link href="https://nephrencake.gitee.io/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part1/"/>
    <id>https://nephrencake.gitee.io/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part1/</id>
    <published>2021-09-26T07:42:53.000Z</published>
    <updated>2021-10-26T15:12:43.209Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推荐系统-Part1——概述"><a href="#推荐系统-Part1——概述" class="headerlink" title="推荐系统-Part1——概述"></a>推荐系统-Part1——概述</h1><p>[TOC]</p><h2 id="推荐系统概述"><a href="#推荐系统概述" class="headerlink" title="推荐系统概述"></a>推荐系统概述</h2><h3 id="传统推荐模型"><a href="#传统推荐模型" class="headerlink" title="传统推荐模型"></a>传统推荐模型</h3><p>传统推荐模型化关系图：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part1/Javaimage-20200923143443499.png" alt="img"></p><ol><li>协同过滤</li><li>矩阵分解算法——协同过滤的进化</li><li>逻辑回归——融合多种特征的推荐模型</li><li>PLOY2、FM、FFM——自动特征交叉</li><li>GBDT+LR——特征工程模型化的开端</li><li>LS-PLM(MLR)——混合逻辑回归</li></ol><h3 id="深度学习推荐模型"><a href="#深度学习推荐模型" class="headerlink" title="深度学习推荐模型"></a>深度学习推荐模型</h3><p>模型发展时间线：<img src="/2021/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Part1/Javaimage-20200923143559968.png" alt="img"></p><ol><li>AutoRec、DeepCrossing——改变神经网络复杂程度</li><li>NeuralCF、PNN——改变特征交叉方式</li><li>Wide&amp;Deep、Deep&amp;Cross——记忆与泛化并存</li><li>FNN、DeepFM、NFM——FM 在深度学习中的身影重现</li><li>AFM、DIN——当推荐系统遇上注意力机制</li><li>DIEN——序列模型与推荐系统的火花碰撞</li></ol><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>随着信息技术和互联网的发展，我们已经步入了一个信息过载的时代，无论是信息消费者还是信息生产者都遇到了很大的挑战：</p><ul><li>信息消费者：希望从大量的信息中找到自己感兴趣的信息。</li><li>信息生产者：希望让自己生产的信息脱颖而出，受到广大用户的关注。</li></ul><p>推荐系统，是缓解信息过载的重要技术。通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐给能够满足他们兴趣和需求的信息。一方面帮助用户发现对自己有价值的信息，一方面让信息能够展现在对它感兴趣的用户前面。</p><h2 id="论文要求"><a href="#论文要求" class="headerlink" title="论文要求"></a>论文要求</h2><p>随着互联网、物联网、区块链、大数据、人工智能、5G技术的不断发展，特别是移动网络的发展，信息量成爆炸式增长。作为缓解信息过载的重要技术，推荐系统目前已经在健康、智能交通、教育、电子商务等领域有相当成功的应用。然而随着用户需求的持续变化，传统的推荐算法面临一些新的问题，如冷启动、推荐新颖度不够、数据稀疏等。针对这些问题，学者们将新技术应用于推荐系统中，产生了一批新的推荐方法。探讨如何将这些新的理论、方法及技术应用于推荐系统，对更好地提供个性化推荐服务具有积极的指导意义和重要的实践价值，也将促进智能时代的信息技术的持续发展。 </p><p>《武汉大学学报（理学版）》是国内知名期刊，也是北大核心期刊和CSCD-C收录期刊。《武汉大学学报（理学版）》与中国软件大会（ChinaSoft2021）合作征稿，计划于2021年出版“面向人工智能的推荐系统前沿研究”专题。本专题将收录国内外有关推荐系统研究领域创新性和突破性的高水平研究成果，深入探讨推荐系统相关基础理论、关键技术以及支撑平台等方面创新成果，并探讨相关成果在产业界的应用前景。读者群体包括软件工程、人工智能、服务计算等相关领域的专家学者、研究人员和专业工程师等。欢迎从事该领域研究的学者积极投稿。 </p><p>一、征文范围 </p><p>包括但不限于以下主题： </p><p>（1）  推荐系统的理论性研究 </p><p>（2）  面向时间序列的推荐系统研究 </p><p>（3）  基于因果推断的推荐系统研究 </p><p>（4）  面向工业领域的推荐系统及决策研究 </p><p>（4）  基于推荐系统的软件工程 </p><p>二、投稿要求 </p><p>（1）  论文应属于作者的科研成果，未在国内外公开发行的刊物或会议上发表，不存在一稿多投问题。 </p><p>（2）  论文一律用word格式排版，格式体例参考近期出版的《武汉大学学报（理学版）》的要求。<a href="http://whdy.cbpt.cnki.net/WKD/WebPublication/index.aspx?mid=whdy">http://whdy.cbpt.cnki.net/WKD/WebPublication/index.aspx?mid=whdy</a> </p><p>（3）  投稿和评审方式：本专题投稿论文需经过两轮投稿和评审。 </p><p>三、论文截止时间：10月10号 </p><p>四、投稿方式： </p><p>编 辑 部： <a href="mailto:whdz@whu.edu.cn">whdz@whu.edu.cn</a>, 027-68752220          </p><p>通信地址：武汉大学本科生院楼北楼《武汉大学学报（理学版）》编辑部，430072 </p><p>特邀编委：黄 勃   副教授   上海工程技术大学  <a href="mailto:huangbosues@sues.edu.cn">huangbosues@sues.edu.cn</a>   </p><p>​               武 星   副教授   上海大学  <a href="mailto:xingwu@shu.edu.cn">xingwu@shu.edu.cn</a></p>]]></content>
    
    
    <summary type="html">概述</summary>
    
    
    
    <category term="推荐系统" scheme="https://nephrencake.gitee.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch-完结目录</title>
    <link href="https://nephrencake.gitee.io/2021/09/PyTorch-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
    <id>https://nephrencake.gitee.io/2021/09/PyTorch-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</id>
    <published>2021-09-17T01:26:26.000Z</published>
    <updated>2021-10-26T15:14:20.240Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch-完结目录"><a href="#PyTorch-完结目录" class="headerlink" title="PyTorch-完结目录"></a>PyTorch-完结目录</h1>]]></content>
    
    
    <summary type="html">完结目录</summary>
    
    
    
    <category term="PyTorch" scheme="https://nephrencake.gitee.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch-Part3</title>
    <link href="https://nephrencake.gitee.io/2021/09/PyTorch-Part3/"/>
    <id>https://nephrencake.gitee.io/2021/09/PyTorch-Part3/</id>
    <published>2021-09-12T09:36:23.000Z</published>
    <updated>2021-10-26T15:14:00.057Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch-Part3——TensorFlow转PyTorch"><a href="#PyTorch-Part3——TensorFlow转PyTorch" class="headerlink" title="PyTorch-Part3——TensorFlow转PyTorch"></a>PyTorch-Part3——TensorFlow转PyTorch</h1><p>[TOC]</p><h2 id="函数对应"><a href="#函数对应" class="headerlink" title="函数对应"></a>函数对应</h2><p>TensorFlow - tf.reshape 函数 &amp; Pytorch - torch.reshape 函数</p><p>TensorFlow - tf.concat 函数 &amp; Pytorch - torch.cat 函数</p><p>TensorFlow - tf.abs 函数 &amp;</p><ul><li>针对张量的每一位的元素分别计算其绝对值，并输出原格式的新张量。</li></ul><p>TensorFlow - tf.expand_dims 函数 &amp; torch.unsqueeze</p><ul><li><p>TensorFlow - tf.less 函数 &amp;<br>torch.le same as torch.ge — &gt;=<br>torch.lt same as torch.gt — &gt;<br>与torch.min相同的是，其均是用来比较两个张量每一位元素的最小值，不同的是tf.less输出的是是否为最小值的bool值，torch.min是输出两元素中的最小值并重新组合成张量。<br>返回值： 一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;= other )。 返回类型： Tensor</p></li><li><p>torch.le(input, other, out=None) -&gt; Tensor:</p></li><li><p>逐元素比较input和other，即是否input &lt;= other，第二个参数可以为一个数或与第一个参数相同形状和类型的张量。</p></li><li><p>torch.lt(input, other, out=None) -&gt; Tensor:</p></li><li><p>逐元素比较input和other，即是否input &lt; other</p></li></ul><p>TensorFlow - tf.equal 函数 &amp; torch.equal</p><ul><li>若两个张量有相同的形状和元素值，则返回True， 否则False。</li></ul><p>TensorFlow - tf.greater_equal 函数 &amp; torch.ge torch.gt</p><ul><li>逐元素比较input和other，是否input &gt; other。若两个张量有相同的形状和元素值，则返回True，否则False。</li></ul><p>TensorFlow - tf.norm 函数 &amp; torch.norm</p><p>TensorFlow - tf.argmin 函数 &amp; torch.argmin</p><p>TensorFlow - tf.gather_nd 函数 &amp;</p><ul><li><p>目前torch中自带的同类型函数只有torch.gather，用于处理指定一维度，无gather_nd的用法，但开源工具torchsample中则加入了类似的函数th_gather_nd，模仿tensorflow中的使用，详见：<br>torchsample推荐安装，有点类似于tf中的keras，提供高级api，减少开发时间，实用。</p><p>Pytorch的高级训练，数据增强和实用程序(torchsample/Keras)</p></li></ul><p>TensorFlow - tf.gather 函数 &amp; torch.gather</p><ul><li>根据索引从参数轴上收集切片.</li></ul><p>TensorFlow - tf.shape 函数 &amp; torch.size</p><p>TensorFlow - tf.range 函数 &amp; torch.arange / torch.range</p><p>TensorFlow - tf.stack 函数 &amp; torch.stack</p><p>TensorFlow - tf.reduce_mean 函数 &amp; torch.mean</p><p>TensorFlow - tf.cast 函数 &amp;</p><ul><li>torch中没有相似的类型转换函数，直接使用torch.FloatTensor()或data.float()类似的进行转换就可以了。</li></ul><p>TensorFlow - tf.count_nonzero 函数 &amp; torch.nonzero</p><ul><li>计算并输出各个维度上非０元素的个数。</li></ul><p>TensorFlow - tf.tile 函数 &amp; Tensor.repeat()</p><p>Tensorflow tf.where &amp; torch.le / torch.lt</p><p>Tensorflow tf.nn.in_top_k</p><ul><li><p>tensorflow    pytroch<br>tf.reshape(input, shape)    input.view()<br>tf.expand_dims(input, dim)    input.unsqueeze(dim) / input.view()<br>tf.squeeze(input, dim)    torch.squeeze(dim)/ input.view()<br>tf.gather(input1, input2)    input1[input2]<br>tf.tile(input, shape)    input.repeat(shape)<br>tf.boolean_mask(input, mask)    input[mask] #注意，mask是bool值，不是0,1的数值<br>tf.concat(input1, input2)    torch.cat(input1, input2)<br>tf.matmul()    torch.matmul()<br>tf.minium(input, min)    torch.clamp(input, max=min)<br>tf.equal(input1, input2)    torch.eq(input1, input2)/ input1 == input2<br>tf.logical_and(input1, input2)    input1 &amp; input2<br>tf.logical_not(input)    ~ input<br>tf.reduce_logsumexp(input, [dim])    torch.logsumexp(input, dim=dim)<br>tf.reduce_any(input, dim)    input.any(dim)<br>tf.reduce_mean(input)    torch.mean(input)<br>tf.reduce_sum(input)    input.sum()<br>tf.transpose(input)    input.t()<br>tf.softmax_cross_entroy_with_logits(logits, labels)    torch.nn.CrossEntropyLoss(logits, labels)</p></li><li><p>获取词向量：</p></li><li><p>tensorflow中: input_emb = tf.gather(tf.get_variable(“input_emb”, [num, embedding_size]), input)<br>pytorch中：1.embedding = nn.Embedding(num, embedding_size)，2.input_emb = embedding (input)<br>只是初始化embedding矩阵</p></li><li><p>tensorflow中: input_emb = tf.get_variable(“input_emb”, [num, embedding_size])<br>pytorch中：1.embedding = nn.Embedding(num, embedding_size)， 2.input_emb = embedding .weight</p></li><li><p>基本重构完成代码之后，检查代码以下方面：</p><ol><li>👌 初始化网络权重是否正确</li><li>👌 如果有预训练模型，检查是否加载正确</li><li>👌 检查优化器，整个任务是相同的学习率，还是说是不同层，设置了不同的学习率</li><li>👌 如果数据预处理也需要改动，最好也检查一下预处理是否正确</li></ol></li></ul><p><a href="https://blog.csdn.net/qq_41895190/article/details/89954394">https://blog.csdn.net/qq_41895190/article/details/89954394</a></p><blockquote><p>两个 tf 流程</p><ol><li><a href="https://www.cnblogs.com/tengge/p/6376073.html">https://www.cnblogs.com/tengge/p/6376073.html</a></li><li><a href="https://blog.csdn.net/lcczzu/article/details/91449731">https://blog.csdn.net/lcczzu/article/details/91449731</a></li></ol></blockquote><h2 id="常见的loss问题"><a href="#常见的loss问题" class="headerlink" title="常见的loss问题"></a>常见的loss问题</h2><h3 id="train-loss与test-loss结果分析"><a href="#train-loss与test-loss结果分析" class="headerlink" title="train loss与test loss结果分析"></a>train loss与test loss结果分析</h3><p>train loss 不断下降，test loss不断下降，说明网络仍在学习;</p><p>train loss 不断下降，test loss趋于不变，说明网络过拟合;</p><p>train loss 趋于不变，test loss不断下降，说明数据集100%有问题;</p><p>train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;</p><p>train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。</p><p>二，<br>这个比较长，比较完整 Loss和神经网络训练</p><p><a href="https://blog.csdn.net/u011534057/article/details/51452564">https://blog.csdn.net/u011534057/article/details/51452564</a></p><p>有1.梯度检验2.训练前检查，3.训练中监控4.首层可视化5.模型融合和优化等等等</p><p>三，<a href="https://www.zhihu.com/question/38937343">https://www.zhihu.com/question/38937343</a></p><p>四，<a href="https://blog.csdn.net/u010911921/article/details/71079367">https://blog.csdn.net/u010911921/article/details/71079367</a></p><p>原文地址：<a href="http://blog.csdn.net/u010911921/article/details/71079367">http://blog.csdn.net/u010911921/article/details/71079367</a><br>这段在使用caffe的时候遇到了两个问题都是在训练的过程中loss基本保持常数值，特此记录一下。</p><p>1.loss等于87.33不变<br>loss等于87.33这个问题是在对Inception-V3网络不管是fine-tuning还是train的时候遇到的，无论网络迭代多少次，网络的loss一直保持恒定。</p><p>查阅相关资料以后发现是由于loss的最大值由FLT_MIN计算得到，FLT_MIN是其对应的自然对数正好是-87.3356，这也就对应上了loss保持87.3356了。</p><p>这说明softmax在计算的过程中得到了概率值出现了零，由于softmax是用指数函数计算的，指数函数的值都是大于0的，所以应该是计算过程中出现了float溢出的异常，也就是出现了inf，nan等异常值导致softmax输出为0.</p><p>当softmax之前的feature值过大时，由于softmax先求指数，会超出float的数据范围，成为inf。inf与其他任何数值的和都是inf，softmax在做除法时任何正常范围的数值除以inf都会变成0.然后求loss就出现了87.3356的情况。</p><p>解决办法</p><p>由于softmax输入的feature由两部分计算得到：一部分是输入数据，另一部分是各层的权值等组成</p><p>减小初始化权重，以使得softmax的输入feature处于一个比较小的范围</p><p>降低学习率，这样可以减小权重的波动范围</p><p>如果有BN(batch normalization)层，finetune时最好不要冻结BN的参数，否则数据分布不一致时很容易使输出值变得很大(注意将batch_norm_param中的use_global_stats设置为false )。</p><p>观察数据中是否有异常样本或异常label导致数据读取异常</p><p>本文遇到的情况采用降低学习率的方法，learning rate设置为0.01或者原来loss的或者。</p><p>2.loss保持0.69左右<br>采用VGG-16在做一个二分类问题,所以计算loss时等价与下面的公式：</p><p>当p=0.5时，loss正好为0.693147，也就是训练过程中，无论如何调节网络都不收敛。最初的网络配置文件卷积层的参数如下所示：</p><p>从VGG-16训练好的模型进行fine-tuning也不发生改变，当在网络中加入初始化参数和decay_mult以后再次训练网络开始收敛。</p><p>但是具体是什么原因造成的，暂时还没有找到，难道是初始化参数的问题还是？</p><p>参考资料<br><a href="http://blog.csdn.net/jkfdqjjy/article/details/52268565?locationNum=14">http://blog.csdn.net/jkfdqjjy/article/details/52268565?locationNum=14</a></p><p><a href="https://groups.google.com/forum/#!topic/caffe-users/KEgrRlwXz9c">https://groups.google.com/forum/#!topic/caffe-users/KEgrRlwXz9c</a></p><p><a href="https://www.zhihu.com/question/68603783">https://www.zhihu.com/question/68603783</a></p><p>loss一直不下降的原因有很多，可以从头到尾滤一遍： 1）数据的输入是否正常，data和label是否一致。 2）网络架构的选择，一般是越深越好，也分数据集。 并且用不用在大数据集上pre-train的参数也很重要的 3）loss 对不对。</p><p>具体到语音，很多是把audio转成频谱图送给CNN训练。</p><p>NIPS16 有个soundNet(torch的code)，语音分类的performance很高，我觉得可以用来初始化你的model 参数, 可以参考下。</p><p>还有我见的3D-CNN 多用于视频，做audio 用3D 的工作比较少，倒是见过是用1维卷积做audio的</p><blockquote><p><a href="https://blog.csdn.net/u011534057/article/details/51452564/">https://blog.csdn.net/u011534057/article/details/51452564/</a></p></blockquote>]]></content>
    
    
    <summary type="html">TensorFlow转PyTorch</summary>
    
    
    
    <category term="PyTorch" scheme="https://nephrencake.gitee.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch-Part2</title>
    <link href="https://nephrencake.gitee.io/2021/09/PyTorch-Part2/"/>
    <id>https://nephrencake.gitee.io/2021/09/PyTorch-Part2/</id>
    <published>2021-09-12T09:36:17.000Z</published>
    <updated>2021-10-26T15:14:07.638Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch-Part2——Pytorch模型训练"><a href="#PyTorch-Part2——Pytorch模型训练" class="headerlink" title="PyTorch-Part2——Pytorch模型训练"></a>PyTorch-Part2——Pytorch模型训练</h1><p>[TOC]</p><h2 id="数据-Data"><a href="#数据-Data" class="headerlink" title="数据(Data)"></a>数据(Data)</h2><h3 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h3><ul><li><p>严谨的人工智能模型应用应当划分训练集(train set)、测试集(test set)和验证集(valid/dev set)三部分。</p></li><li><p>训练集就用来训练模型，测试集是用来估计模型在实际应用中的泛化能力，而验证集是用于模型选择和调参的。</p><p>在研究过程中，验证集和测试集作用都是一样的，只是对模型的泛化能力进行一个观测。</p><p>而当在工程应用中，由于尽可能地用尽所有数据集并迭代，要防止模型过拟合到测试集上，要有验证集对其进行约束。</p></li><li><p>可以使用 <code>train : test : valid = 8 : 1 : 1</code> 这个比例。</p></li></ul><blockquote><p><a href="https://blog.csdn.net/u011995719/article/details/77451213">https://blog.csdn.net/u011995719/article/details/77451213</a></p></blockquote><h3 id="PyTorch读取数据集"><a href="#PyTorch读取数据集" class="headerlink" title="PyTorch读取数据集"></a>PyTorch读取数据集</h3><ul><li><p>自定义数据集要继承 <code>Dataset</code> 类，并重写 <code>__getitem__()</code> 和 <code>__len__()</code> 方法</p><ul><li><code>__init__()</code>：生成数据的路径列表，尤其对于非结构化数据集，不能直接将所有数据读入内存。</li><li><code>__getitem__()</code>：由 DataLoader 进行调用，返回相应索引的数据，同时进行一系列的数据增强扩充数据集的多样性。</li><li><code>__len__()</code>：提供数据集长度的查询。</li></ul></li><li><p>读取数据流程：</p><ol><li><p>在 MyDataset 中初始化图片路径和标签、数据增强方式</p></li><li><p>在 DataLoader 中初始化 num_workers、shuffle、batch_size、sampler、batch_sampler、collate_fn。即多进程读取数据、采样与拼接方法。</p><p>在 sampler 中会调用到 MyDataset 的 <code>__len__()</code> 方法。</p></li><li><p>在 iteration 进行时，DataLoder 才读取一个 batch 的图片数据。由 batch_sampler 与 collate_fn 确定一个 batch 的 indices 并进行拼接。</p><p>在 collate_fn 中会调用 MyDataset 类中的 <code>__getitem__()</code> 方法。</p></li></ol></li><li><p>在训练时，一般要对图片进行以下操作：</p><ol><li>随机裁剪</li><li>ToTensor：包含①[h, w, c] -&gt; [c, h, w]；② /255：归一化至 0~1 区间。</li><li>数据标准化(减均值，除以标准差)</li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, txt_path, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        fh = <span class="built_in">open</span>(txt_path, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        imgs = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fh:</span><br><span class="line">            line = line.rstrip()</span><br><span class="line">            words = line.split()</span><br><span class="line">            imgs.append((words[<span class="number">0</span>], <span class="built_in">int</span>(words[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">        self.imgs = imgs  <span class="comment"># 最主要就是要生成这个list， 然后DataLoader中给index，通过getitem读取图片数据</span></span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        fn, label = self.imgs[index]</span><br><span class="line">        img = Image.<span class="built_in">open</span>(fn).convert(<span class="string">&#x27;RGB&#x27;</span>)  <span class="comment"># 像素值 0~255，在transfrom.totensor会除以255，使像素值变成 0~1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)  <span class="comment"># 在这里做transform，转为tensor等等</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.imgs)</span><br></pre></td></tr></table></figure><h3 id="transforms-的二十二个方法-干货"><a href="#transforms-的二十二个方法-干货" class="headerlink" title="transforms 的二十二个方法(干货)"></a>transforms 的二十二个方法(干货)</h3><ol><li>裁剪——Crop<ul><li>中心裁剪：<code>transforms.CenterCrop(size)</code><ul><li><strong>size-</strong> (sequence or int)，若为 sequence，则为(h,w)，若为 int，则(size,size)</li></ul></li><li>随机裁剪：<code>transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=&#39;constant&#39;)</code><ul><li><strong>size-</strong> (sequence or int)，若为 sequence，则为(h,w)，若为 int，则(size,size)</li><li><strong>padding-</strong>(sequence or int, optional)，当为 int时， 例如padding=a时，图片上下左右均填充a个像素；当为tupple时，(a, b)，则左右填充a个像素，上下填充b个像素；(a, b, c, d)， 则左填充a个像素，下填充b个像素，右填充c个像素, 上填充d个像素。</li><li><strong>fill-</strong> (int or tuple)，填充的值（仅当padding_mode=’constant’）。int 时，各通道均填充该值，当长度为 3 的 tuple 时，表示 RGB 通道需要填充的值。</li><li><strong>padding_mode-</strong> 填充模式：1. constant，常量。2. edge，按照图片边缘的像素值来填充。3. reflect，镜像填充，最后一个像素不镜像。 4. symmetric，最后一个像素镜像。</li></ul></li><li>随机长宽比裁剪：<code>transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.33), interpolation=2) </code><ul><li><strong>size-</strong> 所需裁剪图片尺寸。</li><li><strong>scale-</strong> 随机裁剪面积比例，默认scale=(0.08, 1.0)，表示随机 crop 出来的图片会在的 0.08倍至 1 倍之间。</li><li><strong>ratio-</strong> 随机长宽比设置，默认(3/4, 4/3)</li><li><strong>interpolation</strong>- 插值的方法，默认为双线性插值(PIL.Image.BILINEAR)</li></ul></li><li>上下左右中心裁剪：<code>transforms.FiveCrop(size)</code><ul><li><strong>size-</strong> (sequence or int)，若为 sequence,则为(h,w)，若为 int，则(size,size)</li></ul></li><li>上下左右中心裁剪后翻转：<code>transforms.TenCrop(size, vertical_flip=False)</code><ul><li><strong>size-</strong> (sequence or int)，若为 sequence,则为(h,w)，若为 int，则(size,size)</li><li><strong>vertical_flip</strong>- (bool)，是否垂直翻转，默认为 flase，即默认为水平翻转</li></ul></li></ul></li><li>翻转和旋转——Flip and Rotation<ul><li>依概率 p 水平翻转：<code>transforms.RandomHorizontalFlip(p=0.5)</code><ul><li><strong>p-</strong> 概率，默认值为 0.5</li></ul></li><li>依概率 p 垂直翻转：<code>transforms.RandomVerticalFlip(p=0.5)</code><ul><li><strong>p-</strong> 概率，默认值为 0.5</li></ul></li><li>随机旋转：<code>transforms.RandomRotation(degrees, resample=False, expand=False, center=None)</code><ul><li><strong>degress</strong>- (sequence or float or int) ，若为 int，则在（-int，+int）之间随机旋转，若为 sequence，则在 s[0]~s[1] 度之间随机旋转</li><li><strong>resample</strong>- 重采样方法</li><li><strong>expand</strong>- 是否扩大图片，以保存图片原有信息</li><li><strong>center</strong>- 旋转中心，(0, 0)为左上角。默认为图片中心</li></ul></li></ul></li><li>图像变换<ul><li>resize：<code>transforms.Resize(size, interpolation=2)</code><ul><li><strong>size</strong>- If size is an int, if height &gt; width, then image will be rescaled to (size * height / width, size)，所以建议 size 设定为 h*w</li><li><strong>interpolation</strong>- 插值方法选择，默认为 PIL.Image.BILINEAR</li></ul></li><li>标准化：<code>transforms.Normalize(mean, std)</code><ul><li>对数据按通道进行标准化，即先减均值，再除以标准差，注意是 [h, w, c]</li></ul></li><li>转为 tensor，并归一化至[0-1]：<code>transforms.ToTensor</code><ul><li>归一化至[0-1]是直接除以 255，若自己的 ndarray 数据尺度有变化，则需要自行修改。</li></ul></li><li>填充图像边缘：<code>transforms.Pad(padding, fill=0, padding_mode=&#39;constant&#39;)</code><ul><li>同 随机裁剪 RandomCrop 的参数</li></ul></li><li>修改亮度、对比度和饱和度：<code>transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)</code><ul><li><strong>brightness</strong>- 亮度调整因子。当为a时，从 max((0, 1-a), 1+a) 中随机选择；当为(a, b)时，从[a, b]中随机选择。</li><li><strong>constant</strong>- 对比度参数，同brightness</li><li><strong>saturation</strong>- 饱和度参数，同brightness</li><li><strong>hue</strong>- 色相参数，当为a时，从[-a, a]中选择参数，注：0 &lt;= a &lt;= 0.5；当为(a, b)时，从[a, b]中选择参数，注：-0.5 &lt;= a &lt;= b &lt;= 0.5</li></ul></li><li>转灰度图：<code>transforms.Grayscale(num_output_channels=1)</code><ul><li><strong>num_output_channels</strong>- 输出通道数，只能设置1或者3</li></ul></li><li>线性变换：<code>transforms.LinearTransformation(transformation_matrix)</code><ul><li>对矩阵做线性变化，可用于白化处理。 whitening: zero-center the data, compute the data covariance matrix</li></ul></li><li>仿射变换：<code>transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)</code><ul><li><strong>degrees</strong>- 旋转角度设置</li><li><strong>translate</strong>- 平移区间设置如(a, b), a设置宽(width), b设置高(height) 。图像在宽维度的平移区间为 -img_width * a &lt; dx &lt; img_width * a</li><li><strong>scale</strong>- 缩放比例（以面积为单位）</li><li><strong>file_color</strong>- 填充颜色设置</li><li><strong>share</strong>- 错切角度设置，有水平错切和垂直错切，若为a，则仅在x轴错切，错切角度为(-a, a)之间；若为(a, b), 则a设置x轴角度，b设置y的角度；若为(a, b, c, d), 则a，b设置x轴度，c， d设置y角度</li><li><strong>resample</strong>- 重采样方式有NEAREST、BILINEAR、BICUBIC</li></ul></li><li>依概率 p 转为灰度图：<code>transforms.RandomGrayscale(p=0.1)</code><ul><li><strong>p</strong>- 概率值，图像被转换为灰度图的概率</li></ul></li><li>将数据转换为 PILImage：<code>transforms.ToPILImage(mode=None)</code><ul><li><strong>mode</strong>- 为 None 时，为 1 通道， mode=3 通道默认转换为 RGB，4 通道默认转换为 RGBA</li></ul></li><li>随机遮挡：<code>transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)</code><ul><li><strong>p</strong>- 概率值</li><li><strong>scale</strong>- 遮挡区域与输入图像的比例范围</li><li><strong>ratio</strong>- 遮挡区域长宽比</li><li><strong>value</strong>- 设置遮挡区域的像素值，(R, G, B) or (Gray), value为字符串(不一定非要random)时，随机填充像素值。</li><li><strong>inplace</strong>- 改变自身</li></ul></li><li>transforms.Lambda：Apply a user-defined lambda as a transform</li></ul></li><li>对 transforms 操作，使数据增强更灵活<ul><li>transforms.RandomChoice(transforms)，从给定的一系列 transforms 中选一个进行操作</li><li>transforms.RandomApply(transforms, p=0.5)，给一个 transform 加上概率，依概率进行操作</li><li>transforms.RandomOrder，将 transforms 中的操作随机打乱</li></ul></li><li>自定义 transforms <ul><li>仅接收一个参数，返回一个参数</li><li>注意上下游的输出与输入</li></ul></li></ol><h3 id="计算-Normalize-所用均值和方差"><a href="#计算-Normalize-所用均值和方差" class="headerlink" title="计算 Normalize 所用均值和方差"></a>计算 Normalize 所用均值和方差</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    随机挑选CNum张图片，进行按通道计算均值mean和标准差std</span></span><br><span class="line"><span class="string">    先将像素从0～255归一化至 0-1 再计算</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">train_txt_path = os.path.join(<span class="string">&quot;..&quot;</span>, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;Data/train.txt&quot;</span>)</span><br><span class="line">CNum = <span class="number">2000</span>  <span class="comment"># 挑选多少图片进行计算</span></span><br><span class="line"></span><br><span class="line">img_h, img_w = <span class="number">32</span>, <span class="number">32</span></span><br><span class="line">imgs = np.zeros([img_w, img_h, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">means, stdevs = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(train_txt_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines()</span><br><span class="line">    random.shuffle(lines)  <span class="comment"># shuffle , 随机挑选图片</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(CNum):</span><br><span class="line">        img_path = lines[i].rstrip().split()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        img = cv2.imread(img_path)</span><br><span class="line">        img = cv2.resize(img, (img_h, img_w))</span><br><span class="line"></span><br><span class="line">        img = img[:, :, :, np.newaxis]</span><br><span class="line">        imgs = np.concatenate((imgs, img), axis=<span class="number">3</span>)</span><br><span class="line">        print(i)</span><br><span class="line"></span><br><span class="line">imgs = imgs.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    pixels = imgs[:, :, i, :].ravel()  <span class="comment"># 拉成一行</span></span><br><span class="line">    means.append(np.mean(pixels))</span><br><span class="line">    stdevs.append(np.std(pixels))</span><br><span class="line"></span><br><span class="line">means.reverse()  <span class="comment"># BGR --&gt; RGB</span></span><br><span class="line">stdevs.reverse()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;normMean = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(means))</span><br><span class="line">print(<span class="string">&quot;normStd = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(stdevs))</span><br><span class="line">print(<span class="string">&#x27;transforms.Normalize(normMean = &#123;&#125;, normStd = &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(means, stdevs))</span><br></pre></td></tr></table></figure><h2 id="模型-Model"><a href="#模型-Model" class="headerlink" title="模型(Model)"></a>模型(Model)</h2><h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><ul><li>三个要点<ul><li>必须继承 <code>nn.Module</code> 这个类，要让 PyTorch 知道这个类是一个 Module。</li><li>在 <code>__init__(self)</code> 中设置好需要的隐藏层(如 conv、pooling、Linear、BatchNorm等)。</li><li>在 <code>forward(self, x)</code> 中用定义好的网络结构进行组装，定义前馈过程。</li></ul></li><li>可以使用类似 <code>_make_layer()</code> 类似的方法来辅助自定义网络层。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .BasicModule <span class="keyword">import</span> BasicModule</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现子module: Residual Block</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, inchannel, outchannel, stride=<span class="number">1</span>, shortcut=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResidualBlock, self).__init__()</span><br><span class="line">        self.left = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inchannel, outchannel, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outchannel),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(outchannel, outchannel, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outchannel))</span><br><span class="line">        self.right = shortcut</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.left(x)</span><br><span class="line">        residual = x <span class="keyword">if</span> self.right <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> self.right(x)</span><br><span class="line">        out += residual</span><br><span class="line">        <span class="keyword">return</span> F.relu(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet34</span>(<span class="params">BasicModule</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现主module：ResNet34</span></span><br><span class="line"><span class="string">    ResNet34包含多个layer，每个layer又包含多个Residual block</span></span><br><span class="line"><span class="string">    用子module来实现Residual block，用_make_layer函数来实现layer</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes=<span class="number">2</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet34, self).__init__()</span><br><span class="line">        self.model_name = <span class="string">&#x27;resnet34&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前几层: 图像转换</span></span><br><span class="line">        self.pre = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重复的layer，分别有3，4，6，3个residual block</span></span><br><span class="line">        self.layer1 = self._make_layer(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>)</span><br><span class="line">        self.layer2 = self._make_layer(<span class="number">128</span>, <span class="number">256</span>, <span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(<span class="number">256</span>, <span class="number">512</span>, <span class="number">6</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分类用的全连接</span></span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span>(<span class="params">self, inchannel, outchannel, block_num, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        构建layer,包含多个residual block</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        shortcut = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inchannel, outchannel, <span class="number">1</span>, stride, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outchannel))</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(ResidualBlock(outchannel, outchannel))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pre(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        x = F.avg_pool2d(x, <span class="number">7</span>)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)  <span class="comment"># 按batch_size展平</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br></pre></td></tr></table></figure><h3 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h3><ul><li>可以用 list 或者 OrderedDict 进行网络的堆叠。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">20</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)),</span><br><span class="line">    (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">    (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>, <span class="number">64</span>, <span class="number">5</span>)),</span><br><span class="line">    (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">]))</span><br></pre></td></tr></table></figure><h3 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h3><h4 id="权值初始化流程"><a href="#权值初始化流程" class="headerlink" title="权值初始化流程"></a>权值初始化流程</h4><ul><li>初始化流程<ul><li>第一步，先设定什么层用什么初始化方法，初始化方法在 torch.nn.init 中给出；</li><li>第二步，实例化一个模型之后，执行该函数，即可完成初始化。</li></ul></li><li><code>named_children()</code>和<code>named_modules()</code>的区别：<a href="https://blog.csdn.net/watermelon1123/article/details/98036360">https://blog.csdn.net/watermelon1123/article/details/98036360</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义权值初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">            torch.nn.init.xavier_normal_(m.weight.data)</span><br><span class="line">            <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">            m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">            m.bias.data.zero_()</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">            torch.nn.init.normal_(m.weight.data, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">            m.bias.data.zero_()</span><br></pre></td></tr></table></figure><h4 id="权值初始化的十种方法-干货"><a href="#权值初始化的十种方法-干货" class="headerlink" title="权值初始化的十种方法(干货)"></a>权值初始化的十种方法(干货)</h4><ol><li>Xavier 均匀分布：<code>torch.nn.init.xavier_uniform_(tensor, gain=1)</code><ul><li>xavier 初始化方法中服从均匀分布 U(−a,a) ，分布的参数 a = gain * sqrt(6/fan_in+fan_out)。也称为 Glorot initialization。</li><li><strong>gain</strong>- 增益的大小是依据激活函数类型来设定。</li><li>eg：nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(‘relu’))</li></ul></li><li>Xavier 正态分布：<code>torch.nn.init.xavier_normal_(tensor, gain=1)</code><ul><li>xavier 初始化方法中服从正态分布，mean=0,std = gain * sqrt(2/fan_in + fan_out)</li><li>kaiming 初始化方法，论文在《 Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification》，公式推导同样从“方差一致性”出法，kaiming是针对 xavier 初始化方法在 relu 这一类激活函数表现不佳而提出的改进，详细可以参看论文。</li></ul></li><li>kaiming 均匀分布：<code>torch.nn.init.kaiming_uniform_(tensor, a=0, mode=&#39;fan_in&#39;, nonlinearity=&#39;leaky_relu&#39;)</code><ul><li>此为均匀分布，U～（-bound, bound）, bound = sqrt(6/(1+a^2)*fan_in)。其中，a 为激活函数的负半轴的斜率，relu 是 0。</li><li><strong>mode</strong>- 可选为 fan_in 或 fan_out, fan_in 使正向传播时，方差一致; fan_out 使反向传播时，方差一致</li><li><strong>nonlinearity</strong>- 可选 relu 和 leaky_relu ，默认值为 。 leaky_relu</li><li>eg：nn.init.kaiming_uniform_(w, mode=’fan_in’, nonlinearity=’relu’)</li></ul></li><li>kaiming 正态分布：<code>torch.nn.init.kaiming_normal_(tensor, a=0, mode=&#39;fan_in&#39;, nonlinearity=&#39;leaky_relu&#39;)</code><ul><li>此为 0 均值的正态分布，N～ (0,std)，其中 std = sqrt(2/(1+a^2)*fan_in) </li><li><strong>a</strong>- 激活函数的负半轴的斜率，relu 是 0</li><li><strong>mode</strong>- 可选为 fan_in 或 fan_out。fan_in 使正向传播时，方差一致；fan_out 使反向传播时，方差一致</li><li><strong>nonlinearity</strong>- 可选 relu 和 leaky_relu ，默认值为 leaky_relu。</li><li>eg：nn.init.kaiming_normal_(w, mode=’fan_out’, nonlinearity=’relu’)</li></ul></li><li>均匀分布初始化：<code>torch.nn.init.uniform_(tensor, a=0, b=1)</code><ul><li>使值服从均匀分布 U(a,b)</li></ul></li><li>正态分布初始化：<code>torch.nn.init.normal_(tensor, mean=0, std=1)</code><ul><li>使值服从正态分布 N(mean, std)，默认值为 0，1</li></ul></li><li>常数初始化：<code>torch.nn.init.constant_(tensor, val)</code><ul><li>使值为常数 val nn.init.constant_(w, 0.3)</li></ul></li><li>单位矩阵初始化：<code>torch.nn.init.eye_(tensor)</code><ul><li>将二维 tensor 初始化为单位矩阵（the identity matrix）</li></ul></li><li>正交初始化：<code>torch.nn.init.orthogonal_(tensor, gain=1)</code><ul><li>使得 tensor 是正交的，论文:Exact solutions to the nonlinear dynamics of learning in deep linear neural networks” - Saxe, A. et al. (2013)</li></ul></li><li>稀疏初始化：<code>torch.nn.init.sparse_(tensor, sparsity, std=0.01)</code><ul><li>从正态分布 N～（0. std）中进行稀疏化，使每一个 column 有一部分为 0</li><li><strong>sparsity</strong>- 每一个 column 稀疏的比例，即为 0 的比例</li><li>eg：nn.init.sparse_(w, sparsity=0.1)</li></ul></li><li>计算增益：<code>torch.nn.init.calculate_gain(nonlinearity, param=None)</code></li></ol><blockquote><p>其实，在创建网络实例的过程中, 一旦调用 nn.Conv2d 的时候就会有对权值进行初始化。</p><p>在 PyTorch1.0 版本后，Conv2d 改用了 <code>kaiming_uniform_()</code> 进行初始化，可以在 <code>torch/nn/modules/conv.py</code> 中的 _ConvNd 类中的函数 <code>reset_parameters()</code> 中看到初始化方式。</p></blockquote><h3 id="模型加载与保存-Finetune"><a href="#模型加载与保存-Finetune" class="headerlink" title="模型加载与保存 Finetune"></a>模型加载与保存 Finetune</h3><ul><li>模型 Finetune 权值初始化：<ol><li>保存模型，拥有一个预训练模型</li><li>加载模型，把预训练模型中的权值取出来</li><li>初始化，将网络的权重用预训练模型初始化</li></ol></li><li>官方文档中介绍了两种保存模型的方法，一种是保存整个模型，另外一种是仅保存模型参数（官方推荐用这种方法）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 创建 net</span></span><br><span class="line">net = Net()</span><br><span class="line"><span class="comment"># 2. 获取已创建 net 的 state_dict</span></span><br><span class="line">net_state_dict = net.state_dict()</span><br><span class="line"><span class="comment"># 3. 加载模型，这里只是加载模型的参数</span></span><br><span class="line">pretrained_dict = torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>)</span><br><span class="line"><span class="comment"># 4. 将 pretrained_dict 里不属于 net_state_dict 的键剔除掉</span></span><br><span class="line">pretrained_dict_1 = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items() <span class="keyword">if</span> k <span class="keyword">in</span> net_state_dict&#125;</span><br><span class="line"><span class="comment"># 5. 用预训练模型的参数字典 对 新模型的参数字典 net_state_dict 进行更新</span></span><br><span class="line">net_state_dict.update(pretrained_dict_1)</span><br><span class="line"><span class="comment"># 6. 将更新了参数的字典 “放”回到网络中</span></span><br><span class="line">net.load_state_dict(net_state_dict)</span><br><span class="line"><span class="comment"># 7. 将网络的参数保存下来</span></span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;net_params.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="不同层设置不同的学习率"><a href="#不同层设置不同的学习率" class="headerlink" title="不同层设置不同的学习率"></a>不同层设置不同的学习率</h3><ul><li>在利用 pre-trained model 的参数做初始化之后，我们可能想让 fc 层更新相对快一些，而希望前面的权值更新小一些，这就可以通过为不同的层设置不同的学习率来达到此目的。</li><li>为不同层设置不同的学习率，主要通过优化器对多个参数组进行设置不同的参数。所以，只需要将原始的参数组，划分成两个，甚至更多的参数组，然后分别进行设置学习率。</li><li>这里将原始参数“切分”成 fc3 层参数和其余参数，为 fc3 层设置更大的学习率。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">lr_init: <span class="built_in">float</span> = <span class="number">0.001</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 将fc3层的参数从原始网络参数中剔除</span></span><br><span class="line">ignored_params = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, net.fc3.parameters()))  <span class="comment"># 返回的是 parameters 的内存地址</span></span><br><span class="line">base_params = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> ignored_params, net.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为fc3层设置需要的学习率</span></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: base_params&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: net.fc3.parameters(), <span class="string">&#x27;lr&#x27;</span>: lr_init*<span class="number">10</span>&#125;</span><br><span class="line">],  lr_init, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()                                                   <span class="comment"># 选择损失函数</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">50</span>, gamma=<span class="number">0.1</span>)     <span class="comment"># 设置学习率下降策略</span></span><br></pre></td></tr></table></figure><ul><li>挑选出特定的层的机制是利用内存地址作为过滤条件，将需要单独设定的部分参数从总的参数中剔除。<ul><li><code>net.fc3.parameters()</code> 是一个&lt;generator object parameters at 0x11b63bf00&gt;</li><li><code>ignored_params</code> 是包含 net.fc3 中 weight、bias 两者对应参数的内存地址列表</li><li><code>base_params</code> 是一个 list，每个元素是一个 Parameter 类，其中剔除了 net.fc3 的 weight、bias</li></ul></li></ul><blockquote><p><strong>冻结权重</strong>与<strong>优化器仅传入部分参数</strong>两者是等价的。</p><p>个人认为：1. 用requires_grad=False会提高内存优化，因为不需要保存梯度。2. 仅传入优化器可以提高运行速度，因为不用对部分参数进行计算</p><p><a href="https://blog.csdn.net/answer3664/article/details/108493753">https://blog.csdn.net/answer3664/article/details/108493753</a></p></blockquote><h2 id="损失函数-Loss-Function"><a href="#损失函数-Loss-Function" class="headerlink" title="损失函数(Loss Function)"></a>损失函数(Loss Function)</h2><h3 id="PyTorch-的十七个损失函数-干货"><a href="#PyTorch-的十七个损失函数-干货" class="headerlink" title="PyTorch 的十七个损失函数(干货)"></a>PyTorch 的十七个损失函数(干货)</h3><ul><li>训练网络的过程，是不断优化网络权值使得损失函数值最小化的过程。</li></ul><ol><li><p>L1loss：<code>torch.nn.L1Loss(size_average=None, reduce=None)</code></p><ul><li><p>计算 output 和 target 之差的绝对值</p><img src="/2021/09/PyTorch-Part2/image-20210913142253796.png" alt="image-20210913142253796" style="zoom:67%;"></li><li><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和</p></li></ul></li><li><p>MSELoss：<code>torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>计算 output 和 target 之差的平方</p><img src="/2021/09/PyTorch-Part2/image-20210913142318096.png" alt="image-20210913142318096" style="zoom:67%;"></li><li><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和</p></li></ul></li><li><p>CrossEntropyLoss：<code>torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>将输入经过 softmax 激活函数之后，再计算其与 target 的交叉熵损失。</p><p>该方法将 nn.LogSoftmax() 和 nn.NLLLoss() 进行了结合。严格意义上的交叉熵损失函数应该是 nn.NLLLoss()。</p><img src="/2021/09/PyTorch-Part2/image-20210913142855509.png" alt="image-20210913142855509" style="zoom:67%;"></li><li><p><strong>weight</strong>(Tensor)- 为每个类别的 loss 设置权值，常用于类别不均衡问题。weight 必须是 float类型的 tensor，其长度要于类别 C 一致，即每一个类别都要设置有 weight。</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True</p><p><strong>ignore_index</strong>(int)- 忽略某一类别，不计算其 loss，其 loss 会为 0，并且，在采用size_average 时，不会计算那一类的 loss，除的时候的分母也不会统计那一类的样本。</p></li></ul></li><li><p>NLLLoss：<code>torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>不带 log_softmax 层的 CrossEntropyLoss 。</p><img src="/2021/09/PyTorch-Part2/image-20210913144640040.png" alt="image-20210913144640040" style="zoom:67%;"></li><li><p><strong>weight</strong>(Tensor)- 为每个类别的 loss 设置权值，常用于类别不均衡问题。weight 必须是 float类型的 tensor，其长度要于类别 C 一致，即每一个类别都要设置有 weight。</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为除以权重之和的平均值；为 False 时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p><p><strong>ignore_index</strong>(int)- 忽略某一类别，不计算其 loss，其 loss 会为 0，并且，在采用 size_average 时，不会计算那一类的 loss，除的时候的分母也不会统计那一类的样本。</p></li></ul></li><li><p>PoissonNLLLoss：<code>torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>用于 target 服从泊松分布的分类任务。</p><img src="/2021/09/PyTorch-Part2/image-20210913144654680.png" alt="image-20210913144654680" style="zoom:67%;"></li><li><p>log_input(bool)- 为 True 时，计算公式为：loss(input,target)=exp(input) - target * input; 为 False 时，loss(input,target)=input - target * log(input+eps)</p><p><strong>full</strong>(bool)- 是否计算全部的 loss。例如，当采用斯特林公式近似阶乘项时，此为 target*log(target) - target+0.5∗log(2πtarget)</p><p><strong>eps</strong>(float)- 当 log_input = False 时，用来防止计算 log(0)，而增加的一个修正项。即</p><p><strong>loss</strong>(input,target)=input - target * log(input+eps)</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True</p></li></ul></li><li><p>KLDivLoss：<code>torch.nn.KLDivLoss(size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>计算 input 和 target 之间的 KL 散度( Kullback–Leibler divergence) 。又称为相对熵(Relative Entropy)，用于描述两个概率分布之间的差异。</p><p>要想获得真正的 KL 散度，需要如下操作：1. reduce = True ；size_average=False；2. 计算得到的 loss 要对 batch 进行求平均</p><img src="/2021/09/PyTorch-Part2/image-20210913144829614.png" alt="image-20210913144829614" style="zoom:67%;"></li><li><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值，平均值为</p><p><strong>element</strong>-wise 的，而不是针对样本的平均；为 False 时，返回是各样本各维度的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p></li></ul></li><li><p>BCELoss：<code>torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>二分类任务时的交叉熵计算函数。可以认为是 <strong>nn.CrossEntropyLoss</strong> 函数的特例。在 BCELoss 之前，input 一般为 sigmoid 激活层的输出。</p><img src="/2021/09/PyTorch-Part2/image-20210913145243878.png" alt="image-20210913145243878" style="zoom:67%;"></li><li><p><strong>weight</strong>(Tensor)- 为每个类别的 loss 设置权值，常用于类别不均衡问题。</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False 时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True</p></li></ul></li><li><p>BCEWithLogitsLoss：<code>torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;, pos_weight=None)</code></p><ul><li><p>将 Sigmoid 与 BCELoss 结合，类似于 CrossEntropyLoss(将 nn.LogSoftmax()和 nn.NLLLoss() 进行结合）。即 input 会经过 Sigmoid 激活函数，将 input 变成概率分布的形式。</p><img src="/2021/09/PyTorch-Part2/image-20210913145354209.png" alt="image-20210913145354209" style="zoom:67%;"></li><li><p><strong>weight</strong>(Tensor)- : 为 batch 中单个样本设置权值，If given, has to be a Tensor of size “nbatch”.</p><p><strong>pos_weight</strong>-: 正样本的权重, 当 p&gt;1，提高召回率，当 P&lt;1，提高精确度。可达到权衡召回率(Recall)和精确度(Precision)的作用。 Must be a vector with length equal to the number of classes.</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False 时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True</p></li></ul></li><li><p>MarginRankingLoss：<code>torch.nn.MarginRankingLoss(margin=0, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>计算两个向量之间的相似度，当两个向量之间的距离大于 margin，则 loss 为正，小于 margin，loss 为 0。</p><img src="/2021/09/PyTorch-Part2/image-20210913145522074.png" alt="image-20210913145522074" style="zoom:67%;"></li><li><p><strong>margin</strong>(float)- x1 和 x2 之间的差异。</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p></li></ul></li><li><p>HingeEmbeddingLoss：<code>torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>为折页损失的拓展，主要用于衡量两个输入是否相似。used for learning nonlinear embeddings or semi-supervised</p></li><li><p><strong>margin</strong>(float)- 默认值为 1，容忍的差距。</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p></li></ul></li><li><p>MultiLabelMarginLoss：<code>torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>用于一个样本属于多个类别时的分类任务。例如一个四分类任务，样本 x 属于第 0 类，第 1 类，不属于第 2 类，第 3 类。</p><img src="/2021/09/PyTorch-Part2/image-20210913145842583.png" alt="image-20210913145842583"></li><li><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p><p><strong>Input</strong>: (C) or (N,C) where N is the batch size and C is the number of classes.</p><p><strong>Target</strong>: (C) or (N,C), same shape as the input.</p></li></ul></li><li><p>SmoothL1Loss：<code>torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>计算平滑 L1 损失，属于 Huber Loss 中的一种(因为参数 δ 固定为 1 了)。</p><p>Huber Loss 常用于回归问题，其最大的特点是对离群点（outliers）、噪声不敏感，具有较强的鲁棒性。</p><p>当误差绝对值小于 δ，采用 L2 损失；若大于 δ，采用 L1 损失。</p><img src="/2021/09/PyTorch-Part2/image-20210913150004347.png" alt="image-20210913150004347" style="zoom:67%;"></li><li><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p></li></ul></li><li><p>SoftMarginLoss：<code>torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>Creates a criterion that optimizes a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1). </p><img src="/2021/09/PyTorch-Part2/image-20210913150303241.png" alt="image-20210913150303241"></li><li><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p></li></ul></li><li><p>MultiLabelSoftMarginLoss：<code>torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>SoftMarginLoss 多标签版本，a multi-label one-versus-all loss based on max-entropy.</p><p><img src="/2021/09/PyTorch-Part2/image-20210913150349060.png" alt="image-20210913150349060"></p></li><li><p><strong>weight</strong>(Tensor)- 为每个类别的 loss 设置权值。weight 必须是 float 类型的 tensor，其长度要于类别 C 一致，即每一个类别都要设置有 weight。</p></li></ul></li><li><p>CosineEmbeddingLoss：torch.nn.CosineEmbeddingLoss(margin=0, size_average=None, reduce=None, reduction=’elementwise_mean’)</p><ul><li><p>用 Cosine 函数来衡量两个输入是否相似。 used for learning nonlinear embeddings or semi-supervised</p><p><img src="/2021/09/PyTorch-Part2/image-20210913150448355.png" alt="image-20210913150448355"></p></li><li><p><strong>margin</strong>(float)- ： 取值范围[-1,1]， 推荐设置范围 [0, 0.5] </p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p></li></ul></li><li><p>MultiMarginLoss：<code>torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;)</code></p><ul><li><p>计算多分类的折页损失。</p><p><img src="/2021/09/PyTorch-Part2/image-20210913150527386.png" alt="image-20210913150527386"></p></li><li><p><strong>p</strong>(int)- 默认值为 1，仅可选 1 或者 2。</p><p><strong>margin</strong>(float)- 默认值为 1</p><p><strong>weight</strong>(Tensor)- 为每个类别的 loss 设置权值。weight 必须是 float 类型的 tensor，其长度要与类别 C 一致，即每一个类别都要设置有 weight。</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p></li></ul></li><li><p>TripletMarginLoss：torch.nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction=’elementwise_mean’)</p><ul><li><p>计算三元组损失，人脸验证中常用。</p><p><img src="/2021/09/PyTorch-Part2/image-20210913150644479.png" alt="image-20210913150644479"></p></li><li><p><strong>margin</strong>(float)- 默认值为 1</p><p><strong>p</strong>(int)- The norm degree ，默认值为 2</p><p><strong>swap</strong>(float)– The distance swap is described in detail in the paper Learning shallow convolutional </p><p><strong>feature</strong> descriptors with triplet losses by V. Balntas, E. Riba et al. Default: False</p><p><strong>size_average</strong>(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值；为 False时，返回的各样本的 loss 之和。</p><p><strong>reduce</strong>(bool)- 返回值是否为标量，默认为 True。</p></li></ul></li></ol><h2 id="优化器-Optimizer"><a href="#优化器-Optimizer" class="headerlink" title="优化器(Optimizer)"></a>优化器(Optimizer)</h2><h3 id="优化器类"><a href="#优化器类" class="headerlink" title="优化器类"></a>优化器类</h3><ul><li>PyTorch 中所有的优化器均是 Optimizer 的子类。</li></ul><h4 id="参数组"><a href="#参数组" class="headerlink" title="参数组"></a>参数组</h4><ul><li>参数组(param_groups)在 finetune、某层定制学习率、某层学习率置零等操作中，将发挥重要作用。</li><li>optimizer 对参数的管理是基于组的概念，可以为每一组参数配置特定的 lr、momentum、weight_decay 等等。</li><li>参数组在 optimizer 中表现为一个 list(self.param_groups)，其中每个元素是 dict，表示一个参数及其相应配置，在 dict 中包含’params’、’weight_decay’、’lr’ 、’momentum’等字段。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">w1 = torch.randn((<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个参数组</span></span><br><span class="line">optimizer_1 = optim.SGD([w1, w3], lr=<span class="number">0.1</span>)</span><br><span class="line">print(<span class="string">&#x27;len(optimizer.param_groups): &#x27;</span>, <span class="built_in">len</span>(optimizer_1.param_groups))</span><br><span class="line">print(optimizer_1.param_groups, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两个参数组</span></span><br><span class="line">optimizer_2 = optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span>: w1, <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>&#125;,</span><br><span class="line">                         &#123;<span class="string">&#x27;params&#x27;</span>: w2, <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.001</span>&#125;])</span><br><span class="line">print(<span class="string">&#x27;len(optimizer.param_groups): &#x27;</span>, <span class="built_in">len</span>(optimizer_2.param_groups))</span><br><span class="line">print(optimizer_2.param_groups)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">len(optimizer.param_groups): 1</span></span><br><span class="line"><span class="string">[&#123;&#x27;params&#x27;: [tensor([[0.6320, 0.4332], [-0.0429, -0.4769]], requires_grad=True), </span></span><br><span class="line"><span class="string">             tensor([[0.4598, 0.3449], [0.5621, -1.2329]], requires_grad=True)],</span></span><br><span class="line"><span class="string">  &#x27;lr&#x27;: 0.1, &#x27;momentum&#x27;: 0, &#x27;dampening&#x27;: 0, &#x27;weight_decay&#x27;: 0, &#x27;nesterov&#x27;: False&#125;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">len(optimizer.param_groups): 2</span></span><br><span class="line"><span class="string">[&#123;&#x27;params&#x27;: [tensor([[0.6320, 0.4332], [-0.0429, -0.4769]], requires_grad=True)], </span></span><br><span class="line"><span class="string">  &#x27;lr&#x27;: 0.1, &#x27;momentum&#x27;: 0, &#x27;dampening&#x27;: 0, &#x27;weight_decay&#x27;: 0, &#x27;nesterov&#x27;: False&#125;, </span></span><br><span class="line"><span class="string"> &#123;&#x27;params&#x27;: [tensor([[-0.8244, 2.3955], [0.6752, -0.0980]], requires_grad=True)], </span></span><br><span class="line"><span class="string">  &#x27;lr&#x27;: 0.001, &#x27;momentum&#x27;: 0, &#x27;dampening&#x27;: 0, &#x27;weight_decay&#x27;: 0, &#x27;nesterov&#x27;: False&#125;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="优化器常用方法"><a href="#优化器常用方法" class="headerlink" title="优化器常用方法"></a>优化器常用方法</h4><ol><li><code>zero_grad()</code>：将梯度清零。<ul><li>由于 PyTorch 不会自动清零梯度，所以在每一次反向传播之前都应当进行此操作。</li></ul></li><li><code>state_dict()</code>：将优化器的状态作为dict返回。返回 state、param_groups 组成的字典。<ul><li><strong>state</strong> - 保存当前优化状态的字典。 其内容优化器类之间有所不同。<br><strong>param_groups</strong> - 包含所有参数组的字典</li></ul></li><li><code>load_state_dict(state_dict)</code>：加载优化器状态。<ul><li>常用于 finetune。</li></ul></li><li><code>add_param_group()</code>：给 optimizer 管理的参数组中增加一组参数。<ul><li>可为该组参数定制 lr、momentum、weight_decay 等，在 finetune 中常用。</li></ul></li><li><code>step(closure)</code>：执行一步权值更新, 其中可传入参数 closure（一个闭包）。<ul><li>如，当采用 LBFGS 优化方法时，需要多次计算，因此需要传入一个闭包去允许它们重新计算 loss 。</li></ul></li></ol><h3 id="PyTorch-的十个优化器-干货"><a href="#PyTorch-的十个优化器-干货" class="headerlink" title="PyTorch 的十个优化器(干货)"></a>PyTorch 的十个优化器(干货)</h3><ol><li><p>torch.optim.SGD：<code>torch.optim.SGD(params, lr=&lt;object object&gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code></p><ul><li><p>实现带动量的 SGD 优化算法，并且均可拥有 weight_decay 项。</p></li><li><p><strong>params</strong>(iterable)- 参数组(参数组的概念请查看 3.2 优化器基类：Optimizer)，优化器要管理的那部分参数。</p><p><strong>lr</strong>(float)- 初始学习率，可按需随着训练过程不断调整学习率。</p><p><strong>momentum</strong>(float)- 动量，通常设置为 0.9，0.8</p><p><strong>dampening</strong>(float)- 若采用 nesterov，dampening 必须为 0.</p><p><strong>weight_decay</strong>(float)- 权值衰减系数，也就是 L2 正则项的系数</p><p><strong>nesterov</strong>(bool)- bool 选项，是否使用 NAG(Nesterov accelerated gradient)</p></li></ul></li><li><p>torch.optim.ASGD：<code>torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)</code></p><ul><li><p>ASGD 也称为 SAG，表示随机平均梯度下降(Averaged Stochastic Gradient Descent)，简单地说 ASGD 就是用空间换时间的一种 SGD。</p><p><a href="http://riejohnson.com/rie/stograd_nips.pdf">http://riejohnson.com/rie/stograd_nips.pdf</a> </p></li><li><p><strong>params</strong>(iterable)- 参数组(参数组的概念请查看 3.1 优化器基类：Optimizer)，优化器要优化的那些参数。</p><p><strong>lr</strong>(float)- 初始学习率，可按需随着训练过程不断调整学习率。</p><p><strong>lambd</strong>(float)- 衰减项，默认值 1e-4。</p><p><strong>alpha</strong>(float)- power for eta update ，默认值 0.75。</p><p><strong>t0</strong>(float)- point at which to start averaging，默认值 1e6。</p><p><strong>weight_decay</strong>(float)- 权值衰减系数，也就是 L2 正则项的系数。</p></li></ul></li><li><p>torch.optim.Rprop：<code>torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))</code></p><ul><li><p>实现 Rprop 优化方法(弹性反向传播)，该优化方法适用于 full-batch，不适用于 mini-batch，因而在 mini-batch 大行其道的时代里，很少见到。</p><p>《Martin Riedmiller und Heinrich Braun: Rprop -A Fast Adaptive Learning Algorithm. Proceedings of the International Symposium on Computer and Information Science VII, 1992》</p></li></ul></li><li><p>torch.optim.Adagrad：<code>torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0)</code></p><ul><li><p>Adagrad(Adaptive Gradient) 是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。</p><p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization John Duchi, Elad Hazan, Yoram Singer; 12(Jul):2121−2159, 2011.</a></p></li></ul></li><li><p>torch.optim.Adadelta：<code>torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)</code></p><ul><li><p>实现 Adadelta 优化方法。<strong>Adadelta</strong> 是 <strong>Adagrad</strong> 的改进。Adadelta 分母中采用距离当前时间点比较近的累计项，这可以避免在训练后期，学习率过小。</p><p><a href="https://arxiv.org/pdf/1212.5701.pdf">https://arxiv.org/pdf/1212.5701.pdf</a></p></li></ul></li><li><p>torch.optim.RMSprop：<code>torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)</code></p><ul><li><p>实现 RMSprop 优化方法（Hinton 提出），RMS 是均方根（root meam square）的意思。RMSprop 和 Adadelta 一样，也是对 Adagrad 的一种改进。RMSprop 采用均方根作为分母，可缓解 Adagrad 学习率下降较快的问题，并且引入均方根，可以减少摆动</p><p><a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a></p></li></ul></li><li><p>torch.optim.Adam(AMSGrad)：<code>torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e- 08, weight_decay=0, amsgrad=False)</code></p><ul><li><p>实现 Adam(Adaptive Moment Estimation))优化方法。Adam 是一种自适应学习率的优化方法，Adam 利用梯度的一阶矩估计和二阶矩估计动态的调整学习率。吴老师课上说过，Adam 是结合了 Momentum 和 RMSprop，并进行了偏差修正。</p><p><a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a></p></li><li><p><strong>amsgrad</strong>- 是否采用 AMSGrad 优化方法，asmgrad 优化方法是针对 Adam 的改进，通过添加额外的约束，使学习率始终为正值。</p></li></ul></li><li><p>torch.optim.Adamax：<code>torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)</code></p><ul><li><p>实现 Adamax 优化方法。Adamax 是对 Adam 增加了一个学习率上限的概念</p><p><a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a></p></li></ul></li><li><p>torch.optim.SparseAdam：<code>torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)</code></p><ul><li>针对稀疏张量的一种“阉割版”Adam 优化方法。</li></ul></li><li><p>torch.optim.LBFGS：<code>torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None)</code></p><ul><li>实现 L-BFGS（Limited-memory Broyden–Fletcher–Goldfarb–Shanno）优化方法。L-BFGS 属于拟牛顿算法。L-BFGS 是对 BFGS 的改进，特点就是节省内存。</li></ul></li></ol><h3 id="PyTorch-的六个学习率调整方法-干货"><a href="#PyTorch-的六个学习率调整方法-干货" class="headerlink" title="PyTorch 的六个学习率调整方法(干货)"></a>PyTorch 的六个学习率调整方法(干货)</h3><ol><li><p>StepLR：<code>torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)</code></p><ul><li><p>等间隔调整学习率，调整倍数为 gamma 倍，调整间隔为 step_size。</p><p>间隔单位是 step。需要注意的是，step 通常是指 epoch，不要当成 iteration 。</p></li><li><p><strong>step_size</strong>(int)- 学习率下降间隔数，若为 30，则会在 30、60、90……个 step 时，将学习率调整为 lr*gamma。</p><p><strong>gamma</strong>(float)- 学习率调整倍数，默认为 0.1 倍，即下降 10 倍。</p><p><strong>last_epoch</strong>(int)- 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 -1 时，学习率设置为初始值。</p></li></ul></li><li><p>MultiStepLR：<code>torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)</code></p><ul><li><p>按设定的间隔调整学习率。这个方法适合后期调试使用，观察 loss 曲线，为每个实验定制学习率调整时机</p></li><li><p><strong>milestones</strong>(list)- 一个 list，每一个元素代表何时调整学习率，list 元素必须是递增的。如 milestones=[30, 80, 120]</p><p><strong>gamma</strong>(float)- 学习率调整倍数，默认为 0.1 倍，即下降 10 倍。</p><p><strong>last_epoch</strong>(int)- 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 -1 时，学习率设置为初始值。</p></li></ul></li><li><p>ExponentialLR：<code>torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)</code></p><ul><li><p>按指数衰减调整学习率，调整公式: lr = lr * gamma**epoch</p></li><li><p><strong>gamma</strong>- 学习率调整倍数的底，指数为 epoch，即 gamma**epoch</p><p><strong>last_epoch</strong>(int)- 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为-1 时，学习率设置为初始值。</p></li></ul></li><li><p>CosineAnnealingLR：<code>torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)</code></p><ul><li><p>余弦退火，以余弦函数为周期，并在每个周期最大值时重新设置学习率。</p><p><a href="https://arxiv.org/abs/1608.03983">《SGDR: Stochastic Gradient Descent with Warm Restarts》(ICLR-2017)</a></p></li><li><p><strong>T_max</strong>(int)- 一次学习率周期的迭代次数，即 T_max 个 epoch 之后重新设置学习率。</p><p><strong>eta_min</strong>(float)- 最小学习率，即在一个周期中，学习率最小会下降到 eta_min，默认值为 0。</p></li></ul></li><li><p>ReduceLROnPlateau：<code>torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;,factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode=&#39;rel&#39;, cooldown=0, min_lr=0, eps=1e-08)</code></p><ul><li><p>检测指定的指标，当某指标不再变化（下降或升高）时，调整学习率。这是非常实用的学习率调整策略。</p><p>例如，当验证集的 loss 不再下降时，进行学习率调整；或者监测验证集的 accuracy，当 accuracy 不再上升时，则调整学习率。</p></li><li><p><strong>mode</strong>(str)- 模式选择，有 min 和 max 两种模式，min 表示当指标不再降低(如监测loss)，max 表示当指标不再升高(如监测 accuracy)。</p><p><strong>factor</strong>(float)- 学习率调整倍数(等同于其它方法的 gamma)，即学习率更新为 lr = lr * factor</p><p><strong>patience</strong>(int)- “耐心”，即忍受该指标多少个 step 不变化，当忍无可忍时，调整学习率。</p><p><strong>verbose</strong>(bool)- 是否打印学习率信息：print(‘Epoch {:5d}: reducing learning rate’ ‘ of group {} to {:.4e}.’.format(epoch, i, new_lr))</p><p><strong>threshold</strong>(float)- Threshold for measuring the new optimum，配合 threshold_mode 使用。</p><p><strong>threshold_mode</strong>(str)- 选择判断指标是否达最优的模式，有两种模式，rel 和 abs。 当 threshold_mode=rel，并且 mode=max 时，dynamic_threshold = best * ( 1 + threshold )；</p><p>​        当 threshold_mode=rel，并且 mode=min 时，dynamic_threshold = best * ( 1 -threshold )； </p><p>​        当 threshold_mode=abs，并且 mode=max 时，dynamic_threshold = best + threshold ； </p><p>​        当 threshold_mode=rel，并且 mode=max 时，dynamic_threshold = best - threshold </p><p><strong>cooldown</strong>(int)- “冷却时间“，当调整学习率之后，让学习率调整策略冷静一下，让模型再训练一段时间，再重启监测模式。</p><p><strong>min_lr</strong>(float or list)- 学习率下限，可为 float，或者 list，当有多个参数组时，可用 list 进行设置。</p><p><strong>eps</strong>(float)- 学习率衰减的最小值，当学习率变化小于 eps 时，则不调整学习率。</p></li></ul></li><li><p>LambdaLR：<code>torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=- 1)</code></p><ul><li><p>为不同参数组自定义学习率调整策略。调整规则为，lr = base_lr * lmbda(self.last_epoch) 。</p></li><li><p><strong>lr_lambda</strong>(function or list)- 一个计算学习率调整倍数的函数，输入通常为 step，当有多个参数组时，设为 list。</p><p><strong>last_epoch</strong>(int)- 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为-1 时，学习率设置为初始值。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">model = resnet18(pretrained=<span class="literal">False</span>)  <span class="comment"># 加载模型</span></span><br><span class="line">optimizer = torch.optim.SGD(params=[  <span class="comment"># 初始化优化器，并设置两个param_groups</span></span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: model.layer2.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: model.layer3.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.2</span>&#125;,</span><br><span class="line">], lr=<span class="number">1</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.005</span>)  <span class="comment"># base_lr = 0.1</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">500</span>  <span class="comment"># 训练次数</span></span><br><span class="line">warm_up_epoch = <span class="number">5</span></span><br><span class="line">t_max = epochs - warm_up_epoch  <span class="comment"># cos衰减周期</span></span><br><span class="line">lr_max = <span class="number">0.1</span>  <span class="comment"># 最大值</span></span><br><span class="line">lr_min = <span class="number">0.001</span>  <span class="comment"># 最小值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_warmup_cos_lambda</span>(<span class="params">lr_max, lr_min, start, warm_up_epoch, t_max</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">warmup_cos_lambda</span>(<span class="params">cur_epoch</span>):</span></span><br><span class="line">        <span class="keyword">if</span> cur_epoch &lt; start:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">elif</span> cur_epoch &lt; warm_up_epoch + start:</span><br><span class="line">            <span class="keyword">return</span> (cur_epoch - start) / warm_up_epoch + lr_min</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (lr_max - lr_min) * (<span class="number">1.0</span> + math.cos((cur_epoch - start - warm_up_epoch) / t_max * math.pi)) / <span class="number">2</span> + lr_min</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> warmup_cos_lambda</span><br><span class="line"></span><br><span class="line"><span class="comment"># LambdaLR</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(</span><br><span class="line">    optimizer,</span><br><span class="line">    lr_lambda=[</span><br><span class="line">        get_warmup_cos_lambda(lr_max, lr_min, <span class="number">0</span>, warm_up_epoch, t_max),</span><br><span class="line">        get_warmup_cos_lambda(lr_max, lr_min, <span class="number">5</span>, warm_up_epoch, t_max)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    print(optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>], optimizer.param_groups[<span class="number">1</span>][<span class="string">&#x27;lr&#x27;</span>])</span><br><span class="line">    optimizer.step()</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><h3 id="scheduler-step"><a href="#scheduler-step" class="headerlink" title="scheduler.step()"></a>scheduler.step()</h3></li></ol><ul><li>当调用 <code>scheduler.step(epoch=None)</code> 时，如果不传入 epoch，默认成员变量 last_epoch+=1，如果传入 epoch，则直接更新 last_epoch。</li><li>因此，scheduler.step() 要放在 epoch 的 for 循环当中执行。当然也可以放在每个 batch 的 iter 中更新，这样更加细致。</li><li>更新完 last_epoch 之后，则调用 <code>get_lr()</code> 获取当前 epoch 下，该参数组的学习率。</li></ul><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h3 id="TensorBoardX"><a href="#TensorBoardX" class="headerlink" title="TensorBoardX"></a>TensorBoardX</h3><ul><li>无法显示图表有可能是因为浏览器差异。</li></ul><h4 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h4><blockquote><p>在浏览器中查看可视化数据，只要在命令行中开启 tensorboard ：<code>tensorboard --logdir=&lt;your_log_dir&gt;</code></p><p>其中的 <your_log_dir> 既可以是单个 run 的路径，也可以是多个 run 的父目录。如 runs/ 下面可能会有很多的子文件夹，每个文件夹都代表了一次实验，我们令 –logdir=runs/ 就可以在 tensorboard 可视化界面中方便地横向比较不同实验所得数据的差异。</your_log_dir></p></blockquote><ol><li><p>SummaryWriter：<code>SummaryWriter(logdir=None, comment=&quot;&quot;, purge_step=None, max_queue=10, flush_secs=120, filename_suffix=&#39;&#39;, write_to_disk=True, log_dir=None, comet_config=&#123;&quot;disabled&quot;: True&#125;, **kwargs)</code></p><ul><li><p>创建一个 SummaryWriter 的实例</p></li><li><p><strong>logdir</strong>- 用该路径来保存日志。无参数，默认将使用 runs/日期时间</p><p><strong>comment</strong>- 文件夹后缀，将使用 runs/日期时间-comment 路径来保存日志</p><p><strong>filename_suffix</strong>- 设置 event file 文件名后缀</p></li><li><p>```python<br>writer = SummaryWriter(log_dir=’./tensorboard event file’, filename_suffix=str(cfg.EPOCH_NUMBER), comment=’test_tensorboard’)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2. add_scalar：&#96;add_scalar(tag, scalar_value, global_step&#x3D;None, walltime&#x3D;None)&#96;</span><br><span class="line"></span><br><span class="line">   - 在一个图表中记录一个标量的变化，常用于 loss、accuracy、learning rate 曲线的记录。</span><br><span class="line"></span><br><span class="line">   - **tag**(string)- 该图的标签，类似于 polt.title</span><br><span class="line"></span><br><span class="line">     **scalar_value**(float or string&#x2F;blobname)- 用于存储的值，曲线图的 y 坐标。注意，对于 PyTorch scalar tensor，需要调用 &#96;.item()&#96; 方法获取其数值</span><br><span class="line"></span><br><span class="line">     **global_step**(int)- 曲线图的 x 坐标</span><br><span class="line"></span><br><span class="line">     **walltime**(float)- 为 event 文件的文件名设置时间，默认为 time.time()</span><br><span class="line">     </span><br><span class="line">   - &#96;&#96;&#96;python</span><br><span class="line">     writer.add_scalar(&#39;Train Loss&#39;, train_loss &#x2F; num_mini_batch, epoch)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>add_scalars：<code>add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None)</code></p><ul><li><p>在一个图表中记录多个标量的变化，常用于对比，如 trainLoss 和 validLoss 的比较等。</p></li><li><p><strong>main_tag</strong>(string)- 该图的标签。</p><p><strong>tag_scalar_dict</strong>(dict)- key 是变量的 tag，value 是变量的值。</p><p><strong>global_step</strong>(int)- 曲线图的 x 坐标</p><p><strong>walltime</strong>(float)- 为 event 文件的文件名设置时间，默认为 time.time()</p></li><li><pre><code class="python">    writer.add_scalars(&#39;data/scalar_group&#39;, &#123;&quot;xsinx&quot;: x * np.sin(x),                                             &quot;xcosx&quot;: x * np.cos(x),                                             &quot;arctanx&quot;: np.arctan(x)&#125;, x)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">4. add_histogram：&#96;add_histogram(tag, values, global_step&#x3D;None, bins&#x3D;&#39;tensorflow&#39;, walltime&#x3D;None)&#96;</span><br><span class="line"></span><br><span class="line">   - 绘制直方图和多分位数折线图，常用于监测权值及梯度的分布变化情况，便于诊断网络更新方向是否正确。</span><br><span class="line"></span><br><span class="line">   - **tag**(string)- 该图的标签，类似于 polt.title。</span><br><span class="line"></span><br><span class="line">     **values**(torch.Tensor, numpy.array or string&#x2F;blobname)- 用于绘制直方图的值</span><br><span class="line"></span><br><span class="line">     **global_step**(int)- 曲线图的 y 坐标</span><br><span class="line"></span><br><span class="line">     **bins**(string)- 决定如何取 bins，默认为‘tensorflow’，可选：’auto’, ‘fd’等</span><br><span class="line"></span><br><span class="line">     **walltime**(float)- 为 event 文件的文件名设置时间，默认为 time.time()</span><br><span class="line"></span><br><span class="line">5. add_image：&#96;add_image(tag, img_tensor, global_step&#x3D;None, walltime&#x3D;None)&#96;</span><br><span class="line"></span><br><span class="line">   - 绘制图片，可用于检查模型的输入，监测 feature map 的变化，或是观察 weight。</span><br><span class="line"></span><br><span class="line">   - **tag**(string)- 该图的标签，类似于 polt.title。</span><br><span class="line"></span><br><span class="line">     **img_tensor**(torch.Tensor,numpy.array, or string&#x2F;blobname)- 需要可视化的图片数据， shape &#x3D; [C,H,W]。</span><br><span class="line"></span><br><span class="line">     **global_step**(int)- x 坐标。</span><br><span class="line"></span><br><span class="line">     **walltime**(float)- 为 event 文件的文件名设置时间，默认为 time.time()。</span><br><span class="line"></span><br><span class="line">   - 通常会借助 torchvision.utils.make_grid() 将一组图片绘制到一个窗口</span><br><span class="line"></span><br><span class="line">   - torchvision.utils.make_grid：&#96;torchvision.utils.make_grid(tensor, nrow&#x3D;8, padding&#x3D;2, normalize&#x3D;False, range&#x3D;None, scale_each&#x3D;False, pad_value&#x3D;0)&#96;</span><br><span class="line"></span><br><span class="line">     - 将一组图片拼接成一张图片，便于可视化。</span><br><span class="line"></span><br><span class="line">     - **tensor**(Tensor or list)- 需可视化的数据，shape:(B x C x H x W) ,B 表示 batch 数，即几张图片</span><br><span class="line"></span><br><span class="line">       **nrow**(int)- 一行显示几张图，默认值为 8。</span><br><span class="line"></span><br><span class="line">       **padding**(int)- 每张图片之间的间隔，默认值为 2。</span><br><span class="line"></span><br><span class="line">       **normalize**(bool)- 是否进行归一化至(0,1)。</span><br><span class="line"></span><br><span class="line">       **range**(tuple)- 设置归一化的 min 和 max，若不设置，默认从 tensor 中找 min 和 max。</span><br><span class="line"></span><br><span class="line">       **scale_each**(bool)- 每张图片是否单独进行归一化，还是 min 和 max 的一个选择。</span><br><span class="line"></span><br><span class="line">       **pad_value**(float)- 填充部分的像素值，默认为 0，即黑色。</span><br><span class="line"></span><br><span class="line">6. add_graph：&#96;add_graph(model, input_to_model&#x3D;None, verbose&#x3D;False, **kwargs)&#96;</span><br><span class="line"></span><br><span class="line">   - 绘制网络结构拓扑图。</span><br><span class="line"></span><br><span class="line">   - **model**(torch.nn.Module)- 模型实例</span><br><span class="line"></span><br><span class="line">     **inpjt_to_model**(torch.autograd.Variable)- 模型的输入数据，可以生成一个随机数，只要 shape 符合要求即可</span><br><span class="line">     </span><br><span class="line">   - &#96;&#96;&#96;python</span><br><span class="line">     init_img &#x3D; torch.zeros((1, 3, 400, 400), device&#x3D;device)</span><br><span class="line">     init_msg &#x3D; torch.zeros((1, secret_size), device&#x3D;device)</span><br><span class="line">     tb_writer.add_graph(StegaStampEncoder, &#123;&quot;img&quot;: init_img, &quot;msg&quot;: init_msg&#125;)</span><br></pre></td></tr></table></figure></code></pre></li><li><p>另外一种用于 debug 检查模型的方法：summary() 可输出模型每层输入输出的 shape 以及模型总量。使用前需要在终端 pip install torchsummary。</p><ul><li><pre><code class="python">from torchsummary import summaryprint(summary(net, (3, 360, 640), device=&quot;cpu&quot;))<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">7. add_embedding：&#96;add_embedding(mat, metadata&#x3D;None, label_img&#x3D;None, global_step&#x3D;None, tag&#x3D;&#39;default&#39;, metadata_header&#x3D;None)&#96;</span><br><span class="line"></span><br><span class="line">   - 在三维空间或二维空间展示数据分布，可选 T-SNE、PCA 和 CUSTOM 方法。</span><br><span class="line"></span><br><span class="line">   - **mat**(torch.Tensor or numpy.array)- 需要绘制的数据，一个样本必须是一个向量形式。</span><br><span class="line"></span><br><span class="line">     **shape** &#x3D; (N,D)，N 是样本数，D 是特征维数。</span><br><span class="line"></span><br><span class="line">     **metadata**(list)- 数据的标签，是一个 list，长度为 N。</span><br><span class="line"></span><br><span class="line">     **label_img**(torch.Tensor)- 空间中展示的图片，shape &#x3D; (N,C,H,W)。</span><br><span class="line"></span><br><span class="line">     **global_step**(int)- Global step value to record，不理解这里有何用处呢？知道的朋友补充一下吧。</span><br><span class="line"></span><br><span class="line">     **tag**(string)- 标签</span><br><span class="line"></span><br><span class="line">8. add_text：&#96;add_text(tag, text_string, global_step&#x3D;None, walltime&#x3D;None)&#96;</span><br><span class="line"></span><br><span class="line">   - 记录文字</span><br><span class="line"></span><br><span class="line">9. add_video：&#96;add_video(tag, vid_tensor, global_step&#x3D;None, fps&#x3D;4, walltime&#x3D;None)&#96;</span><br><span class="line"></span><br><span class="line">   - 记录 video</span><br><span class="line"></span><br><span class="line">10. add_figure：&#96;add_figure(tag, figure, global_step&#x3D;None, close&#x3D;True, walltime&#x3D;None)&#96;</span><br><span class="line"></span><br><span class="line">    - 添加 matplotlib 图片到图像中</span><br><span class="line"></span><br><span class="line">11. add_image_with_boxes：&#96;add_image_with_boxes(tag, img_tensor, box_tensor, global_step&#x3D;None, walltime&#x3D;None, **kwargs)&#96;</span><br><span class="line"></span><br><span class="line">    - 图像中绘制 Box，目标检测中会用到</span><br><span class="line"></span><br><span class="line">12. add_pr_curve：&#96;add_pr_curve(tag, labels, predictions, global_step&#x3D;None, num_thresholds&#x3D;127, weights&#x3D;None, walltime&#x3D;None)&#96;</span><br><span class="line"></span><br><span class="line">    - 绘制 PR 曲线</span><br><span class="line"></span><br><span class="line">13. add_pr_curve_raw：&#96;add_pr_curve_raw(tag, true_positive_counts, false_positive_counts, true_negative_counts, false_negative_counts, precision, recall, global_step&#x3D;None, num_thresholds&#x3D;127, weights&#x3D;None, walltime&#x3D;None)&#96;</span><br><span class="line"></span><br><span class="line">    - 从原始数据上绘制 PR 曲线</span><br><span class="line"></span><br><span class="line">14. export_scalars_to_json：&#96;export_scalars_to_json(path)&#96;</span><br><span class="line"></span><br><span class="line">    - 将 scalars 信息保存到 json 文件，便于后期使用</span><br><span class="line"></span><br><span class="line">#### 卷积核可视化 </span><br><span class="line"></span><br><span class="line">- 神经网络中最重要的就是权值，而人们对神经网络理解有限，所以我们需要通过尽可能了解权值来帮助诊断网络的训练情况。除了查看权值分布图和多折线分位图，还可以对卷积核权值进行可视化，来辅助我们分析网络。对卷积核权值进行可视化，在一定程度上帮助我们诊断网络的训练好坏，因此对卷积核权值的可视化十分有必要。</span><br><span class="line">- 可视化原理很简单，对单个卷积核进行“归一化”至 0～255，然后将其展现出来即可，这一系列操作可以借助 TensorboardX 的 add_image 来实现。</span><br><span class="line">- 决定一张特征图需要的卷积核的维度由输入通道决定，生成的特征图数量由卷积核的数量决定。</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line">import os</span><br><span class="line">import torch</span><br><span class="line">import torchvision.utils as vutils</span><br><span class="line">from tensorboardX import SummaryWriter</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">net &#x3D; Net()  # 创建一个网络</span><br><span class="line">pretrained_dict &#x3D; torch.load(os.path.join(&quot;..&quot;, &quot;2_model&quot;, &quot;net_params.pkl&quot;))</span><br><span class="line">net.load_state_dict(pretrained_dict)</span><br><span class="line"></span><br><span class="line">writer &#x3D; SummaryWriter(log_dir&#x3D;os.path.join(&quot;..&quot;, &quot;..&quot; &quot;Result&quot;, &quot;visual_weights&quot;))</span><br><span class="line">params &#x3D; net.state_dict()</span><br><span class="line">for k, v in params.items():</span><br><span class="line">    if &#39;conv&#39; in k and &#39;weight&#39; in k:</span><br><span class="line"></span><br><span class="line">        c_int &#x3D; v.size()[1]  # 输入层通道数</span><br><span class="line">        c_out &#x3D; v.size()[0]  # 输出层通道数</span><br><span class="line"></span><br><span class="line">        # 以feature map为单位，绘制一组卷积核，一张feature map对应的卷积核个数为输入通道数</span><br><span class="line">        for j in range(c_out):</span><br><span class="line">            print(k, v.size(), j)</span><br><span class="line">            kernel_j &#x3D; v[j, :, :, :].unsqueeze(1)  # 压缩维度，为make_grid制作输入</span><br><span class="line">            kernel_grid &#x3D; vutils.make_grid(kernel_j, normalize&#x3D;True, scale_each&#x3D;True, nrow&#x3D;c_int)  # 1*输入通道数, w, h</span><br><span class="line">            writer.add_image(k + &#39;_split_in_channel&#39;, kernel_grid, global_step&#x3D;j)  # j 表示feature map数</span><br><span class="line"></span><br><span class="line">        # 将一个卷积层的卷积核绘制在一起，每一行是一个 feature map 的卷积核</span><br><span class="line">        k_w, k_h &#x3D; v.size()[-1], v.size()[-2]</span><br><span class="line">        kernel_all &#x3D; v.view(-1, 1, k_w, k_h)</span><br><span class="line">        kernel_grid &#x3D; vutils.make_grid(kernel_all, normalize&#x3D;True, scale_each&#x3D;True, nrow&#x3D;c_int)  # 1*输入通道数, w, h</span><br><span class="line">        writer.add_image(k + &#39;_all&#39;, kernel_grid, global_step&#x3D;666)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>#### 特征图可视化</code></pre></li></ul></li></ul></li><li><p>获取图片，将其转换成模型输入前的数据格式，即一系列 transform，</p></li><li><p>获取模型各层操作，手动的执行每一层操作，拿到所需的 feature maps，</p></li><li><p>借助 tensorboardX 进行绘制。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> utils.utils <span class="keyword">import</span> MyDataset, Net, normalize_invert</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">vis_layer = <span class="string">&#x27;conv1&#x27;</span></span><br><span class="line">log_dir = os.path.join(<span class="string">&quot;..&quot;</span>, <span class="string">&quot;..&quot;</span> <span class="string">&quot;Result&quot;</span>, <span class="string">&quot;visual_featuremaps&quot;</span>)</span><br><span class="line">txt_path = os.path.join(<span class="string">&quot;..&quot;</span>, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;Data&quot;</span>, <span class="string">&quot;visual.txt&quot;</span>)</span><br><span class="line">pretrained_path = os.path.join(<span class="string">&quot;..&quot;</span>, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;Data&quot;</span>, <span class="string">&quot;net_params_72p.pkl&quot;</span>)</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(pretrained_path))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">normMean = [<span class="number">0.49139968</span>, <span class="number">0.48215827</span>, <span class="number">0.44653124</span>]</span><br><span class="line">normStd = [<span class="number">0.24703233</span>, <span class="number">0.24348505</span>, <span class="number">0.26158768</span>]</span><br><span class="line">testTransform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(normMean, normStd)</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 载入数据</span></span><br><span class="line">test_data = MyDataset(txt_path=txt_path, transform=testTransform)</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">1</span>)</span><br><span class="line">img, label = <span class="built_in">iter</span>(test_loader).<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line">x = img</span><br><span class="line">writer = SummaryWriter(log_dir=log_dir)</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net._modules.items():</span><br><span class="line">    <span class="comment"># 为fc层预处理x</span></span><br><span class="line">    x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>) <span class="keyword">if</span> <span class="string">&quot;fc&quot;</span> <span class="keyword">in</span> name <span class="keyword">else</span> x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对x执行单层运算</span></span><br><span class="line">    x = layer(x)</span><br><span class="line">    print(x.size())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 由于__init__()相较于forward()缺少relu操作，需要手动增加</span></span><br><span class="line">    x = F.relu(x) <span class="keyword">if</span> <span class="string">&#x27;conv&#x27;</span> <span class="keyword">in</span> name <span class="keyword">else</span> x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 依据选择的层，进行记录feature maps</span></span><br><span class="line">    <span class="keyword">if</span> name == vis_layer:</span><br><span class="line">        <span class="comment"># 绘制feature maps</span></span><br><span class="line">        x1 = x.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># C，B, H, W  ---&gt; B，C, H, W</span></span><br><span class="line">        img_grid = vutils.make_grid(x1, normalize=<span class="literal">True</span>, scale_each=<span class="literal">True</span>, nrow=<span class="number">2</span>)  <span class="comment"># B，C, H, W</span></span><br><span class="line">        writer.add_image(vis_layer + <span class="string">&#x27;_feature_maps&#x27;</span>, img_grid, global_step=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绘制原始图像</span></span><br><span class="line">        img_raw = normalize_invert(img, normMean, normStd)  <span class="comment"># 图像去标准化</span></span><br><span class="line">        img_raw = np.array(img_raw * <span class="number">255</span>).clip(<span class="number">0</span>, <span class="number">255</span>).squeeze().astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">        writer.add_image(<span class="string">&#x27;raw img&#x27;</span>, img_raw, global_step=<span class="number">666</span>)  <span class="comment"># j 表示feature map数</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h4 id="梯度及权值分布可视化"><a href="#梯度及权值分布可视化" class="headerlink" title="梯度及权值分布可视化"></a>梯度及权值分布可视化</h4><h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><ul><li>在网络训练过程中，我们常常会遇到梯度消失、梯度爆炸等问题，我们可以通过记录每个 epoch 的梯度的值来监测梯度的情况，还可以记录权值，分析权值更新的方向是否符合规律。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> utils.utils <span class="keyword">import</span> MyDataset, validate, show_confMat, Net</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line">train_txt_path = os.path.join(<span class="string">&quot;..&quot;</span>, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;Data&quot;</span>, <span class="string">&quot;train.txt&quot;</span>)</span><br><span class="line">valid_txt_path = os.path.join(<span class="string">&quot;..&quot;</span>, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;Data&quot;</span>, <span class="string">&quot;valid.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">classes_name = [<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>]</span><br><span class="line"></span><br><span class="line">train_bs = <span class="number">16</span></span><br><span class="line">valid_bs = <span class="number">16</span></span><br><span class="line">lr_init = <span class="number">0.001</span></span><br><span class="line">max_epoch = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># log</span></span><br><span class="line">log_dir = os.path.join(<span class="string">&quot;..&quot;</span>, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;Result&quot;</span>, <span class="string">&quot;hist_grad_weight&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(log_dir=log_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------ step 1/4 : 加载数据-------------------------------------------------</span></span><br><span class="line"><span class="comment"># 数据预处理设置</span></span><br><span class="line">normMean = [<span class="number">0.4948052</span>, <span class="number">0.48568845</span>, <span class="number">0.44682974</span>]</span><br><span class="line">normStd = [<span class="number">0.24580306</span>, <span class="number">0.24236229</span>, <span class="number">0.2603115</span>]</span><br><span class="line">trainTransform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">32</span>),</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(normMean, normStd)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">validTransform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(normMean, normStd)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建MyDataset实例</span></span><br><span class="line">train_data = MyDataset(txt_path=train_txt_path, transform=trainTransform)</span><br><span class="line">valid_data = MyDataset(txt_path=valid_txt_path, transform=validTransform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建DataLoder</span></span><br><span class="line">train_loader = DataLoader(dataset=train_data, batch_size=train_bs, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_loader = DataLoader(dataset=valid_data, batch_size=valid_bs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------ step 2/4 : 网络初始化----------------------------------------------</span></span><br><span class="line">net = Net()  <span class="comment"># 创建一个网络</span></span><br><span class="line">net.initialize_weights()  <span class="comment"># 初始化权值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------ step 3/4 : 定义损失函数和优化器 ------------------------------------</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 选择损失函数</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=lr_init, momentum=<span class="number">0.9</span>, dampening=<span class="number">0.1</span>)  <span class="comment"># 选择优化器</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">50</span>, gamma=<span class="number">0.1</span>)  <span class="comment"># 设置学习率下降策略</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------ step 4/4 : 训练 --------------------------------------------------</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    loss_sigma = <span class="number">0.0</span>  <span class="comment"># 记录一个epoch的loss之和</span></span><br><span class="line">    correct = <span class="number">0.0</span></span><br><span class="line">    total = <span class="number">0.0</span></span><br><span class="line">    scheduler.step()  <span class="comment"># 更新学习率</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment"># 获取图片和标签</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line">        inputs, labels = Variable(inputs), Variable(labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward, backward, update weights</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 统计预测信息</span></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).squeeze().<span class="built_in">sum</span>().numpy()</span><br><span class="line">        loss_sigma += loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每10个iteration 打印一次训练信息，loss为10个iteration的平均</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">9</span>:</span><br><span class="line">            loss_avg = loss_sigma / <span class="number">10</span></span><br><span class="line">            loss_sigma = <span class="number">0.0</span></span><br><span class="line">            print(<span class="string">&quot;Training: Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Loss: &#123;:.4f&#125; Acc:&#123;:.2%&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch + <span class="number">1</span>, max_epoch, i + <span class="number">1</span>, <span class="built_in">len</span>(train_loader), loss_avg, correct / total))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个epoch，记录梯度，权值</span></span><br><span class="line">    <span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">        writer.add_histogram(name + <span class="string">&#x27;_grad&#x27;</span>, layer.grad.cpu().data.numpy(), epoch)</span><br><span class="line">        writer.add_histogram(name + <span class="string">&#x27;_data&#x27;</span>, layer.cpu().data.numpy(), epoch)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><h5 id="可视化分析"><a href="#可视化分析" class="headerlink" title="可视化分析"></a>可视化分析</h5><h6 id="权值-weights-的监控"><a href="#权值-weights-的监控" class="headerlink" title="权值 weights 的监控"></a>权值 weights 的监控</h6><p>经过 100 个 epoch 的训练，来看看第一个卷积层的权值分布的变化。x 轴即变量大小，y 轴为 gloabl_step。</p><p>图 1 x=0.306， y=0， 数值显示为 0.00，表示第 0 个 epoch 时，权值为 0.306 的个数为 0.00。 </p><img src="/2021/09/PyTorch-Part2/image-20210914221527396.png" alt="image-20210914221527396" style="zoom:67%;"><p>图 2， x=0.306， y=85， 数值显示为 5.71，表示第 85 个 epoch 时，权值在 0.306 区间的有 5.71 个。</p><img src="/2021/09/PyTorch-Part2/image-20210914221609780.png" alt="image-20210914221609780" style="zoom:67%;"><p>通过 HISTOGRAMS 可以看到第一个卷积层的权值随着训练的不断的“扩散”，一开始是个比较标准的高斯分布，并且最大值不会超过 0.3。</p><p>而到了后期，权值会发散到 0.6+，这个问题也是需要关注的，若权值太大容易导致过拟合。因为模型的输出值会被该特征所主导，从而引起过拟合现象，这个可以通过权值衰减(weight_decay)来缓解。</p><h6 id="偏置-bias-的监控"><a href="#偏置-bias-的监控" class="headerlink" title="偏置 bias 的监控"></a>偏置 bias 的监控</h6><p>通常会监控输出层的 bias 的大小，若有特别大，或者特别小的 bias，那么某一类别的召回率可能会很低，可以通过观察输出层的 bias 来诊断是否在这一环节出问题。</p><p>从图上可以看到，一开始 10 个类别的 bias 都比较小，随着训练的进行，每个类别都有了自己的固定的 bias 大小。</p><img src="/2021/09/PyTorch-Part2/image-20210914221837983.png" alt="image-20210914221837983" style="zoom:67%;"><h6 id="梯度的监控"><a href="#梯度的监控" class="headerlink" title="梯度的监控"></a>梯度的监控</h6><p>下图为第一个卷积层权值的梯度变化情况，可以看到，几乎都是服从高斯分布的。倘若前面几层的梯度非常小，那么就是梯度流通不畅导致的，可以考虑残差结构或者辅助损失层等 trick 来解决梯度消失。</p><img src="/2021/09/PyTorch-Part2/image-20210914221932315.png" alt="image-20210914221932315" style="zoom:67%;"><h6 id="文末思考："><a href="#文末思考：" class="headerlink" title="文末思考："></a>文末思考：</h6><ol><li><p>通过观察各层的梯度，权值分布，我们可以针对性的设置学习率，为那些梯度小的层设置更大的学习率，让那些层可以有效的更新。</p></li><li><p>对权值特别大的那些层，可以考虑为那一层设置更大的 weight_decay，是否能有效降低该层权值大小呢。</p></li><li><p>通过对梯度的观察，可以合理的设置梯度 clip 的值。</p></li></ol><h4 id="混淆矩阵及其可视化"><a href="#混淆矩阵及其可视化" class="headerlink" title="混淆矩阵及其可视化"></a>混淆矩阵及其可视化</h4><p>混淆矩阵(Confusion Matrix)常用来观察分类结果，其是一个 N*N 的方阵，N 表示类别数。混淆矩阵的行表示真实类别，列表示预测类别。</p><img src="/2021/09/PyTorch-Part2/image-20210914222323808.png" alt="image-20210914222323808" style="zoom: 67%;"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_confMat</span>(<span class="params">confusion_mat, classes_name, set_name, out_dir</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    可视化混淆矩阵，保存png格式</span></span><br><span class="line"><span class="string">    :param confusion_mat: nd-array</span></span><br><span class="line"><span class="string">    :param classes_name: list,各类别名称</span></span><br><span class="line"><span class="string">    :param set_name: str, eg: &#x27;valid&#x27;, &#x27;train&#x27;</span></span><br><span class="line"><span class="string">    :param out_dir: str, png输出的文件夹</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    confusion_mat_N = confusion_mat.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes_name)):</span><br><span class="line">        confusion_mat_N[i, :] = confusion_mat[i, :] / confusion_mat[i, :].<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取颜色</span></span><br><span class="line">    cmap = plt.cm.get_cmap(<span class="string">&#x27;Greys&#x27;</span>)  <span class="comment"># 更多颜色: http://matplotlib.org/examples/color/colormaps_reference.html</span></span><br><span class="line">    plt.imshow(confusion_mat_N, cmap=cmap)</span><br><span class="line">    plt.colorbar()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置文字</span></span><br><span class="line">    xlocations = np.array(<span class="built_in">range</span>(<span class="built_in">len</span>(classes_name)))</span><br><span class="line">    plt.xticks(xlocations, classes_name, rotation=<span class="number">60</span>)</span><br><span class="line">    plt.yticks(xlocations, classes_name)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Predict label&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;True label&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Confusion_Matrix_&#x27;</span> + set_name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印数字</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(confusion_mat_N.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(confusion_mat_N.shape[<span class="number">1</span>]):</span><br><span class="line">            plt.text(x=j, y=i, s=<span class="built_in">int</span>(confusion_mat[i, j]), va=<span class="string">&#x27;center&#x27;</span>, ha=<span class="string">&#x27;center&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># 保存</span></span><br><span class="line">    plt.savefig(os.path.join(out_dir, <span class="string">&#x27;Confusion_Matrix_&#x27;</span> + set_name + <span class="string">&#x27;.png&#x27;</span>))</span><br><span class="line">    plt.close()</span><br></pre></td></tr></table></figure><h3 id="wandb-待补充"><a href="#wandb-待补充" class="headerlink" title="wandb(待补充)"></a>wandb(待补充)</h3><blockquote><p>GitHub：<a href="https://github.com/wandb/client">https://github.com/wandb/client</a></p><p>文档：<a href="https://docs.wandb.ai/">https://docs.wandb.ai/</a></p></blockquote>]]></content>
    
    
    <summary type="html">Pytorch模型训练</summary>
    
    
    
    <category term="PyTorch" scheme="https://nephrencake.gitee.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch-Part1</title>
    <link href="https://nephrencake.gitee.io/2021/09/PyTorch-Part1/"/>
    <id>https://nephrencake.gitee.io/2021/09/PyTorch-Part1/</id>
    <published>2021-09-11T14:15:30.000Z</published>
    <updated>2021-10-26T15:14:14.080Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch-Part1——基本概念"><a href="#PyTorch-Part1——基本概念" class="headerlink" title="PyTorch-Part1——基本概念"></a>PyTorch-Part1——基本概念</h1><p>[TOC]</p><h2 id="资源汇总（后续放入总结篇）"><a href="#资源汇总（后续放入总结篇）" class="headerlink" title="资源汇总（后续放入总结篇）"></a>资源汇总（后续放入总结篇）</h2><ol><li><a href="https://github.com/zergtant/pytorch-handbook">PyTorch-handbook 中文手册</a>：与 PyTorch 版本保持一致。</li><li><a href="https://github.com/TingsongYu/PyTorch_Tutorial">《Pytorch模型训练实用教程》</a>：PyTorch 模型训练方面的干货教程。特别工业化，真的非常棒。</li><li><a href="https://www.pytorch123.com/">PyTorch官方教程中文版</a>：标准教程，主要是因为有stn。</li><li><a href="https://github.com/chenyuntc/pytorch-book">《深度学习框架PyTorch：入门与实践》</a>：理论和实战，动漫头像生成器。</li><li><a href="https://github.com/bharathgs/Awesome-pytorch-list">Awesome-Pytorch-list</a>：庞大的 PyTorch 资源库。</li><li><a href="https://github.com/pytorch/examples">PyTorch Examples</a>：入门案例，可以在这个基础上增改自己的代码。</li><li><a href="https://discuss.pytorch.org/">PyTorch Forums</a>：PyTorch 官方论坛，可以经常翻阅，减少弯路。</li></ol><ol><li><a href="https://blog.csdn.net/tszupup/article/details/112916388">检查是否可导</a></li><li><a href="https://www.zhihu.com/question/291987781">哪些操作不可微</a></li></ol><p>本篇笔记只记录 PyTorch 常用操作</p><ol><li>按照训练顺序记录各步骤常用方法</li><li>实战干货总结</li><li>不同网络案例代码</li><li>底层剖析与数学原理</li><li>tf1转pytorch</li></ol><h2 id="Pytorch-简介"><a href="#Pytorch-简介" class="headerlink" title="Pytorch 简介"></a>Pytorch 简介</h2><ul><li>Torch 是一个与 Numpy 类似的张量（Tensor）操作库，与 Numpy 不同的是 Torch 对GPU支持的很好，Lua 是 Torch 的上层包装。</li><li>PyTorch 和 Torch 使用包含所有相同性能的C库：TH, THC, THNN, THCUNN，只是使用了不同的上层包装语言。</li><li>PyTorch 框架设计相当简洁优雅且高效快速。</li><li>与 google 的 Tensorflow 类似，FAIR 的支持足以确保 PyTorch 获得持续的开发更新。</li><li>PyTorch 拥有完善的文档，作者亲自维护论坛。</li></ul><h2 id="PyTorch-安装与测试"><a href="#PyTorch-安装与测试" class="headerlink" title="PyTorch 安装与测试"></a>PyTorch 安装与测试</h2><ul><li><p>PyTorch 官网：<a href="https://pytorch.org/">https://pytorch.org/</a></p></li><li><p>CUDA安装：<a href="https://blog.csdn.net/Mind_programmonkey/article/details/99688839/">https://blog.csdn.net/Mind_programmonkey/article/details/99688839/</a></p></li><li><p>测试安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.__version__</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</a>)</p></li></ul><h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><h3 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h3><ul><li>快速测试可以经常使用 <code>torch.rand()</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建未初始化的5行3列的矩阵，注意和 torch.zeros 是不同的</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 创建一个随机初始化的矩阵</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 创建一个 0 填充的矩阵，数据类型为 long，long 不允许计算梯度</span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line"><span class="comment"># 创建 tensor 并使用现有数据初始化，只要有一个是 float，则都为 float</span></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># _like 方法: 根据现有的张量创建相同大小的张量</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">x = torch.ones_like(x).to(torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># 获取张量大小。size() 和 shape 是等价的。</span></span><br><span class="line">print(x.size())</span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure><ul><li>可以好好体会下面一个例子: <code>batch_size=2 channel=3 size=(h=4, w=5)</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span> x = torch.rand((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([[[[<span class="number">0.0628</span>, <span class="number">0.6673</span>, <span class="number">0.3958</span>, <span class="number">0.0904</span>, <span class="number">0.2442</span>],</span><br><span class="line">          [<span class="number">0.4635</span>, <span class="number">0.0213</span>, <span class="number">0.2310</span>, <span class="number">0.1643</span>, <span class="number">0.7705</span>],</span><br><span class="line">          [<span class="number">0.6754</span>, <span class="number">0.9084</span>, <span class="number">0.3516</span>, <span class="number">0.8552</span>, <span class="number">0.5362</span>],</span><br><span class="line">          [<span class="number">0.0650</span>, <span class="number">0.8016</span>, <span class="number">0.1424</span>, <span class="number">0.3343</span>, <span class="number">0.0216</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.2237</span>, <span class="number">0.5758</span>, <span class="number">0.1204</span>, <span class="number">0.8498</span>, <span class="number">0.4453</span>],</span><br><span class="line">          [<span class="number">0.0703</span>, <span class="number">0.1054</span>, <span class="number">0.4191</span>, <span class="number">0.1271</span>, <span class="number">0.9603</span>],</span><br><span class="line">          [<span class="number">0.4301</span>, <span class="number">0.9627</span>, <span class="number">0.9707</span>, <span class="number">0.9125</span>, <span class="number">0.9281</span>],</span><br><span class="line">          [<span class="number">0.4365</span>, <span class="number">0.1514</span>, <span class="number">0.9759</span>, <span class="number">0.4679</span>, <span class="number">0.8695</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.4225</span>, <span class="number">0.5115</span>, <span class="number">0.2755</span>, <span class="number">0.1248</span>, <span class="number">0.8858</span>],</span><br><span class="line">          [<span class="number">0.4288</span>, <span class="number">0.8042</span>, <span class="number">0.2394</span>, <span class="number">0.6829</span>, <span class="number">0.5082</span>],</span><br><span class="line">          [<span class="number">0.7765</span>, <span class="number">0.7435</span>, <span class="number">0.2163</span>, <span class="number">0.9029</span>, <span class="number">0.6852</span>],</span><br><span class="line">          [<span class="number">0.2889</span>, <span class="number">0.3367</span>, <span class="number">0.8794</span>, <span class="number">0.9265</span>, <span class="number">0.6639</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">0.8373</span>, <span class="number">0.0672</span>, <span class="number">0.8151</span>, <span class="number">0.7912</span>, <span class="number">0.2508</span>],</span><br><span class="line">          [<span class="number">0.9232</span>, <span class="number">0.5653</span>, <span class="number">0.1964</span>, <span class="number">0.0986</span>, <span class="number">0.5448</span>],</span><br><span class="line">          [<span class="number">0.8444</span>, <span class="number">0.8974</span>, <span class="number">0.0763</span>, <span class="number">0.9074</span>, <span class="number">0.7959</span>],</span><br><span class="line">          [<span class="number">0.6146</span>, <span class="number">0.1738</span>, <span class="number">0.0814</span>, <span class="number">0.7200</span>, <span class="number">0.0448</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.9523</span>, <span class="number">0.3441</span>, <span class="number">0.1840</span>, <span class="number">0.9286</span>, <span class="number">0.4231</span>],</span><br><span class="line">          [<span class="number">0.9800</span>, <span class="number">0.4126</span>, <span class="number">0.8632</span>, <span class="number">0.8323</span>, <span class="number">0.2245</span>],</span><br><span class="line">          [<span class="number">0.9756</span>, <span class="number">0.5459</span>, <span class="number">0.1382</span>, <span class="number">0.2115</span>, <span class="number">0.0617</span>],</span><br><span class="line">          [<span class="number">0.8045</span>, <span class="number">0.4060</span>, <span class="number">0.6943</span>, <span class="number">0.0992</span>, <span class="number">0.4955</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.0712</span>, <span class="number">0.9649</span>, <span class="number">0.7187</span>, <span class="number">0.5300</span>, <span class="number">0.8720</span>],</span><br><span class="line">          [<span class="number">0.2673</span>, <span class="number">0.9442</span>, <span class="number">0.5604</span>, <span class="number">0.2986</span>, <span class="number">0.2902</span>],</span><br><span class="line">          [<span class="number">0.8061</span>, <span class="number">0.5989</span>, <span class="number">0.4864</span>, <span class="number">0.7042</span>, <span class="number">0.1167</span>],</span><br><span class="line">          [<span class="number">0.6609</span>, <span class="number">0.0652</span>, <span class="number">0.9130</span>, <span class="number">0.8308</span>, <span class="number">0.6552</span>]]]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="操作张量"><a href="#操作张量" class="headerlink" title="操作张量"></a>操作张量</h3><ul><li>加法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.ones(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 1. 直接运算符相加</span></span><br><span class="line">z = x + y</span><br><span class="line"><span class="comment"># 2. .add() 方法</span></span><br><span class="line">z = torch.add(x, y)</span><br><span class="line"><span class="comment"># 3. 替换指定张量</span></span><br><span class="line">z = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=z)</span><br><span class="line"><span class="comment"># 4. 将 x 加在y上</span></span><br><span class="line">y.add_(x)</span><br></pre></td></tr></table></figure><blockquote><p>任何 以 <code>_</code> 结尾的操作都会用结果替换原变量。例如：<code>x.copy_(y)</code>、<code>x.t_()</code>，都会改变 <code>x</code>。</p></blockquote><ul><li>使用索引切片操作张量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x[:, <span class="number">1</span>])</span><br><span class="line">print(x[:, [<span class="number">1</span>]])  <span class="comment"># 依然保持二维</span></span><br></pre></td></tr></table></figure><ul><li><code>torch.view()</code>：改变张量的维度和大小</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand((<span class="number">4</span>, <span class="number">4</span>), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># -1 为自动推断</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure><ul><li><code>.item()</code>：以Python数据类型获取张量中的数值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(<span class="built_in">type</span>(x.item()))</span><br></pre></td></tr></table></figure><blockquote><p>100+ Tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc., are described here <a href="https://pytorch.org/docs/torch">https://pytorch.org/docs/torch</a>.</p></blockquote><h3 id="NumPy-转换"><a href="#NumPy-转换" class="headerlink" title="NumPy 转换"></a>NumPy 转换</h3><blockquote><p>Torch Tensor与NumPy数组共享底层内存地址，修改任何一个都会导致另一个变化。</p></blockquote><ul><li><code>.numpy()</code>：将一个 Torch Tensor 转换为 NumPy 数组</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)  <span class="comment"># tensor([1., 1., 1., 1., 1.])</span></span><br><span class="line">b = a.numpy()  <span class="comment"># [1. 1. 1. 1. 1.]</span></span><br><span class="line"><span class="comment"># 共享内存的操作</span></span><br><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)  <span class="comment"># tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line">print(b)  <span class="comment"># [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="comment"># 不共享内存的操作</span></span><br><span class="line">a = a + <span class="number">1</span></span><br><span class="line">print(a)  <span class="comment"># tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line">print(b)  <span class="comment"># [1. 1. 1. 1. 1.]</span></span><br></pre></td></tr></table></figure><ul><li><code>torch.from_numpy(a)</code>：NumPy Array 转化成 Torch Tensor</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line"><span class="comment"># 共享内存的操作</span></span><br><span class="line">a += <span class="number">1</span></span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line"><span class="comment"># 不共享内存的操作</span></span><br><span class="line">a = a + <span class="number">1</span></span><br><span class="line">a = np.add(a, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="CUDA-张量"><a href="#CUDA-张量" class="headerlink" title="CUDA 张量"></a>CUDA 张量</h3><ul><li><code>torch.device(&quot;cuda:0&quot;)</code>：参数可以为 <code>&quot;cuda:0&quot;/&quot;cuda&quot;/&quot;cpu&quot;</code>。</li><li><code>device参数</code>：可传参同上。</li><li><code>.to(device)</code>：可以指定数据类型，也移动到指定设备。<ul><li>同时指定时顺序需要为：<code>.to(device, torch.float)</code></li><li>注意：只调用 <code>.to(&quot;cuda&quot;)</code> 并没有复制张量到 GPU 上，而是返回了一个 copy。所以，需要把它赋值给一个新的张量并在GPU上使用这个张量。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)        <span class="comment"># 创建 CUDA 设备对象</span></span><br><span class="line">y = torch.ones_like(x, device=device)  <span class="comment"># 直接在 CUDA 中创建张量</span></span><br><span class="line">x = x.to(device)                       <span class="comment"># .to(&quot;cuda&quot;) 将张量移动到 cuda 中</span></span><br><span class="line">z = x + y</span><br><span class="line">print(z)</span><br><span class="line">print(z.to(<span class="string">&quot;cpu&quot;</span>, torch.double))       <span class="comment"># 同时指定设备与类型</span></span><br></pre></td></tr></table></figure><h2 id="自动求导机制-Autograd"><a href="#自动求导机制-Autograd" class="headerlink" title="自动求导机制(Autograd)"></a>自动求导机制(Autograd)</h2><ul><li> <code>torch.autograd</code> 包是 PyTorch 中所有神经网络的核心，它为张量上的所有操作提供了自动求导。</li><li> <code>torch.autograd</code> 是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。</li></ul><h3 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h3><ul><li><p><code>torch.Tensor</code> 是这个包的核心类。</p><ul><li>如果设置 <code>.requires_grad=True</code>，那么将会追踪所有对于该张量的操作。当完成计算后通过调用 <code>.backward()</code>，自动计算所有的梯度，这个张量的所有梯度将会积累到 <code>.grad</code> 属性。</li><li>要阻止张量跟踪历史记录，可以调用 <code>.detach()</code> 方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。</li><li>为了防止跟踪历史记录（和使用内存），可以将代码块包装在<code>with torch.no_grad()：</code>中。在评估模型时特别有用，因为模型可能具有 <code>requires_grad = True</code> 的可训练参数，但是我们不需要梯度计算。</li></ul></li><li><p>在自动梯度计算中还有另外一个重要的类 <code>Function</code>.</p><ul><li><code>Tensor</code> 和 <code>Function</code> 互相连接并生成一个非循环图，它表示和存储了完整的计算历史。每个张量都有一个 <code>.grad_fn</code> 属性，这个属性引用了一个创建了 <code>Tensor</code> 的 <code>Function</code> （除非这个张量是用户手动创建的，即，这个张量的 <code>grad_fn</code> 是 <code>None</code>）。</li><li>如果需要计算导数，你可以在 <code>Tensor</code> 上调用 <code>.backward()</code>。 如果 <code>Tensor</code> 是一个标量（即它包含一个元素数据）则不需要为 <code>backward()</code> 指定任何参数，但是如果它有更多的元素，你需要指定一个<code>gradient</code> 参数来匹配张量的形状。</li></ul></li><li><p>在其他的文章中可能会看到说将 Tensor 包裹到 Variable 中提供自动梯度计算。Variable 在0.41版中已经被标注为过期了，现在可以直接使用 Tensor，官方文档：<a href="https://pytorch.org/docs/stable/autograd.html#variable-deprecated">https://pytorch.org/docs/stable/autograd.html#variable-deprecated</a></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)  <span class="comment"># requires_grad=True</span></span><br><span class="line">y = x + <span class="number">2</span>  <span class="comment"># &lt;AddBackward0 object at 0x0000025D5A03A860&gt;</span></span><br><span class="line">print(y)  <span class="comment"># 进行了一次加运算得出结果 y，因此自动生成了 grad_fn 追踪张量操作，但还没有生成梯度</span></span><br><span class="line"></span><br><span class="line">z = y * y * <span class="number">3</span>  <span class="comment"># grad_fn=&lt;MulBackward0&gt;</span></span><br><span class="line">out = z.mean()  <span class="comment"># grad_fn=&lt;MeanBackward0&gt;</span></span><br></pre></td></tr></table></figure><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><ul><li><img src="/2021/09/PyTorch-Part1/image-20210912171608055.png" alt="image-20210912171608055" style="zoom:150%;"></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 现在让我们来看一个vector-Jacobian product的例子</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这个情形中，y不再是个标量。torch.autograd无法直接计算出完整的雅可比行列，但是如果我们只想要vector-Jacobian product，只需将向量作为参数传入backward：</span></span><br><span class="line">gradients = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(gradients)</span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果.requires_grad=True但是你又不希望进行autograd的计算， 那么可以将变量包裹在 with torch.no_grad()中:</span></span><br><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure><blockquote><p><code>autograd</code> 和 <code>Function</code> 的官方文档 <a href="https://pytorch.org/docs/autograd">https://pytorch.org/docs/autograd</a></p></blockquote><p>model.train():<br>在使用pytorch构建神经网络的时候，训练过程中会在程序上方添加一句model.train()，作用是启用batch normalization和drop out。</p><p>model.eval():<br>测试过程中会使用model.eval()，这时神经网络会沿用batch normalization的值，并不使用drop out。</p>]]></content>
    
    
    <summary type="html">基本概念</summary>
    
    
    
    <category term="PyTorch" scheme="https://nephrencake.gitee.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>Typora-完结目录</title>
    <link href="https://nephrencake.gitee.io/2021/09/Typora-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
    <id>https://nephrencake.gitee.io/2021/09/Typora-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</id>
    <published>2021-09-01T14:55:52.000Z</published>
    <updated>2021-10-26T15:14:40.256Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Typora——完结目录"><a href="#Typora——完结目录" class="headerlink" title="Typora——完结目录"></a>Typora——完结目录</h1><p>推荐资源：</p><ul><li>Typora 使用手册：<a href="https://support.typora.io/">https://support.typora.io/</a></li></ul><p>写了大半年的Markdown，总结并重新学习。</p><table><thead><tr><th align="center"><strong><a href="/2021/09/Typora-Part1/">Typora-Part1——Markdown、Hexo与Typora</a></strong></th></tr></thead><tbody><tr><td align="center"><strong><a href="/2021/09/Typora-Part2/">Typora-Part2——技术文档写作规范</a></strong></td></tr></tbody></table>]]></content>
    
    
    <summary type="html">完结目录</summary>
    
    
    
    <category term="Typora" scheme="https://nephrencake.gitee.io/categories/Typora/"/>
    
    
    <category term="MarkDown" scheme="https://nephrencake.gitee.io/tags/MarkDown/"/>
    
  </entry>
  
  <entry>
    <title>Typora-Part2</title>
    <link href="https://nephrencake.gitee.io/2021/09/Typora-Part2/"/>
    <id>https://nephrencake.gitee.io/2021/09/Typora-Part2/</id>
    <published>2021-09-01T14:44:55.000Z</published>
    <updated>2021-10-26T15:14:55.531Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Typora-Part2——技术文档写作规范"><a href="#Typora-Part2——技术文档写作规范" class="headerlink" title="Typora-Part2——技术文档写作规范"></a>Typora-Part2——技术文档写作规范</h1><p>[TOC]</p><blockquote><p>本文来自： <a href="https://github.com/ruanyf">ruanyf</a>/<a href="https://github.com/ruanyf/document-style-guide">document-style-guide</a></p></blockquote><h2 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h2><h3 id="1-1层级"><a href="#1-1层级" class="headerlink" title="1.1层级"></a>1.1层级</h3><p>标题分为四级。</p><ul><li>一级标题：文章的标题</li><li>二级标题：文章主要部分的大标题</li><li>三级标题：二级标题下面一级的小标题</li><li>四级标题：三级标题下面某一方面的小标题</li></ul><p>下面是示例。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 一级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">## 二级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 三级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">#### 四级标题</span></span><br></pre></td></tr></table></figure><h3 id="1-2原则"><a href="#1-2原则" class="headerlink" title="1.2原则"></a>1.2原则</h3><p><strong>（1）一级标题下，不能直接出现三级标题。</strong></p><p>示例：下面的文章结构，缺少二级标题。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 一级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 三级标题</span></span><br></pre></td></tr></table></figure><p><strong>（2）标题要避免孤立编号（即同级标题只有一个）。</strong></p><p>示例：下面的文章结构，<code>二级标题 A</code>只包含一个三级标题，完全可以省略<code>三级标题 A</code>。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">## 二级标题 A</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 三级标题 A</span></span><br><span class="line"></span><br><span class="line"><span class="section">## 二级标题 B</span></span><br></pre></td></tr></table></figure><p><strong>（3）下级标题不重复上一级标题的名字。</strong></p><p>示例：下面的文章结构，二级标题与下属的三级标题同名，建议避免。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">## 概述</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 概述</span></span><br></pre></td></tr></table></figure><p><strong>（4）谨慎使用四级标题，尽量避免出现，保持层级的简单，防止出现过于复杂的章节。</strong></p><p>如果三级标题下有并列性的内容，建议只使用项目列表（Item list）。</p><p>示例：下面的结构二要好于结构一。后者适用的场景，主要是较长篇幅的内容。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">结构一</span><br><span class="line"></span><br><span class="line"><span class="section">### 三级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">#### 四级标题 A</span></span><br><span class="line"></span><br><span class="line"><span class="section">#### 四级标题 B</span></span><br><span class="line"></span><br><span class="line"><span class="section">#### 四级标题 C</span></span><br><span class="line"></span><br><span class="line">结构二</span><br><span class="line"></span><br><span class="line"><span class="section">### 三级标题</span></span><br><span class="line"></span><br><span class="line"><span class="strong">**（1）A**</span></span><br><span class="line"></span><br><span class="line"><span class="strong">**（2）B**</span></span><br><span class="line"></span><br><span class="line"><span class="strong">**（3）C**</span></span><br></pre></td></tr></table></figure><h2 id="2-文本"><a href="#2-文本" class="headerlink" title="2.文本"></a>2.文本</h2><h3 id="2-1字间距"><a href="#2-1字间距" class="headerlink" title="2.1字间距"></a>2.1字间距</h3><p>全角中文字符与半角英文字符之间，应有一个半角空格。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：本文介绍如何快速启动Windows系统。</span><br><span class="line"></span><br><span class="line">正确：本文介绍如何快速启动 Windows 系统。</span><br></pre></td></tr></table></figure><p>全角中文字符与半角阿拉伯数字之间，有没有半角空格都可，但必须保证风格统一，不能两种风格混杂。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">正确：2011年5月15日，我订购了5台笔记本电脑与10台平板电脑。</span><br><span class="line"></span><br><span class="line">正确：2011 年 5 月 15 日，我订购了 5 台笔记本电脑与 10 台平板电脑。</span><br></pre></td></tr></table></figure><p>半角的百分号，视同阿拉伯数字。</p><p>英文单位若不翻译，单位前的阿拉伯数字与单位间不留空格。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：一部容量为 16 GB 的智能手机</span><br><span class="line"></span><br><span class="line">正确：一部容量为 16GB 的智能手机</span><br></pre></td></tr></table></figure><p>半角英文字符和半角阿拉伯数字，与全角标点符号之间不留空格。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：他的电脑是 MacBook Air 。</span><br><span class="line"></span><br><span class="line">正确：他的电脑是 MacBook Air。</span><br></pre></td></tr></table></figure><h3 id="2-2-句子"><a href="#2-2-句子" class="headerlink" title="2.2 句子"></a>2.2 句子</h3><ul><li>避免使用长句。句子内部不使用逗号时，总长度不应该超过 40 个字；使用逗号时，总长度不应该超过 100 字或者正文的 3 行。</li><li>尽量使用简单句和并列句，避免使用复合句。</li></ul><h3 id="2-3写作风格"><a href="#2-3写作风格" class="headerlink" title="2.3写作风格"></a>2.3写作风格</h3><p><strong>（1）尽量不使用被动语态，改为使用主动语态。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：假如此软件尚未被安装，</span><br><span class="line"></span><br><span class="line">正确：假如尚未安装这个软件，</span><br></pre></td></tr></table></figure><p><strong>（2）不使用非正式的语言风格。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：Lady Gaga 的演唱会真是酷毙了，从没看过这么给力的表演！！！</span><br><span class="line"></span><br><span class="line">正确：无法参加本次活动，我深感遗憾。</span><br></pre></td></tr></table></figure><p><strong>（3）不使用冷僻、生造或者文言文的词语，而要使用现代汉语的常用表达方式。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：这是唯二的快速启动的方法。</span><br><span class="line"></span><br><span class="line">正确：这是仅有的两种快速启动的方法。</span><br></pre></td></tr></table></figure><p><strong>（4）用对“的”、“地”、“得”。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">她露出了开心的笑容。</span><br><span class="line">（形容词＋的＋名词）</span><br><span class="line"></span><br><span class="line">她开心地笑了。</span><br><span class="line">（副词＋地＋动词）</span><br><span class="line"></span><br><span class="line">她笑得很开心。</span><br><span class="line">（动词＋得＋副词）</span><br></pre></td></tr></table></figure><p><strong>（5）使用代词时（比如“其”、“该”、“此”、“这”等词），必须明确指代的内容，保证只有一个含义。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：从管理系统可以监视中继系统和受其直接控制的分配系统。</span><br><span class="line"></span><br><span class="line">正确：从管理系统可以监视两个系统：中继系统和受中继系统直接控制的分配系统。</span><br></pre></td></tr></table></figure><p><strong>（6）名词前不要使用过多的形容词。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：此设备的使用必须在接受过本公司举办的正式的设备培训的技师的指导下进行。</span><br><span class="line"></span><br><span class="line">正确：此设备必须在技师的指导下使用，且指导技师必须接受过由本公司举办的正式设备培训。</span><br></pre></td></tr></table></figure><p><strong>（7）不包含任何标点符号的单个句子，或者以逗号分隔的句子构件，长度尽量保持在 20 个字以内；20～29 个字的句子，可以接受；30～39 个字的句子，语义必须明确，才能接受；多于 40 个字的句子，在任何情况下都不能接受。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：本产品适用于从由一台服务器进行动作控制的单一节点结构到由多台服务器进行动作控制的并行处理程序结构等多种体系结构。</span><br><span class="line"></span><br><span class="line">正确：本产品适用于多种体系结构。无论是由一台服务器（单一节点结构），还是由多台服务器（并行处理结构）进行动作控制，均可以使用本产品。</span><br></pre></td></tr></table></figure><p><strong>（8）同样一个意思，尽量使用肯定句表达，不使用否定句表达。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：请确认没有接通装置的电源。</span><br><span class="line"></span><br><span class="line">正确：请确认装置的电源已关闭。</span><br></pre></td></tr></table></figure><p><strong>（9）避免使用双重否定句。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：没有删除权限的用户，不能删除此文件。</span><br><span class="line"></span><br><span class="line">正确：用户必须拥有删除权限，才能删除此文件。</span><br></pre></td></tr></table></figure><h3 id="2-4英文处理"><a href="#2-4英文处理" class="headerlink" title="2.4英文处理"></a>2.4英文处理</h3><p><strong>（1）英文原文如果使用了复数形式，翻译成中文时，应该将其还原为单数形式。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">英文：⋯information stored in random access memory (RAMs)⋯</span><br><span class="line"></span><br><span class="line">中文：……存储在随机存取存储器（RAM）里的信息……</span><br></pre></td></tr></table></figure><p><strong>（2）外文缩写可以使用半角圆点(<code>.</code>)表示缩写。</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">U</span><span class="selector-class">.S</span><span class="selector-class">.A</span>.</span><br><span class="line"><span class="selector-tag">Apple</span>, <span class="selector-tag">Inc</span>.</span><br></pre></td></tr></table></figure><p><strong>（3）表示中文时，英文省略号（<code>⋯</code>）应改为中文省略号（<code>……</code>）。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">英文：5 minutes later⋯</span><br><span class="line"></span><br><span class="line">中文：5 分钟过去了⋯⋯</span><br></pre></td></tr></table></figure><p><strong>（4）英文书名或电影名改用中文表达时，双引号应改为书名号。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">英文：He published an article entitled <span class="string">&quot;The Future of the Aviation&quot;</span>.</span><br><span class="line"></span><br><span class="line">中文：他发表了一篇名为《航空业的未来》的文章。</span><br></pre></td></tr></table></figure><p><strong>（5）第一次出现英文词汇时，在括号中给出中文标注。此后再次出现时，直接使用英文缩写即可。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IOC（International Olympic Committee，国际奥林匹克委员会）。这样定义后，便可以直接使用“IOC”了。</span><br></pre></td></tr></table></figure><p><strong>（6）专有名词中每个词第一个字母均应大写，非专有名词则不需要大写。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">“American Association of Physicists in Medicine”（美国医学物理学家协会）是专有名词，需要大写。</span><br><span class="line"></span><br><span class="line">“online transaction processing”（在线事务处理）不是专有名词，不应大写。</span><br></pre></td></tr></table></figure><h2 id="3-段落"><a href="#3-段落" class="headerlink" title="3.段落"></a>3.段落</h2><h3 id="3-1原则"><a href="#3-1原则" class="headerlink" title="3.1原则"></a>3.1原则</h3><ul><li>一个段落只能有一个主题，或一个中心句子。</li><li>段落的中心句子放在段首，对全段内容进行概述。后面陈述的句子为核心句服务。</li><li>一个段落的长度不能超过七行，最佳段落长度小于等于四行。</li><li>段落的句子语气要使用陈述和肯定语气，避免使用感叹语气。</li><li>段落之间使用一个空行隔开。</li><li>段落开头不要留出空白字符。</li></ul><h3 id="3-2引用"><a href="#3-2引用" class="headerlink" title="3.2引用"></a>3.2引用</h3><p><strong>（1）引用第三方内容时，应注明出处。</strong></p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">One man’s constant <span class="keyword">is</span> another man’s variable. — Alan Perlis</span><br></pre></td></tr></table></figure><p><strong>（2）如果是全篇转载，请在全文开头显著位置注明作者和出处，并链接至原文。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">本文转载自 WikiQuote</span><br></pre></td></tr></table></figure><p><strong>（3）使用外部图片时，必须在图片下方或文末标明来源。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">本文部分图片来自 Wikipedia</span><br></pre></td></tr></table></figure><h2 id="4-数值"><a href="#4-数值" class="headerlink" title="4.数值"></a>4.数值</h2><h3 id="4-1半角数字"><a href="#4-1半角数字" class="headerlink" title="4.1半角数字"></a>4.1半角数字</h3><p>数字一律使用半角形式，不得使用全角形式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误： 这件商品的价格是１０００元。</span><br><span class="line"></span><br><span class="line">正确： 这件商品的价格是 1000 元。</span><br></pre></td></tr></table></figure><h3 id="4-2千分号"><a href="#4-2千分号" class="headerlink" title="4.2千分号"></a>4.2千分号</h3><p>数值为千位以上，应添加千分号（半角逗号）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">XXX 公司的实收资本为 RMB1,258,000。</span><br></pre></td></tr></table></figure><p>对于 4 ～ 6 位的数值，千分号是选用的，比如<code>1000</code>和<code>1,000</code>都可以接受。对于7位及以上的数值，千分号是必须的。</p><p>多位小数要从小数点后从左向右添加千分号，比如<code>4.234,345</code>。</p><h3 id="4-3货币"><a href="#4-3货币" class="headerlink" title="4.3货币"></a>4.3货币</h3><p>货币应为阿拉伯数字，并在数字前写出货币符号，或在数字后写出货币中文名称。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$1</span>,000</span><br><span class="line">1,000 美元</span><br></pre></td></tr></table></figure><h3 id="4-4数值范围"><a href="#4-4数值范围" class="headerlink" title="4.4数值范围"></a>4.4数值范围</h3><p>表示数值范围时，用<code>～</code>连接。参见《标点符号》一节的“连接号”部分。</p><p>带有单位或百分号时，两个数字都要加上单位或百分号，不能只加后面一个。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">错误：132～234kg</span><br><span class="line">正确：132kg～234kg</span><br><span class="line"></span><br><span class="line">错误：67～89%</span><br><span class="line">正确：67%～89%</span><br></pre></td></tr></table></figure><h3 id="4-5变化程度的表示法"><a href="#4-5变化程度的表示法" class="headerlink" title="4.5变化程度的表示法"></a>4.5变化程度的表示法</h3><p>数字的增加要使用“增加了”、“增加到”。“了”表示增量，“到”表示定量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">增加到过去的两倍</span><br><span class="line">（过去为一，现在为二）</span><br><span class="line"></span><br><span class="line">增加了两倍</span><br><span class="line">（过去为一，现在为三）</span><br></pre></td></tr></table></figure><p>数字的减少要使用“降低了”、“降低到”。“了”表示增量，“到”表示定量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">降低到百分之八十</span><br><span class="line">（定额是一百，现在是八十）</span><br><span class="line"></span><br><span class="line">降低了百分之八十</span><br><span class="line">（原来是一百，现在是二十）</span><br></pre></td></tr></table></figure><p>不能用“降低N倍”或“减少N倍”的表示法，要用“降低百分之几”或“减少百分之几”。因为减少（或降低）一倍表示数值原来为一百，现在等于零。</p><h2 id="5-标点符号"><a href="#5-标点符号" class="headerlink" title="5.标点符号"></a>5.标点符号</h2><h3 id="5-1原则"><a href="#5-1原则" class="headerlink" title="5.1原则"></a>5.1原则</h3><ul><li>中文语句的标点符号，均应该采取全角符号，这样可以保证视觉的一致。</li><li>如果整句为英文，则该句使用英文/半角标点。</li><li>句号、问号、叹号、逗号、顿号、分号和冒号不得出现在一行之首。</li></ul><h3 id="5-2句号"><a href="#5-2句号" class="headerlink" title="5.2句号"></a>5.2句号</h3><p>中文语句中的结尾处应该用全角句号（<code>。</code>）。</p><p>句子末尾用括号加注时，句号应在括号之外。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：关于文件的输出，请参照第 1.3 节（见第 26 页。）</span><br><span class="line"></span><br><span class="line">正确：关于文件的输出，请参照第 1.3 节（见第 26 页）。</span><br></pre></td></tr></table></figure><h3 id="5-3逗号"><a href="#5-3逗号" class="headerlink" title="5.3逗号"></a>5.3逗号</h3><p>逗号<code>，</code>表示句子内部的一般性停顿。</p><p>注意避免“一逗到底”，即整个段落除了结尾，全部停顿都使用逗号。</p><h3 id="5-4顿号"><a href="#5-4顿号" class="headerlink" title="5.4顿号"></a>5.4顿号</h3><p>句子内部的并列词，应该用全角顿号(<code>、</code>) 分隔，而不用逗号，即使并列词是英语也是如此。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：我最欣赏的科技公司有 Google, Facebook, 腾讯, 阿里和百度等。</span><br><span class="line"></span><br><span class="line">正确：我最欣赏的科技公司有 Google、Facebook、腾讯、阿里和百度等。</span><br></pre></td></tr></table></figure><p>英文句子中，并列词语之间使用半角逗号（<code>,</code>）分隔。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例句：Microsoft Office includes Word, Excel, PowerPoint, Outlook and other components.</span><br></pre></td></tr></table></figure><h3 id="5-5分号"><a href="#5-5分号" class="headerlink" title="5.5分号"></a>5.5分号</h3><p>分号<code>；</code>表示复句内部并列分句之间的停顿。</p><h3 id="5-6引号"><a href="#5-6引号" class="headerlink" title="5.6引号"></a>5.6引号</h3><p>引用时，应该使用全角双引号（<code>“ ”</code>），注意前后双引号不同。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例句：许多人都认为客户服务的核心是“友好”和“专业”。</span><br></pre></td></tr></table></figure><p>引号里面还要用引号时，外面一层用双引号，里面一层用单引号（<code>‘ ’</code>），注意前后单引号不同。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例句：鲍勃解释道：“我要放音乐，可萨利说，‘不行！’。”</span><br></pre></td></tr></table></figure><h3 id="5-7圆括号"><a href="#5-7圆括号" class="headerlink" title="5.7圆括号"></a>5.7圆括号</h3><p>补充说明时，使用全角圆括号<code>（）</code>，括号前后不加空格。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例句：请确认所有的连接（电缆和接插件）均安装牢固。</span><br></pre></td></tr></table></figure><h3 id="5-8冒号"><a href="#5-8冒号" class="headerlink" title="5.8冒号"></a>5.8冒号</h3><p>全角冒号（<code>：</code>）常用在需要解释的词语后边，引出解释和说明。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例句：请确认以下几项内容：时间、地点、活动名称，以及来宾数量。</span><br></pre></td></tr></table></figure><p>表示时间时，应使用半角冒号（<code>:</code>）。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例句：早上 8<span class="selector-pseudo">:00</span></span><br></pre></td></tr></table></figure><h3 id="5-9省略号"><a href="#5-9省略号" class="headerlink" title="5.9省略号"></a>5.9省略号</h3><p>省略号<code>……</code>表示语句未完、或者语气的不连续。它占两个汉字空间、包含六个省略点，不要使用<code>。。。</code>或<code>...</code>等非标准形式。</p><p>省略号不应与“等”这个词一起使用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">错误：我们为会餐准备了香蕉、苹果、梨…等各色水果。</span><br><span class="line"></span><br><span class="line">正确：我们为会餐准备了各色水果，有香蕉、苹果、梨……</span><br><span class="line"></span><br><span class="line">正确：我们为会餐准备了香蕉、苹果、梨等各色水果。</span><br></pre></td></tr></table></figure><h3 id="5-10感叹号"><a href="#5-10感叹号" class="headerlink" title="5.10感叹号"></a>5.10感叹号</h3><p>应该使用平静的语气叙述，尽量避免使用感叹号<code>！</code>。</p><p>不得多个感叹号连用，比如<code>！！</code>和<code>!!!</code>。</p><h3 id="5-11破折号"><a href="#5-11破折号" class="headerlink" title="5.11破折号"></a>5.11破折号</h3><p>破折号<code>————</code>一般用于进一步解释。</p><p>破折号应占两个汉字的位置。如果破折号本身只占一个汉字的位置，那么前后应该留出一个半角空格。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">例句：直觉————尽管它并不总是可靠的————告诉我，这事可能出了些问题。</span><br><span class="line"></span><br><span class="line">例句：直觉 —— 尽管它并不总是可靠的 —— 告诉我，这事可能出了些问题。</span><br></pre></td></tr></table></figure><h3 id="5-12连接号"><a href="#5-12连接号" class="headerlink" title="5.12连接号"></a>5.12连接号</h3><p>连接号用于连接两个类似的词。</p><p>以下场合应该使用直线连接号（<code>-</code>），占一个半角字符的位置。</p><ul><li>两个名词的复合</li><li>图表编号</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">例句：氧化-还原反应</span><br><span class="line"></span><br><span class="line">例句：图 1-1</span><br></pre></td></tr></table></figure><p>以下场合应该使用波浪连接号（<code>～</code>），占一个全角字符的位置。</p><ul><li>数值范围（例如日期、时间或数字）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例句：2009 年～2011 年</span><br></pre></td></tr></table></figure><p>注意，波浪连接号前后两个值都应该加上单位。</p><p>波浪连接号也可以用汉字“至”代替。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例句：周围温度：-20°C 至 -10°C</span><br></pre></td></tr></table></figure><h2 id="6-文档体系"><a href="#6-文档体系" class="headerlink" title="6.文档体系"></a>6.文档体系</h2><h3 id="6-1结构"><a href="#6-1结构" class="headerlink" title="6.1结构"></a>6.1结构</h3><p>软件手册是一部完整的书，建议采用下面的结构。</p><ul><li><p><strong>简介</strong>（Introduction）： [必备] [文件] 提供对产品和文档本身的总体的、扼要的说明</p></li><li><p><strong>快速上手</strong>（Getting Started）：[可选] [文件] 如何最快速地使用产品</p></li><li><p>入门篇</p><p>（Basics）： [必备] [目录] 又称”使用篇“，提供初级的使用教程</p><ul><li><strong>环境准备</strong>（Prerequisite）：[必备] [文件] 软件使用需要满足的前置条件</li><li><strong>安装</strong>（Installation）：[可选] [文件] 软件的安装方法</li><li><strong>设置</strong>（Configuration）：[必备] [文件] 软件的设置</li></ul></li><li><p><strong>进阶篇</strong>（Advanced)：[可选] [目录] 又称”开发篇“，提供中高级的开发教程</p></li><li><p><strong>API</strong>（Reference）：[可选] [目录|文件] 软件 API 的逐一介绍</p></li><li><p><strong>FAQ</strong>：[可选] [文件] 常见问题解答</p></li><li><p>附录</p><p>（Appendix）：[可选] [目录] 不属于教程本身、但对阅读教程有帮助的内容</p><ul><li><strong>Glossary</strong>：[可选] [文件] 名词解释</li><li><strong>Recipes</strong>：[可选] [文件] 最佳实践</li><li><strong>Troubleshooting</strong>：[可选] [文件] 故障处理</li><li><strong>ChangeLog</strong>：[可选] [文件] 版本说明</li><li><strong>Feedback</strong>：[可选] [文件] 反馈方式</li></ul></li></ul><p>下面是两个真实范例，可参考。</p><ul><li><a href="http://redux.js.org/index.html">Redux 手册</a></li><li><a href="http://flight-manual.atom.io/">Atom 手册</a></li></ul><h3 id="6-2文件名"><a href="#6-2文件名" class="headerlink" title="6.2文件名"></a>6.2文件名</h3><p>文档的文件名不得含有空格。</p><p>文件名必须使用半角字符，不得使用全角字符。这也意味着，中文不能用于文件名。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误： 名词解释<span class="selector-class">.md</span></span><br><span class="line"></span><br><span class="line">正确： <span class="selector-tag">glossary</span><span class="selector-class">.md</span></span><br></pre></td></tr></table></figure><p>文件名建议只使用小写字母，不使用大写字母。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：<span class="selector-tag">TroubleShooting</span><span class="selector-class">.md</span></span><br><span class="line"></span><br><span class="line">正确：<span class="selector-tag">troubleshooting</span><span class="selector-class">.md</span> </span><br></pre></td></tr></table></figure><p>为了醒目，某些说明文件的文件名，可以使用大写字母，比如<code>README</code>、<code>LICENSE</code>。</p><p>文件名包含多个单词时，单词之间建议使用半角的连词线（<code>-</code>）分隔。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">不佳：<span class="selector-tag">advanced_usage</span><span class="selector-class">.md</span></span><br><span class="line"></span><br><span class="line">正确：<span class="selector-tag">advanced-usage</span><span class="selector-class">.md</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">技术文档写作规范</summary>
    
    
    
    <category term="Typora" scheme="https://nephrencake.gitee.io/categories/Typora/"/>
    
    
  </entry>
  
  <entry>
    <title>Typora-Part1</title>
    <link href="https://nephrencake.gitee.io/2021/09/Typora-Part1/"/>
    <id>https://nephrencake.gitee.io/2021/09/Typora-Part1/</id>
    <published>2021-09-01T06:40:12.000Z</published>
    <updated>2021-10-26T15:15:00.869Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Typora-Part1——Markdown、Hexo与Typora"><a href="#Typora-Part1——Markdown、Hexo与Typora" class="headerlink" title="Typora-Part1——Markdown、Hexo与Typora"></a>Typora-Part1——Markdown、Hexo与Typora</h1><p>[TOC]</p><h2 id="Markdown"><a href="#Markdown" class="headerlink" title="Markdown"></a>Markdown</h2><ul><li>Markdown 是一种轻量级标记语言，通过简单的标记语法，使普通文本内容具有一定的格式。</li><li>Markdown 编写的文档可以导出 HTML 、Word、图像、PDF、Epub 等多种格式的文件。</li><li>Markdown 编写的文档后缀为 .md、.markdown。</li><li>Markdown 具有一系列衍生版本，用于扩展 Markdown 的功能（如表格、脚注、内嵌HTML等）。这些功能原初的Markdown尚不具备，它们能让Markdown转换成更多的格式，例如LaTeX，Docbook。</li><li>Markdown 增强版中比较有名的有 Markdown Extra、MultiMarkdown、Maruku 等。这些衍生版本要么基于工具，如Pandoc；要么基于网站，如 GitHub 和 Wikipedia，在语法上基本兼容，但在一些语法和渲染效果上有改动。</li></ul><h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><ul><li>Hexo 是一款快速、简洁且高效的博客框架。</li><li>Hexo 支持 GitHub Flavored Markdown 的所有功能，可以快速生成模板文档，编辑完成后可以快速部署到 GitHub Pages 等平台。</li><li>Typora 支持在文档头部加上基于 YAML 的 front-matter 信息，这一特性适用于把 Markdown 文档分类归档上传到用 Hexo 框架搭建的博客中。</li><li>Hexo 和 Typora 两者可以高效结合，非常好用。</li></ul><h2 id="Typora"><a href="#Typora" class="headerlink" title="Typora"></a>Typora</h2><blockquote><p>Typora 官网：<a href="https://typora.io/">https://typora.io/</a> </p></blockquote><ul><li>Typora 是一款 Markdown 编辑器，支持 MacOS 、Windows、Linux 平台。<ul><li>包含多种主题，并且可以自己通过css样式定义风格。</li><li>编辑文本的同时直接渲染出效果——所见即所得。</li><li>支持导出HTML、PDF、Word、图片等多种类型文件。</li></ul></li></ul><h2 id="在-Typora-中编辑-Markdown-文本"><a href="#在-Typora-中编辑-Markdown-文本" class="headerlink" title="在 Typora 中编辑 Markdown 文本"></a>在 Typora 中编辑 Markdown 文本</h2><blockquote><p>可以使用反斜杠 <code>\</code> 来防止转义。</p></blockquote><h3 id="基本功能"><a href="#基本功能" class="headerlink" title="基本功能"></a>基本功能</h3><h4 id="标题段落"><a href="#标题段落" class="headerlink" title="标题段落"></a>标题段落</h4><ul><li><p>段落只是一行或多行连续的文本。在markdown源代码中，段落由两个或多个空行分隔。在Typora中，只需要一个空行（按Enter一次）即可创建一个新段落。</p></li><li><p>按Shift+Enter可创建单个换行符。大多数其他 markdown 解析器将忽略单换行符，因此为了使其他降价解析器识别换行符，可以在行的末尾留下两个空格，或者插入<code>&lt;br/&gt;</code>。</p></li></ul><table><thead><tr><th>功能</th><th>快捷键</th><th>Markdown</th><th>HTML</th></tr></thead><tbody><tr><td>段落</td><td>Ctrl+0</td><td></td><td>&lt;p&gt; &lt;/p&gt;</td></tr><tr><td>n级标题</td><td>Ctrl+n</td><td>#*n + space*2</td><td>&lt;h1&gt; &lt;/h1&gt;</td></tr><tr><td>提升标题级别</td><td>Ctrl+‘+’</td><td></td><td></td></tr><tr><td>降低标题级别</td><td>Ctrl+‘-’</td><td></td><td></td></tr></tbody></table><h4 id="字体操作"><a href="#字体操作" class="headerlink" title="字体操作"></a>字体操作</h4><table><thead><tr><th>功能</th><th>快捷键</th><th>Markdown</th><th>HTML</th><th>备注</th></tr></thead><tbody><tr><td>加粗</td><td>Ctrl+B</td><td>**加粗** 或 __加粗__</td><td>&lt;B&gt;加粗&lt;/B&gt;</td><td></td></tr><tr><td>下划线</td><td>Ctrl+U</td><td></td><td>&lt;u&gt;下划线&lt;/u&gt;</td><td></td></tr><tr><td>倾斜</td><td>Ctrl+I</td><td>*倾斜* 或 _倾斜_</td><td>&lt;I&gt;倾斜&lt;/I&gt;</td><td></td></tr><tr><td>删除线</td><td>Alt+Shift+5</td><td>~~删除线~~</td><td></td><td></td></tr><tr><td>清除样式</td><td>Ctrl+\</td><td></td><td></td><td>这个好用，清干净</td></tr><tr><td>下标</td><td></td><td>H~2~O</td><td>&lt;sub&gt;下标&lt;/sub&gt;</td><td>需要在设置中启动</td></tr><tr><td>上标</td><td></td><td>2^10^</td><td>&lt;sup&gt;上标&lt;/sup&gt;</td><td>需要在设置中启动</td></tr><tr><td>高亮</td><td></td><td>==高亮==</td><td></td><td>需要在设置中启动</td></tr></tbody></table><h4 id="插入功能"><a href="#插入功能" class="headerlink" title="插入功能"></a>插入功能</h4><h5 id="插入快捷键"><a href="#插入快捷键" class="headerlink" title="插入快捷键"></a>插入快捷键</h5><table><thead><tr><th>功能</th><th>快捷键</th><th>Markdown</th><th>HTML</th><th>备注</th></tr></thead><tbody><tr><td>插入图片</td><td>Ctrl+Shift+I</td><td>![alt 属性文本](图片地址 “可选标题”)</td><td>&lt;img src=””/&gt;</td><td>建议直接复制粘贴图片。</td></tr><tr><td>插入表格</td><td>Ctrl+T</td><td></td><td></td><td></td></tr><tr><td>插入有序列表</td><td>Ctrl+Shift+[</td><td>num+ . +space</td><td>&lt;ol&gt;&lt;li&gt;*n&lt;/ol&gt;</td><td></td></tr><tr><td>插入无序列表</td><td>Ctrl+Shift+]</td><td>‘-/+/*’+space</td><td>&lt;ul&gt;&lt;li&gt;*n&lt;/ul&gt;</td><td>选定行按TAB以嵌套</td></tr><tr><td>插入超链接</td><td>Ctrl+K</td><td>[链接名称](链接地址) 或 &lt;链接地址&gt;</td><td>&lt;a href=””&gt; &lt;/a&gt;</td><td></td></tr><tr><td>插入代码片</td><td>Ctrl+Shift+`</td><td>``</td><td></td><td>开启匹配Markdown字符</td></tr><tr><td>插入代码块</td><td>Ctrl+Shift+K</td><td>```+lang</td><td></td><td></td></tr><tr><td>插入公式块</td><td>Ctrl+Shift+M</td><td>$$</td><td></td><td></td></tr><tr><td>插入引用块</td><td>Ctrl+Shift+Q</td><td>&gt;+space</td><td></td><td>可以嵌套</td></tr><tr><td>插入目录</td><td></td><td>[TOC]+enter</td><td></td><td></td></tr><tr><td>插入注释</td><td></td><td></td><td>&lt;!– comments –&gt;</td><td>html注释</td></tr><tr><td>插入分割线</td><td></td><td>— 或 ***</td><td></td><td></td></tr><tr><td>任务列表</td><td></td><td>-space[space]space</td><td></td><td>[]里换成x就是打上勾</td></tr></tbody></table><h5 id="插入图片说明"><a href="#插入图片说明" class="headerlink" title="插入图片说明"></a>插入图片说明</h5><ul><li><p>只有html标签可以指定图片属性。</p></li><li><p>建议将图片保存至当前目录的同名文件夹下，这样结构清晰。</p><p><img src="/2021/09/Typora-Part1/image-20210901220801184.png" alt="image-20210901220801184"></p></li></ul><h5 id="链接详解"><a href="#链接详解" class="headerlink" title="链接详解"></a>链接详解</h5><ul><li>Ctrl + 左键，跳转指定url</li></ul><ol><li>内联链接：<ul><li>[浅幽丶奈芙莲](<a href="http://nephrencake.gitee.io/">http://nephrencake.gitee.io/</a>)</li></ul></li><li>引用链接：<ul><li>[浅幽丶奈芙莲][变量名]</li><li>[变量名]: <a href="http://nephrencake.gitee.io/">http://nephrencake.gitee.io/</a></li></ul></li><li>直接使用地址：<ul><li>&lt;<a href="http://nephrencake.gitee.io/&gt;">http://nephrencake.gitee.io/&gt;</a></li></ul></li></ol><ul><li><p>HTML页内跳转：</p><p name="top">这里是页头</p><p><a href="#top">回页头</a></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">name</span>=<span class="string">&quot;top&quot;</span>&gt;</span>这里是页头<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#top&quot;</span>&gt;</span>回页头<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br></pre></td></tr></table></figure><h5 id="脚注详解"><a href="#脚注详解" class="headerlink" title="脚注详解"></a>脚注详解</h5></li><li><p>脚注语法：</p><ul><li>文本[^说明文字]</li><li>[^说明文字]: 解释说明</li></ul></li><li><p>示例：使用 Markdown<a href="Markdown%E6%98%AF%E4%B8%80%E7%A7%8D%E7%BA%AF%E6%96%87%E6%9C%AC%E6%A0%87%E8%AE%B0%E8%AF%AD%E8%A8%80">^1</a>可以效率的书写文档, 直接转换成 HTML[^2], 你可以使用 Typora[^T] 编辑器进行书写。</p></li></ul><p>[^2]:HyperText Markup Language 超文本标记语言<br>[^T]:NEW WAY TO READ &amp; WRITE MARKDOWN.扩展操作</p><h3 id="扩展操作"><a href="#扩展操作" class="headerlink" title="扩展操作"></a>扩展操作</h3><h4 id="表格扩展"><a href="#表格扩展" class="headerlink" title="表格扩展"></a>表格扩展</h4><h5 id="表格快捷键"><a href="#表格快捷键" class="headerlink" title="表格快捷键"></a>表格快捷键</h5><ul><li>在表格中，可以使用鼠标拖动行或者列，达到交换行和列。</li></ul><table><thead><tr><th>功能</th><th>快捷键</th><th>备注</th></tr></thead><tbody><tr><td>下方插入行</td><td>Ctrl+Enter</td><td></td></tr><tr><td>上移该行</td><td>Alt+↑</td><td></td></tr><tr><td>下移该行</td><td>Alt+↓</td><td></td></tr><tr><td>左移该列</td><td>Win+←</td><td>左右移动表格列的快捷键与WinDows系统自带的快捷键冲突</td></tr><tr><td>右移该列</td><td>Win+→</td><td></td></tr><tr><td>删除该行</td><td>Ctrl+Shift+BackSpace</td><td></td></tr></tbody></table><h5 id="表格详解"><a href="#表格详解" class="headerlink" title="表格详解"></a>表格详解</h5><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 1. 表格的Markdown语法</span><br><span class="line">| 表头   | 表头   | 表头   |</span><br><span class="line">| ------ | ------ | ------ |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br><span class="line">| 单元格 | 单元格 | 单元格 |</span><br><span class="line">    </span><br><span class="line"># 2. 可以设置表格的对齐方式：</span><br><span class="line"># - -: 设置内容和标题栏居右对齐。</span><br><span class="line"># - :- 设置内容和标题栏居左对齐。</span><br><span class="line"># - :-: 设置内容和标题栏居中对齐。</span><br><span class="line">| 左对齐 | 右对齐 | 居中对齐 |</span><br><span class="line">| :----- | -----: | :------: |</span><br><span class="line">| 单元格 | 单元格 |  单元格  |</span><br><span class="line">| 单元格 | 单元格 |  单元格  |</span><br><span class="line">    </span><br><span class="line"># HTML语法</span><br><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">caption</span>&gt;</span>标题内容<span class="tag">&lt;/<span class="name">caption</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>表格内容<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>表格内容<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>表格内容<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>表格内容<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="视图操作"><a href="#视图操作" class="headerlink" title="视图操作"></a>视图操作</h4><h5 id="侧边栏"><a href="#侧边栏" class="headerlink" title="侧边栏"></a>侧边栏</h5><table><thead><tr><th>功能</th><th>快捷键</th></tr></thead><tbody><tr><td>大纲视图</td><td>Ctrl+Shift+1</td></tr><tr><td>文件列表视图</td><td>Ctrl+Shift+2</td></tr><tr><td>文件树视图</td><td>Ctrl+Shift+3</td></tr><tr><td>显示/隐藏侧边栏</td><td>Ctrl+Shift+L</td></tr><tr><td>放大视图</td><td>Ctrl+Shift+‘+’</td></tr><tr><td>缩小视图</td><td>Ctrl+Shift+‘-’</td></tr><tr><td>恢复原来大小视图</td><td>Ctrl+Shift+9</td></tr></tbody></table><h5 id="编辑模式"><a href="#编辑模式" class="headerlink" title="编辑模式"></a>编辑模式</h5><table><thead><tr><th>功能</th><th>快捷键</th><th>备注</th></tr></thead><tbody><tr><td>源代码模式</td><td>Ctrl+/</td><td></td></tr><tr><td>专注模式</td><td>F8</td><td>当前编辑行为黑，其他行为灰色</td></tr><tr><td>打字机模式</td><td>F9</td><td>光标始终在屏幕中央位置</td></tr></tbody></table><h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><table><thead><tr><th>功能</th><th>快捷键</th></tr></thead><tbody><tr><td>全屏</td><td>F11</td></tr><tr><td>应用内窗口切换</td><td>Ctrl+Tab</td></tr><tr><td>开发者工具</td><td>Shift+F12</td></tr></tbody></table><h4 id="搜索扩展"><a href="#搜索扩展" class="headerlink" title="搜索扩展"></a>搜索扩展</h4><table><thead><tr><th>功能</th><th>快捷键</th></tr></thead><tbody><tr><td>查找/搜索</td><td>Ctrl+F</td></tr><tr><td>替换</td><td>Ctrl+H</td></tr><tr><td>查找下一个</td><td>F3</td></tr><tr><td>查找上一个</td><td>Shift+F3</td></tr></tbody></table><h3 id="编辑操作"><a href="#编辑操作" class="headerlink" title="编辑操作"></a>编辑操作</h3><h4 id="选择操作"><a href="#选择操作" class="headerlink" title="选择操作"></a>选择操作</h4><table><thead><tr><th>功能</th><th>快捷键</th></tr></thead><tbody><tr><td>全选</td><td>Ctrl+A</td></tr><tr><td>选择当前行/句</td><td>Ctrl+L</td></tr><tr><td>选择相同格式文字</td><td>Ctrl+E</td></tr><tr><td>选择当前单词</td><td>Ctrl+D</td></tr><tr><td>删除当前单词</td><td>Ctrl+Shift+D</td></tr></tbody></table><h4 id="跳转操作"><a href="#跳转操作" class="headerlink" title="跳转操作"></a>跳转操作</h4><table><thead><tr><th>功能</th><th>快捷键</th></tr></thead><tbody><tr><td>跳转到文首</td><td>Ctrl+Home</td></tr><tr><td>跳转到所选内容</td><td>Ctrl+J</td></tr><tr><td>跳转到文末</td><td>Ctrl+End</td></tr></tbody></table><h4 id="粘贴复制操作"><a href="#粘贴复制操作" class="headerlink" title="粘贴复制操作"></a>粘贴复制操作</h4><table><thead><tr><th>功能</th><th>快捷键</th></tr></thead><tbody><tr><td>普通复制</td><td>Ctrl+C</td></tr><tr><td>普通粘贴</td><td>Ctrl+V</td></tr><tr><td>剪切</td><td>Ctrl+X</td></tr><tr><td>复制为MarkDown标记语法</td><td>Ctrl+Shift+C</td></tr><tr><td>粘贴为纯文本</td><td>Ctrl+Shift+V</td></tr></tbody></table><h4 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h4><table><thead><tr><th>功能</th><th>快捷键</th><th>备注</th></tr></thead><tbody><tr><td>新建</td><td>Ctrl+N</td><td></td></tr><tr><td>新建窗口</td><td>Ctrl+Shift+N</td><td>在Windows中，这两个作用一样</td></tr><tr><td>打开文件</td><td>Ctrl+O</td><td></td></tr><tr><td>快速打开</td><td>Ctrl+P</td><td>在最近打开中打开</td></tr><tr><td>保存</td><td>Ctrl+S</td><td></td></tr><tr><td>另存为</td><td>Ctrl+Shift+S</td><td></td></tr><tr><td>偏好设置</td><td>Ctrl+，</td><td>似乎跟搜狗快捷键冲突</td></tr><tr><td>关闭</td><td>Ctrl+W</td><td></td></tr><tr><td>重新打开关闭的文件</td><td>Ctrl+Shift+T</td><td></td></tr><tr><td>文件目录查找</td><td>Ctrl+Shift+F</td><td></td></tr></tbody></table><h3 id="Typora快键键修改"><a href="#Typora快键键修改" class="headerlink" title="Typora快键键修改"></a>Typora快键键修改</h3><ol><li>在Typora菜单栏中，文件–&gt;偏好设置–&gt;打开高级设置</li><li>在打开的文件夹中找到conf.user.json文件</li><li>修改KeyBeing中的键值对，重新设置快捷键</li><li>重启Typora软件即可生效</li></ol><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;keyBinding&quot;</span>: &#123;</span><br><span class="line">    <span class="comment">// for example:</span></span><br><span class="line">    <span class="comment">// &quot;Always on Top&quot;: &quot;Ctrl+Shift+P&quot;</span></span><br><span class="line">    <span class="string">&quot;Always on Top&quot;</span>: <span class="string">&quot;Ctrl+Shift+P&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Code Fences&quot;</span>: <span class="string">&quot;Ctrl+Shift+F&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Ordered List&quot;</span>: <span class="string">&quot;Ctrl+Alt+o&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Unordered List&quot;</span>: <span class="string">&quot;Ctrl+Alt+u&quot;</span></span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><h3 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h3><blockquote><p>LaTeX 语法移步：<a href="https://blog.csdn.net/happyday_d/article/details/83715440">https://blog.csdn.net/happyday_d/article/details/83715440</a></p></blockquote><ul><li>可以通过使用 MathJax 来实现 LaTeX 的数学符号的表达。</li><li>在Markdown语法中，数学的公式块是通过利用 <code>$...$</code> 以及 <code>$$...$$</code> 标记借用 LaTeX 语言来实现的：</li></ul><p>$$<br>\mathbf{V}_1 \times \mathbf{V}_2 =  \begin{vmatrix}<br>\mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \<br>\frac{\partial X}{\partial u} &amp;  \frac{\partial Y}{\partial u} &amp; 0 \<br>\frac{\partial X}{\partial v} &amp;  \frac{\partial Y}{\partial v} &amp; 0 \<br>\end{vmatrix}<br>$$</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\mathbf&#123;V&#125;<span class="emphasis">_1 \times \mathbf&#123;V&#125;_</span>2 =  \begin&#123;vmatrix&#125; </span><br><span class="line">\mathbf&#123;i&#125; &amp; \mathbf&#123;j&#125; &amp; \mathbf&#123;k&#125; \\</span><br><span class="line">\frac&#123;\partial X&#125;&#123;\partial u&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial u&#125; &amp; 0 \\</span><br><span class="line">\frac&#123;\partial X&#125;&#123;\partial v&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial v&#125; &amp; 0 \\</span><br><span class="line">\end&#123;vmatrix&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><blockquote><p>炫酷画图方法：<a href="https://support.typora.io/Draw-Diagrams-With-Markdown/">https://support.typora.io/Draw-Diagrams-With-Markdown/</a></p></blockquote><ul><li>我选择直接扔图片。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">st&#x3D;&gt;start: Start</span><br><span class="line">op&#x3D;&gt;operation: Your Operation</span><br><span class="line">cond&#x3D;&gt;condition: Yes or No?</span><br><span class="line">e&#x3D;&gt;end</span><br><span class="line"></span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">```flow</span></span><br><span class="line"><span class="code">st=&gt;start: Start</span></span><br><span class="line"><span class="code">op=&gt;operation: Your Operation</span></span><br><span class="line"><span class="code">cond=&gt;condition: Yes or No?</span></span><br><span class="line"><span class="code">e=&gt;end</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">st-&gt;op-&gt;cond</span></span><br><span class="line"><span class="code">cond(yes)-&gt;e</span></span><br><span class="line"><span class="code">cond(no)-&gt;op</span></span><br><span class="line"><span class="code">```</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Markdown、Hexo与Typora</summary>
    
    
    
    <category term="Typora" scheme="https://nephrencake.gitee.io/categories/Typora/"/>
    
    
  </entry>
  
  <entry>
    <title>OpenCV-Part5</title>
    <link href="https://nephrencake.gitee.io/2021/09/OpenCV-Part5/"/>
    <id>https://nephrencake.gitee.io/2021/09/OpenCV-Part5/</id>
    <published>2021-09-01T00:45:22.000Z</published>
    <updated>2021-10-26T15:11:06.071Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OpenCV-Part5——综合运用"><a href="#OpenCV-Part5——综合运用" class="headerlink" title="OpenCV-Part5——综合运用"></a>OpenCV-Part5——综合运用</h1><p>[TOC]</p><p>注：本篇仅为记录本人使用过的比较高阶的OpenCV算法流程。</p><h2 id="K-Means聚类"><a href="#K-Means聚类" class="headerlink" title="K-Means聚类"></a>K-Means聚类</h2><ul><li>从分布的角度重新构造图像色彩度，减少图像中颜色数量。</li><li><code>cv2.kmeans(data, K, bestLabels, criteria, attempts, flags)</code>：<ul><li>data：np.float32数据类型，每个功能应该放在一个列中</li><li>K：集群数(nclusters)</li><li>bestLabels：预设的分类标签，没有则设为None</li><li>criteria：迭代终止标准。满足此条件时，算法迭代停止。它由3个参数的元组组成：<code>（type，max_iter，epsilon）</code>。<ul><li>type：<ul><li><code>cv2.TERM_CRITERIA_EPS</code>：如果达到指定的精度epsilon，则停止算法迭代。</li><li><code>cv2.TERM_CRITERIA_MAX_ITER</code>：在指定的迭代次数max_iter之后停止算法。</li><li><code>cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER</code>：当满足上述任何条件时停止迭代。</li></ul></li><li>max_iter：指定最大迭代次数的整数</li><li>epsilon：要求的准确性</li></ul></li><li>attempts：重复试验kmeans算法次数，将会返回最好的一次结果。</li><li>flags：该标志用于指定初始中心的采用方式。通常会使用两个标志：<code>cv2.KMEANS_PP_CENTERS</code>和<code>cv2.KMEANS_RANDOM_CENTERS</code>。</li><li>返回三个数据：<ul><li>retval：从每个点到它们相应中心的平方距离之和。</li><li>bestLabels：标签数组。是不固定的。</li><li>centers：一组聚类中心。即标签对应的值。</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;test.jpg&#x27;</span>)</span><br><span class="line">Z = img.reshape((-<span class="number">1</span>, <span class="number">3</span>))  <span class="comment"># 把所有像素拉成一条直线</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. convert to np.float32</span></span><br><span class="line">Z = np.float32(Z)  <span class="comment"># uint8 -&gt; float32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. define criteria, number of clusters(K) and apply kmeans()</span></span><br><span class="line">criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, <span class="number">10</span>, <span class="number">1.0</span>)</span><br><span class="line">K = <span class="number">3</span></span><br><span class="line">ret, label, center = cv2.kmeans(Z, K, <span class="literal">None</span>, criteria, <span class="number">10</span>, cv2.KMEANS_RANDOM_CENTERS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Now convert back into uint8, and make original image</span></span><br><span class="line">center = np.uint8(center)  <span class="comment"># float32 -&gt; uint8</span></span><br><span class="line">res = center[label.flatten()]  <span class="comment"># 将标签数组赋予真正的bgr值</span></span><br><span class="line">res2 = res.reshape(img.shape)  <span class="comment"># 重构图像</span></span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">&#x27;res2&#x27;</span>, res2)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><h2 id="极坐标与直角坐标变换"><a href="#极坐标与直角坐标变换" class="headerlink" title="极坐标与直角坐标变换"></a>极坐标与直角坐标变换</h2><h3 id="极坐标转直角坐标"><a href="#极坐标转直角坐标" class="headerlink" title="极坐标转直角坐标"></a>极坐标转直角坐标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">cir_img = cv2.imread(<span class="string">&#x27;4-1.jpg&#x27;</span>)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;panoramagram&#x27;</span>, cir_img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到圆形区域的中心坐标</span></span><br><span class="line">x0, y0 = cir_img.shape[<span class="number">0</span>] // <span class="number">2</span>, cir_img.shape[<span class="number">1</span>] // <span class="number">2</span></span><br><span class="line"><span class="comment"># 通过圆形区域半径构造展开后的图像</span></span><br><span class="line">radius = cir_img.shape[<span class="number">0</span>] // <span class="number">2</span></span><br><span class="line">rect_height = radius</span><br><span class="line">rect_width = <span class="built_in">int</span>(<span class="number">2</span> * math.pi * radius)</span><br><span class="line">rect_img = np.zeros((rect_height, rect_width, <span class="number">3</span>), dtype=<span class="string">&quot;u1&quot;</span>)</span><br><span class="line"></span><br><span class="line">start = <span class="number">0</span>  <span class="comment"># 从正下方开始切，start设定偏移角度，单位为度</span></span><br><span class="line">except_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(rect_width):</span><br><span class="line">    theta = <span class="number">2</span> * math.pi * (j / rect_width) + <span class="number">2</span> * math.pi * (start / <span class="number">360</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(rect_height):</span><br><span class="line">        <span class="comment"># 适应椭圆的极坐标展开</span></span><br><span class="line">        x = (x0 - i) * math.cos(theta) + x0  <span class="comment"># &quot;sin&quot; is clockwise but &quot;cos&quot; is anticlockwise</span></span><br><span class="line">        y = (y0 - i) * math.sin(theta) + y0</span><br><span class="line">        x, y = <span class="built_in">int</span>(x), <span class="built_in">int</span>(y)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            rect_img[i, j, :] = cir_img[x, y, :]</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            except_count = except_count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(except_count)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;rect_img.jpg&quot;</span>, rect_img)</span><br><span class="line">cv2.imshow(<span class="string">&quot;rect_img&quot;</span>, rect_img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="直角坐标转极坐标"><a href="#直角坐标转极坐标" class="headerlink" title="直角坐标转极坐标"></a>直角坐标转极坐标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img_name = <span class="string">&#x27;8.jpeg&#x27;</span></span><br><span class="line">img = cv2.imread(img_name)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;panoramagram&#x27;</span>, img)</span><br><span class="line">img = wrapped_img = cv2.resize(img, <span class="literal">None</span>, fx=<span class="number">2</span>, fy=<span class="number">2</span>, interpolation=cv2.INTER_CUBIC)</span><br><span class="line"><span class="comment"># 准备工作，计算原图像尺寸和变换后的图片大小</span></span><br><span class="line">x0 = img.shape[<span class="number">0</span>]</span><br><span class="line">y0 = img.shape[<span class="number">1</span>]</span><br><span class="line">print(x0, y0)</span><br><span class="line"><span class="comment"># 最大半径计算</span></span><br><span class="line">radius = <span class="built_in">int</span>(y0 / (<span class="number">2</span> * math.pi))</span><br><span class="line">w = <span class="number">2</span> * radius</span><br><span class="line">h = <span class="number">2</span> * radius</span><br><span class="line">wrapped_img = <span class="number">255</span> * np.ones((w, h, <span class="number">3</span>), dtype=<span class="string">&quot;u1&quot;</span>)</span><br><span class="line"></span><br><span class="line">except_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(y0):</span><br><span class="line">    <span class="comment"># 1. 求极坐标系中对应的角度theta</span></span><br><span class="line">    theta = <span class="number">2</span> * math.pi * (j / y0)</span><br><span class="line">    <span class="comment"># print(theta)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x0):</span><br><span class="line">        <span class="comment"># 2. 计算半径缩放系数</span></span><br><span class="line">        wrapped_radius = (i - x0) * radius / x0</span><br><span class="line">        <span class="comment"># 3. 利用对应关系进行换算</span></span><br><span class="line">        y = wrapped_radius * math.cos(theta) + radius</span><br><span class="line">        x = wrapped_radius * math.sin(theta) + radius</span><br><span class="line">        x, y = <span class="built_in">int</span>(x), <span class="built_in">int</span>(y)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            wrapped_img[x, y, :] = img[i, j, :]</span><br><span class="line">            <span class="comment"># 注意点,在数学坐标系中的坐标与数字图像中的坐标表示存在差异需要注意</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            except_count = except_count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(except_count)</span><br><span class="line"><span class="comment"># 提取ROI区域进行平滑处理，效果一般</span></span><br><span class="line">roi = wrapped_img[<span class="number">0</span>:radius, radius - <span class="number">5</span>:radius + <span class="number">5</span>, :]</span><br><span class="line">roi_blur = cv2.blur(roi, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">wrapped_img[<span class="number">0</span>:radius, radius - <span class="number">5</span>:radius + <span class="number">5</span>, :] = roi_blur</span><br><span class="line"><span class="comment"># wrapped_img = cv2.resize(wrapped_img,None,fx=1,fy=1,interpolation=cv2.INTER_CUBIC)</span></span><br><span class="line">name = <span class="string">&#x27;p_&#x27;</span> + img_name</span><br><span class="line">cv2.imwrite(name, wrapped_img)</span><br><span class="line">cv2.imshow(<span class="string">&quot;Unwrapped&quot;</span>, wrapped_img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">综合运用</summary>
    
    
    
    <category term="OpenCV" scheme="https://nephrencake.gitee.io/categories/OpenCV/"/>
    
    
  </entry>
  
</feed>
