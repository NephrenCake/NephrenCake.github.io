<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>浅幽丶奈芙莲的个人博客</title>
  
  <subtitle>NephrenCake Blog</subtitle>
  <link href="https://nephrencake.gitee.io/atom.xml" rel="self"/>
  
  <link href="https://nephrencake.gitee.io/"/>
  <updated>2021-01-18T01:38:47.985Z</updated>
  <id>https://nephrencake.gitee.io/</id>
  
  <author>
    <name>NephrenCake</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch深度学习实践Part8</title>
    <link href="https://nephrencake.gitee.io/2021/01/18/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/"/>
    <id>https://nephrencake.gitee.io/2021/01/18/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/</id>
    <published>2021-01-17T16:03:00.000Z</published>
    <updated>2021-01-18T01:38:47.985Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part8——加载数据集"><a href="#PyTorch深度学习实践Part8——加载数据集" class="headerlink" title="PyTorch深度学习实践Part8——加载数据集"></a>PyTorch深度学习实践Part8——加载数据集</h1><h2 id="Dataset-and-DataLoader"><a href="#Dataset-and-DataLoader" class="headerlink" title="Dataset and DataLoader"></a>Dataset and DataLoader</h2><p>dataset主要构造数据集，支持索引。</p><p>dataloader主要能拿出mini-batch，拿出一组数据以快速使用。</p>]]></content>
    
    
    <summary type="html">PyTorch深度学习实践Part8</summary>
    
    
    
    
    <category term="深度学习" scheme="https://nephrencake.gitee.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch深度学习实践Part7</title>
    <link href="https://nephrencake.gitee.io/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/"/>
    <id>https://nephrencake.gitee.io/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/</id>
    <published>2021-01-17T09:34:00.000Z</published>
    <updated>2021-01-18T01:35:59.717Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part7——处理多维特征的输入"><a href="#PyTorch深度学习实践Part7——处理多维特征的输入" class="headerlink" title="PyTorch深度学习实践Part7——处理多维特征的输入"></a>PyTorch深度学习实践Part7——处理多维特征的输入</h1><h2 id="多维特征输入"><a href="#多维特征输入" class="headerlink" title="多维特征输入"></a>多维特征输入</h2><p>从单一特征的数据，转而输入多为特征的数据，模型发生以下改变：</p><ol><li><p>对于每一条(/第i条)有n个特征(x1…xn)的数据，则有n个不同的weight(w1…wn)和1个相同的bias(b将进行广播)，并通过非线性激活函数，得出一个y_hat（假设输出维度为1）。</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117203233443.png" alt="image-20210117203233443"></p></li><li><p>对于每个zn(=xn*wn+b)都要通过非线性激活函数，Sigmoid函数是对于每个元素的，类似于numpy。</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117203726634.png" alt="image-20210117203726634"></p></li><li><p>转换成矩阵运算可以发挥cpu/gpu并行运算的优势。</p></li><li><p>在之前的代码上，想要进行多维输入，只需要修改样本以及模型构造函数。</p></li></ol><h2 id="增加神经网络层数"><a href="#增加神经网络层数" class="headerlink" title="增加神经网络层数"></a>增加神经网络层数</h2><ol><li>如何增加神经网络层数？将多层模型输入和输出，<strong>头尾相连</strong>。例如：torch.nn.Linear(8, 6)、torch.nn.Linear(6, 4)、torch.nn.Linear(4, 1)。</li><li>什么是矩阵？矩阵是<strong>空间变换函数</strong>。例如：y=A*x，y是M×1的矩阵，x是N×1的矩阵，A是M×N的矩阵，则A就是将x从N维转换到y这个M维空间的空间变换函数。</li><li>矩阵是线性变换，但是很多实际情况都是复杂、非线性的。所以，需要用多个线性变换层，通过找到最优的权重组合起来，来模拟非线性的变换。<strong>寻找非线性变换函数</strong>，就是神经网络的本质。</li><li>多层神经网络可以降维也可以升维，至于如何达到最优，则是<strong>超参数的搜索</strong>。</li><li>神经元、网络层数越多，学习能力就越强，但是同时要小心过拟合的问题。要学习数据真值和具备泛化的能力。</li></ol><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117212227590.png" alt="image-20210117212227590"></p><blockquote><p><strong>能在编程道路上立稳脚跟的核心能力：</strong></p><ol><li>读文档</li><li>基本架构理念(cpu、操作系统、主机、编译原理)</li></ol></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理数据</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)  <span class="comment"># 分隔符&#x27;,&#x27;，大多数显卡只支持32位float</span></span><br><span class="line">x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])  <span class="comment"># 左闭右开，取所有行、第一列到最后第二列。torch.from_numpy返回tensor</span></span><br><span class="line">y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])  <span class="comment"># 取所有行、最后一列。[-1]表示拿出来的是矩阵，-1表示拿出来的是向量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.activate = torch.nn.Sigmoid()  <span class="comment"># 是模块而不是函数，没有参数，没有需要训练的地方，只用来构建计算图</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.activate(self.linear1(x))</span><br><span class="line">        x = self.activate(self.linear2(x))</span><br><span class="line">        x = self.activate(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失&amp;优化</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1.0</span>)</span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500000</span>):</span><br><span class="line">    <span class="comment"># Forward</span></span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># 这里并没有用到mini-batch</span></span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    print(epoch, loss.item())</span><br><span class="line">    loss_list.append(loss.item())</span><br><span class="line">    <span class="comment"># Backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># Update</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">500000</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同网络层参数</span></span><br><span class="line">layer1_weight = model.linear1.weight.data</span><br><span class="line">layer1_bias = model.linear1.bias.data</span><br><span class="line">print(<span class="string">&quot;layer1_weight&quot;</span>, layer1_weight)</span><br><span class="line">print(<span class="string">&quot;layer1_weight.shape&quot;</span>, layer1_weight.shape)</span><br><span class="line">print(<span class="string">&quot;layer1_bias&quot;</span>, layer1_bias)</span><br><span class="line">print(<span class="string">&quot;layer1_bias.shape&quot;</span>, layer1_bias.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li>在100次训练时，损失卡在了0.65。</li><li>在1w次训练时，损失跨过0.65停在了0.45。</li><li>将学习率提升到10.0，1w次训练可以看出图像震荡，无法收敛，但是损失突破0.4以下。</li><li>将学习率调整到1.0，10w次训练，损失突破0.3以下</li><li>学习率1.0，50w次训练，损失达到0.28</li><li><a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">pytorch激活函数文档</a></li></ol><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117233025403.png" alt="image-20210117233025403"></p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117235832948.png" alt="image-20210117235832948"></p>]]></content>
    
    
    <summary type="html">PyTorch深度学习实践Part7</summary>
    
    
    
    
    <category term="深度学习" scheme="https://nephrencake.gitee.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch深度学习实践Part6</title>
    <link href="https://nephrencake.gitee.io/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/"/>
    <id>https://nephrencake.gitee.io/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/</id>
    <published>2021-01-17T07:39:52.000Z</published>
    <updated>2021-01-17T09:56:21.562Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part6——逻辑斯蒂回归"><a href="#PyTorch深度学习实践Part6——逻辑斯蒂回归" class="headerlink" title="PyTorch深度学习实践Part6——逻辑斯蒂回归"></a>PyTorch深度学习实践Part6——逻辑斯蒂回归</h1><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>逻辑斯蒂回归是处理分类问题，而不是回归任务。</p><p>处理分类问题，不能使用回归的思想，即使输出可以为0或1。原因在于：若有一个0-9，10个手写数字的分类问题，在回归模型中，1和0距离很近，0和9离得很远，但是在分类模型中，7和9的相似度就比8与7或9的相似度要高。</p><p>分类问题本质上输出的是概率，例如P(0)、P(1)…。</p><h3 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h3><p>通过考试的概率是多少</p><h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><p>0-9手写数字检测分类</p><h2 id="torchvision工具包"><a href="#torchvision工具包" class="headerlink" title="torchvision工具包"></a>torchvision工具包</h2><p>指定目录，训练/测试，是否需要下载</p><p>MNIST、CIFAR10…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">train_set = torchvision.datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="饱和函数"><a href="#饱和函数" class="headerlink" title="饱和函数"></a>饱和函数</h2><h3 id="逻辑斯蒂函数"><a href="#逻辑斯蒂函数" class="headerlink" title="逻辑斯蒂函数"></a>逻辑斯蒂函数</h3><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117161720715.png" alt="image-20210117161720715"></p><h3 id="其他Sigmoid-functions"><a href="#其他Sigmoid-functions" class="headerlink" title="其他Sigmoid functions"></a>其他Sigmoid functions</h3><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117161834404.png" alt="image-20210117161834404"></p><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><ol><li><p>Logistic Regression类似于正态分布。</p></li><li><p>Logistic Regression是Sigmoid functions中最著名的，所以有些地方用Sigmoid指代Logistic。</p></li><li><p>逻辑斯蒂回归和线性模型的明显区别是在线性模型的后面，添加了激活函数(非线性变换)，将y_hat代入逻辑斯蒂公式中的x。</p></li><li><p><a href="https://blog.csdn.net/C_chuxin/article/details/86174807">交叉熵损失函数的推导过程与直观理解</a></p></li><li><p>y_hat是预测的值[0,1]之间的概率，y是真实值，预测与标签越接近，BCE损失越小。</p></li></ol><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117164616620.png" alt="image-20210117164616620"></p><blockquote><p>要计算的是分布的差异，而不是数值上的距离</p></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><a href="https://blog.csdn.net/weixin_42621901/article/details/107664771">torch.sigmoid()、torch.nn.Sigmoid()和torch.nn.functional.sigmoid()三者之间的区别</a></li><li>BCELoss(Binary CrossEntropyLoss)是CrossEntropyLoss的一个特例，只用于二分类问题，而CrossEntropyLoss可以用于二分类，也可以用于多分类。</li><li><a href="https://www.cnblogs.com/samwoog/p/13857843.html">BCE和CE交叉熵损失函数的区别</a></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------Prepare dataset----------------------------#</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])  <span class="comment"># 二分类</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------Design model using Class----------------------------#</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># nn.functional.sigmoid is deprecated</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(self.linear(x))  <span class="comment"># 激活函数sigmoid不需要参数训练，直接调用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line"><span class="comment"># --------------------------Construct loss and optimizer-----------------------------#</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># 交叉熵，size_average=False已经被弃用</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># --------------------------Training cycle-----------------------------#</span></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># 正向传播</span></span><br><span class="line">    loss = criterion(y_pred, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    print(epoch, loss.item())</span><br><span class="line">    loss_list.append(loss.item())  <span class="comment"># 保存loss</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将梯度归零</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line">print(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line">print(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.item())</span><br><span class="line"></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1000</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># y_pred =  0.8808996081352234</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117172329760.png" alt="image-20210117172329760"></p>]]></content>
    
    
    <summary type="html">PyTorch深度学习实践Part6</summary>
    
    
    
    
    <category term="深度学习" scheme="https://nephrencake.gitee.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch深度学习实践Part5</title>
    <link href="https://nephrencake.gitee.io/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/"/>
    <id>https://nephrencake.gitee.io/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/</id>
    <published>2021-01-17T03:05:41.000Z</published>
    <updated>2021-01-17T07:39:36.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part5——线性回归"><a href="#PyTorch深度学习实践Part5——线性回归" class="headerlink" title="PyTorch深度学习实践Part5——线性回归"></a>PyTorch深度学习实践Part5——线性回归</h1><h2 id="PyTorch周期"><a href="#PyTorch周期" class="headerlink" title="PyTorch周期"></a>PyTorch周期</h2><ol><li>prepare dataset</li><li>design model using Class 目的是为了前馈forward，即计算y hat(预测值)</li><li>Construct loss and optimizer (using PyTorch API) 其中，计算loss是为了进行反向传播，optimizer是为了更新梯度。</li><li>Training cycle (<u><strong><em>forward,backward,update</em></strong></u>)</li></ol><h2 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h2><p>原本w只是1×1的矩阵，比如tensor([0.], requires_grad=True)，很有可能行列数量与xy对不上，这个时候pytorch会进行<strong>广播</strong>，将w<strong>扩展成一个3×1矩阵</strong>。</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117122903497.png" alt="image-20210117122903497"></p><blockquote><p>pytorch直接写“*”表示矩阵<strong>对应位置元素相乘</strong>（哈达玛积），数学上的矩阵乘法有另外的函数torch.matmul</p></blockquote><p>这里x、y的维度都是1（有可能不是1），但是都应当看成一个<strong>矩阵</strong>，而不能是向量。</p><blockquote><p>x、y的列是维度/特征，行是记录/样本</p></blockquote><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ol><li>要把计算模型定义成一个类，<strong>继承于torch.nn.Model</strong>。（nn：neural network）</li><li>如果有pytorch没有提供的需求，或者其效率不够高，可以从Function中继承，构造自己的计算块。</li><li>Linear类包括成员变量weight和bias，默认bias=True，同样继承于torch.nn.Model，可以进行反向传播。</li><li>权重放在x右边，或者转置放在左边。（不管怎么放都是为了凑矩阵基本积）</li><li>父类实现了callable函数，让其能够被调用。在call中会调用前馈forward()，所以必须重写forward()。</li></ol><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117132843838.png" alt="image-20210117132843838"></p><p>*args, **kwargs的用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;args:&#x27;</span>, args)</span><br><span class="line">    print(<span class="string">&#x27;kwargs:&#x27;</span>, kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fun(<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>, x=<span class="number">6</span>, y=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># args: (1, 2, 7)</span></span><br><span class="line"><span class="comment"># kwargs: &#123;&#x27;x&#x27;: 6&#125;</span></span><br></pre></td></tr></table></figure><h2 id="损失-amp-优化"><a href="#损失-amp-优化" class="headerlink" title="损失&amp;优化"></a>损失&amp;优化</h2><ol><li>计算损失使用现成的类torch.nn.MSELoss。</li><li>一般使用随机梯度下降算法，求和平均是没有必要的，torch.nn.MSELoss(size_average=<strong>False</strong>)</li><li>使用现成的优化器类torch.optim.SGD</li><li><a href="https://pytorch.org/docs/1.7.0/optim.html">不同的优化器，官方文档</a></li><li>控制训练次数，不能过少（训练不到位），也不能过多（过拟合）</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据，要是矩阵</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型，继承、重写</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成模型的对象</span></span><br><span class="line">model = LinearModel()</span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># size_average=False已经被弃用</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># __call__()调用forward()正向传播计算预测值</span></span><br><span class="line">    loss = criterion(y_pred, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    print(epoch, loss.item())  <span class="comment"># 可以直接打印loss，因为调用的是__str__()不会产生计算图</span></span><br><span class="line">    loss_list.append(loss.item())  <span class="comment"># 保存loss</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将梯度归零</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line">print(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)  <span class="comment"># 使用训练好的模型进行预测</span></span><br><span class="line">print(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印图表</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><p>测试不同的优化器。除了LBFGS，只需要修改调用对应优化器的构造器。</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>w =  0.20570674538612366</p><p>b =  -0.5057424902915955</p><p>y_pred =  0.31708449125289917</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117152240566.png" alt="image-20210117152240566"></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>w =  1.466607928276062</p><p>b =  0.14079217612743378</p><p>y_pred =  6.007224082946777</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117152322232.png" alt="image-20210117152322232"></p><h3 id="Adamax"><a href="#Adamax" class="headerlink" title="Adamax"></a>Adamax</h3><p>w =  -0.022818174213171005</p><p>b =  0.9245702028274536</p><p>y_pred =  0.8332974910736084</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117152149494.png" alt="image-20210117152149494"></p><h3 id="ASGD"><a href="#ASGD" class="headerlink" title="ASGD"></a>ASGD</h3><p>w =  1.6153326034545898</p><p>b =  0.87442547082901</p><p>y_pred =  7.335755825042725</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117151915659.png" alt="image-20210117151915659"></p><h3 id="LBFGS"><a href="#LBFGS" class="headerlink" title="LBFGS"></a>LBFGS</h3><p>由于LBFGS算法需要重复多次计算函数，因此需要传入一个闭包去允许它们重新计算模型。这个闭包应当清空梯度， 计算损失，然后返回。</p><p>参考<a href="https://blog.csdn.net/ys1305/article/details/94332643">一篇关于优化器的博文</a></p><p>训练模型部分代码应修改为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">closure</span>():</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将梯度归零</span></span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># __call__()调用forward()正向传播计算预测值</span></span><br><span class="line">    loss = criterion(y_pred, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    print(epoch, loss.item())  <span class="comment"># 可以直接打印loss，因为调用的是__str__()不会产生计算图</span></span><br><span class="line">    loss_list.append(loss.item())  <span class="comment"># 保存loss</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.step(closure())  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>w =  1.7660775184631348</p><p>b =  0.531760573387146</p><p>y_pred =  7.596070766448975</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117153648037.png" alt="image-20210117153648037"></p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>w =  1.734222650527954</p><p>b =  0.5857117176055908</p><p>y_pred =  7.522602081298828</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117150905304.png" alt="image-20210117150905304"></p><h3 id="Rprop"><a href="#Rprop" class="headerlink" title="Rprop"></a>Rprop</h3><p>w =  1.9997763633728027</p><p>b =  0.0004527860146481544</p><p>y_pred =  7.999558448791504</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/Rprop.png" alt="image-20210117150540886"></p><h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>w =  1.8483222723007202</p><p>b =  0.3447989821434021</p><p>y_pred =  7.738088130950928</p><p><img src="/2021/01/17/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/SGD.png" alt="image-20210117150219223"></p>]]></content>
    
    
    <summary type="html">PyTorch深度学习实践Part5</summary>
    
    
    
    
    <category term="深度学习" scheme="https://nephrencake.gitee.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch深度学习实践Part4</title>
    <link href="https://nephrencake.gitee.io/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/"/>
    <id>https://nephrencake.gitee.io/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/</id>
    <published>2021-01-16T14:34:19.000Z</published>
    <updated>2021-01-17T03:05:03.806Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part4——反向传播"><a href="#PyTorch深度学习实践Part4——反向传播" class="headerlink" title="PyTorch深度学习实践Part4——反向传播"></a>PyTorch深度学习实践Part4——反向传播</h1><p>对于简单模型可以手动求解析式，但是对于复杂模型求解析式几乎不可能。</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>每一层神经网络包括一次矩阵乘法(Matrix Multiplication)、一次向量加法、非线性变化函数(为了防止展开函数而导致深层神经网络无意义)</p><p><img src="/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/image-20210116225529823.png" alt="image-20210116225529823"></p><h2 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h2><p>在pytorch中，梯度存在变量而不是计算模块里。</p><p><img src="/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/image-20210116230629292.png" alt="image-20210116230629292"></p><p><img src="/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/image-20210117091957454.png" alt="image-20210117091957454"></p><p>在计算过程中中，虽然有些变量可以不求导，但是一样要具备能够求导的能力。比如x的值就有可能是前一层网络的y_hat传递下来的。</p><p>核心在于梯度，loss虽然不会作为变量参与计算过程，但是同样需要保留，作为图像数据来判断最终是否收敛。</p><h2 id="PyTorch实现反向传播"><a href="#PyTorch实现反向传播" class="headerlink" title="PyTorch实现反向传播"></a>PyTorch实现反向传播</h2><p>线性模型y=w*x，用pytorch实现反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = torch.Tensor([<span class="number">1.0</span>])  <span class="comment"># data必须是一个序列</span></span><br><span class="line">w.requires_grad = <span class="literal">True</span>  <span class="comment"># 需要计算梯度</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w  <span class="comment"># w是Tensor，运算符重载，x也会转成tensor</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span>  <span class="comment"># 看到代码一定要有意识想到如何构建计算图</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        l = loss(x, y)  <span class="comment"># 前馈过程，正式构建计算图，计算损失更新l</span></span><br><span class="line">        l.backward()  <span class="comment"># 反向传播，计算梯度，释放计算图</span></span><br><span class="line">        print(<span class="string">&#x27;\tgrad:&#x27;</span>, x, y, w.grad.item())</span><br><span class="line">        w.data = w.data - <span class="number">0.01</span> * w.grad.data  <span class="comment"># 更新权重w，注意grad也是一个tensor，不使用.data的话相当于在构建计算图</span></span><br><span class="line"></span><br><span class="line">        w.grad.data.zero_()  <span class="comment"># 将梯度w.grad.data清零</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;progress:&#x27;</span>, epoch, l.item())  <span class="comment"># 取出loss使用l.item，不要直接使用l（l是tensor会构建计算图）</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"><span class="comment"># predict (after training) 4 7.999998569488525</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><p>二次模型y=w1<em>x²+w2</em>x+b的反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># y = x ** 2 + 2 * x + 1</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">4.0</span>, <span class="number">9.0</span>, <span class="number">16.0</span>]</span><br><span class="line"></span><br><span class="line">w1 = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w2 = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">b = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w1.requires_grad, w2.requires_grad, b.requires_grad = <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> w1 * x * x + w2 * x + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        l = loss(x, y)  <span class="comment"># 前馈过程，正式构建计算图，计算损失更新l</span></span><br><span class="line">        l.backward()  <span class="comment"># 反向传播，计算梯度，释放计算图</span></span><br><span class="line">        print(<span class="string">&#x27;\tgrad:&#x27;</span>, x, y, w1.grad.item(), w2.grad.item(), b.grad.item())</span><br><span class="line">        w1.data = w1.data - <span class="number">0.01</span> * w1.grad.data</span><br><span class="line">        w2.data = w2.data - <span class="number">0.01</span> * w2.grad.data</span><br><span class="line">        b.data = b.data - <span class="number">0.01</span> * b.grad.data</span><br><span class="line">        w1.grad.data.zero_()</span><br><span class="line">        w2.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    print(<span class="string">&#x27;progress:&#x27;</span>, epoch, l.item())  <span class="comment"># 取出loss使用l.item，不要直接使用l（l是tensor会构建计算图）</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line">print(w1.data, w2.data, b.data)</span><br><span class="line"><span class="comment"># predict (after training) 4 25.259323120117188</span></span><br><span class="line"><span class="comment"># tensor([1.1145]) tensor([1.4928]) tensor([1.4557])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>改了一下原本的数据集，更符合二次函数，但是因为样本量过少，预测的权重并不是很好。</p>]]></content>
    
    
    <summary type="html">PyTorch深度学习实践Part4</summary>
    
    
    
    
    <category term="深度学习" scheme="https://nephrencake.gitee.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch深度学习实践Part3</title>
    <link href="https://nephrencake.gitee.io/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/"/>
    <id>https://nephrencake.gitee.io/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/</id>
    <published>2021-01-16T12:23:00.000Z</published>
    <updated>2021-01-16T14:42:50.578Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part3——梯度下降算法"><a href="#PyTorch深度学习实践Part3——梯度下降算法" class="headerlink" title="PyTorch深度学习实践Part3——梯度下降算法"></a>PyTorch深度学习实践Part3——梯度下降算法</h1><h2 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h2><ol><li><p>上讲是穷举所有可能值并肉眼搜索损失最低点。</p></li><li><p>分治法可能错失关键，最终只找到局部最优</p><blockquote><p>穷举和分治都不能有效解决大数据</p></blockquote></li><li><p>梯度(gradient)决定权重w往哪个方向走，梯度即成本对权重求导，为了控制步伐需要设定一个较小的学习率。</p><p><img src="/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/image-20210116203946162.png" alt="image-20210116203946162"></p></li><li><p>在大量的实验中发现，其实很多情况下，我们很难陷入到局部最优点。但是存在另外一个问题，鞍点。鞍点会导致无法继续迭代，可以选择通过引入动量解决。</p></li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">xs, ys</span>):</span></span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):  <span class="comment"># 3行数据</span></span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred - y) ** <span class="number">2</span>  <span class="comment"># 累加损失平方</span></span><br><span class="line">    <span class="keyword">return</span> cost / <span class="built_in">len</span>(xs)  <span class="comment"># 平均</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">xs, ys</span>):</span></span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        grad += <span class="number">2</span> * x * (x * w - y)  <span class="comment"># 成本对权重求导</span></span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Predict(before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">mse_list = []  <span class="comment"># 保存损失的变化曲线</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost_val = cost(x_data, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    mse_list.append(cost_val)  <span class="comment"># 记录损失变化</span></span><br><span class="line">    grad_val = gradient(x_data, y_data)  <span class="comment"># 计算梯度</span></span><br><span class="line">    w -= <span class="number">0.01</span> * grad_val  <span class="comment"># 改善权重</span></span><br><span class="line">    print(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line">print(<span class="string">&#x27;Predict(after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图表打印</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/image-20210116215632394.png" alt="image-20210116215632394"></p><ol><li>绘图时想要消除局部震荡，可以使用指数加权均值方法，使其变成更加平滑的曲线</li><li>如果训练的图像发散，则表明这次训练失败了。其原因有很多，比如，学习率取太大。</li></ol><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>使用梯度下降方法时，更加常用随机梯度下降(Stochastic Gradient Descent)。</p><p>随机梯度下降也是跨越鞍点的一种方法，同时也可以大幅减少计算量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x * (x * w - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Predict(before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">mse_list = []  <span class="comment"># 保存损失的变化曲线</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    x, y, i = <span class="number">0</span>, <span class="number">0</span>, random.randint(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 样本原本就是随机的，所以不需要打乱样本</span></span><br><span class="line">    <span class="keyword">for</span> m, n, j <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data, <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)):</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            x, y = m, n  <span class="comment"># 3组中随机选取一组</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    grad = gradient(x, y)  <span class="comment"># 计算梯度</span></span><br><span class="line">    w -= <span class="number">0.01</span> * grad  <span class="comment"># 改善权重</span></span><br><span class="line">    cost_val = loss(x, y)  <span class="comment"># 计算损失</span></span><br><span class="line">    mse_list.append(cost_val)  <span class="comment"># 记录损失变化</span></span><br><span class="line">    print(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line">print(<span class="string">&#x27;Predict(after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图表打印</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/image-20210116222847776.png" alt="image-20210116222847776"></p><p>随机梯度下降可能享受不到并行计算的效率加成，因此会使用折中方法，批量随机梯度下降(Mini-Batch/Batch)</p>]]></content>
    
    
    <summary type="html">PyTorch深度学习实践Part3</summary>
    
    
    
    
    <category term="深度学习" scheme="https://nephrencake.gitee.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch深度学习实践Part2</title>
    <link href="https://nephrencake.gitee.io/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/"/>
    <id>https://nephrencake.gitee.io/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/</id>
    <published>2021-01-16T01:50:34.000Z</published>
    <updated>2021-01-16T14:40:54.047Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part2——线性模型"><a href="#PyTorch深度学习实践Part2——线性模型" class="headerlink" title="PyTorch深度学习实践Part2——线性模型"></a>PyTorch深度学习实践Part2——线性模型</h1><h2 id="一般过程"><a href="#一般过程" class="headerlink" title="一般过程"></a>一般过程</h2><ol><li>Data Set </li><li>Model（神经网络、决策树、朴素贝叶斯）</li><li>Trainning</li><li>Infering</li></ol><h2 id="训练-amp-测试"><a href="#训练-amp-测试" class="headerlink" title="训练&amp;测试"></a>训练&amp;测试</h2><h3 id="训练集拆分"><a href="#训练集拆分" class="headerlink" title="训练集拆分"></a>训练集拆分</h3><p>在竞赛中，训练集是可见的，测试集一般是不可见的。为了提高或验证模型的准确度，一般会把手中的训练集拆分，以及交叉验证。</p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>当模型对训练集的噪声也学习进去的时候，对训练集以外的数据可能会出现准确率下降的情况。因此一个好的模型需要有良好的泛化能力。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/image-20210116101533492.png" alt="image-20210116101533492"></p><p>平均平方误差（MSE MeanSquareError）</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前馈</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 存放结果，所有权重和对应的均方差</span></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"><span class="comment"># 穷举所有权重0.0-4.1步长0.1</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    print(<span class="string">&#x27;w=&#x27;</span>, w)</span><br><span class="line">    l_sum = <span class="number">0</span>  <span class="comment"># 损失的和</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):  <span class="comment"># 打包，一共三行</span></span><br><span class="line">        y_pred_val = forward(x_val)  <span class="comment"># 前馈算出此权重和样本得出的预测值，其实已经包含在loss()中，只是为了打印</span></span><br><span class="line">        loss_val = loss(x_val, y_val)  <span class="comment"># 计算该权重预测值得损失</span></span><br><span class="line">        l_sum += loss_val  <span class="comment"># 求损失和</span></span><br><span class="line">        print(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)  <span class="comment"># 当前的x、y值、预测值、损失</span></span><br><span class="line">    print(<span class="string">&#x27;MSE=&#x27;</span>, l_sum / <span class="number">3</span>)  <span class="comment"># 求损失的平均</span></span><br><span class="line">    <span class="comment"># 保存记录</span></span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum / <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 图表打印</span></span><br><span class="line">plt.plot(w_list, mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/image-20210116155722125.png" alt="image-20210116155722125"></p><blockquote><p>做深度学习要定期存盘，防止意外导致数据丢失</p></blockquote><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里设函数为y=3x+2</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">5.0</span>, <span class="number">8.0</span>, <span class="number">11.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.arange()左闭右开；2.打印 0.0、1.0 时只会显示 0.、1.；3.meshgrid()之后w、b都是41*41矩阵</span></span><br><span class="line"><span class="comment"># 4.这里前馈中是矩阵点对点的运算，但注意并不是矩阵运算，好处是省去了n层for循环，举例：</span></span><br><span class="line"><span class="comment"># a=[[1 2 3][1 2 3]]</span></span><br><span class="line"><span class="comment"># b=[[7 7 7][8 8 8]]</span></span><br><span class="line"><span class="comment"># a*b=[[ 7 14 21][ 8 16 24]]</span></span><br><span class="line">w_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">b_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">w, b = np.meshgrid(w_list, b_list)</span><br><span class="line"></span><br><span class="line">l_sum = <span class="number">0</span>  <span class="comment"># 损失的和</span></span><br><span class="line"><span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):  <span class="comment"># 遍历三次</span></span><br><span class="line">    <span class="comment"># y_pred_val、loss_val都是41*41的矩阵，即41个w和41个b组合的预测结果和损失</span></span><br><span class="line">    y_pred_val = forward(x_val)</span><br><span class="line">    loss_val = loss(x_val, y_val)</span><br><span class="line">    l_sum += loss_val</span><br><span class="line">    print(<span class="string">&#x27;\nx_val：&#x27;</span>, x_val,<span class="string">&#x27;\ny_val：&#x27;</span>, y_val, <span class="string">&#x27;\ny_pred_val：&#x27;</span>,y_pred_val, <span class="string">&#x27;\nloss_val：&#x27;</span>,loss_val)  <span class="comment"># 当前的x、y值、预测值、损失</span></span><br><span class="line">mse_list = l_sum / <span class="number">3</span></span><br><span class="line"><span class="comment"># mse_list也是一个ndarray类型的41*41矩阵</span></span><br><span class="line">print(<span class="string">&#x27;MSE=&#x27;</span>, mse_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3d图表</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(w, b, mse_list)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里设函数为y=3x+2</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">5.0</span>, <span class="number">8.0</span>, <span class="number">11.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里都是矩阵的运算</span></span><br><span class="line"><span class="comment"># 1.arange()左闭右开；2.打印 0.0、1.0 时只会显示 0.、1.；3.meshgrid之后w、b都是41*41矩阵</span></span><br><span class="line">w_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">b_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">w, b = np.meshgrid(w_list, b_list)</span><br><span class="line"></span><br><span class="line">l_sum = <span class="number">0</span>  <span class="comment"># 损失的和</span></span><br><span class="line"><span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):  <span class="comment"># 遍历三次</span></span><br><span class="line">    y_pred_val = forward(x_val)</span><br><span class="line">    loss_val = loss(x_val, y_val)</span><br><span class="line">    l_sum += loss_val</span><br><span class="line">    print(<span class="string">&#x27;\nx_val：&#x27;</span>, x_val,<span class="string">&#x27;\ny_val：&#x27;</span>, y_val, <span class="string">&#x27;\ny_pred_val：&#x27;</span>,y_pred_val, <span class="string">&#x27;\nloss_val：&#x27;</span>,loss_val)  <span class="comment"># 当前的x、y值、预测值、损失</span></span><br><span class="line">mse_list = l_sum / <span class="number">3</span></span><br><span class="line"><span class="comment"># mse_list也是一个ndarray类型的41*41矩阵</span></span><br><span class="line">print(<span class="string">&#x27;MSE=&#x27;</span>, mse_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3d图表</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(w, b, mse_list)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/16/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/image-20210116155626917.png" alt="image-20210116155626917"></p>]]></content>
    
    
    <summary type="html">PyTorch深度学习实践Part2</summary>
    
    
    
    
    <category term="深度学习" scheme="https://nephrencake.gitee.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch深度学习实践Part1</title>
    <link href="https://nephrencake.gitee.io/2021/01/15/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/"/>
    <id>https://nephrencake.gitee.io/2021/01/15/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/</id>
    <published>2021-01-15T14:09:46.000Z</published>
    <updated>2021-01-16T14:41:13.770Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part1——概论"><a href="#PyTorch深度学习实践Part1——概论" class="headerlink" title="PyTorch深度学习实践Part1——概论"></a>PyTorch深度学习实践Part1——概论</h1><h2 id="技术成熟度曲线"><a href="#技术成熟度曲线" class="headerlink" title="技术成熟度曲线"></a>技术成熟度曲线</h2><p><img src="https://pics0.baidu.com/feed/3b87e950352ac65ca9eb5abaa5e3f21692138a21.jpeg?token=f040365f69edd2ccffd0cf40ebe8dd1d" alt="技术成熟度曲线"></p><p><strong>科技诞生的促动期</strong> (Technology Trigger)：在此阶段，随着媒体大肆的报道过度，非理性的渲染，产品的知名度无所不在，然而随着这个科技的缺点、问题、限制出现，失败的案例大于成功的案例，例如:.com公司 1998~2000年之间的非理性疯狂飙升期。</p><p><strong>过高期望的峰值</strong>（Peak of Inflated Expectations）：早期公众的过分关注演绎出了一系列成功的故事——当然同时也有众多失败的例子。对于失败，有些公司采取了补救措施，而大部分却无动于衷。</p><p><strong>泡沫化的底谷期</strong> (Trough of Disillusionment)：在历经前面阶段所存活的科技经过多方扎实有重点的试验，而对此科技的适用范围及限制是以客观的并实际的了解，成功并能存活的经营模式逐渐成长。</p><p><strong>稳步爬升的光明期</strong> (Slope of Enlightenment)：在此阶段，有一新科技的诞生，在市面上受到主要媒体与业界高度的注意，例如:1996年的Internet ，Web。</p><p><strong>实质生产的高峰期</strong> (Plateau of Productivity)：在此阶段，新科技产生的利益与潜力被市场实际接受，实质支援此经营模式的工具、方法论经过数代的演进，进入了非常成熟的阶段。</p><blockquote><p>在使用pytorch或一系列新技术的时候，一定要学会看官方文档，这是一个非常重要的能力！</p></blockquote><h2 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h2><p><img src="/2021/01/15/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210115233930514.png" alt="AI技术"></p><p>AI除了machine learning之外还有机器视觉、自然语言处理nlp、因果推断等。</p><p>机器学习大部分都是监督学习，即用一组标签过的值进行模型训练。</p><p>机器学习中的算法区别于普通的算法（穷举、贪心等），是通过数据训练并验证得出一个好用的模型，其计算过程来自于数据而不是人工的设计。</p><p>深度学习从模型上看用的是神经网络，从目标上看属于表示学习的分支。方法有，多层感知机、卷积神经网络、循环神经网络等。</p><h2 id="维度诅咒"><a href="#维度诅咒" class="headerlink" title="维度诅咒"></a>维度诅咒</h2><p>随着feature上升，为了保持准确性，其所需的数据量将急速上升，然而获取打过标签的数据，工作量大、成本高。</p><p>n<em>1的向量采样点需要一个3</em>n的矩阵来映射到3*1的向量，实现降维（PCA主成成分分析）。但是降维的同时也要尽量保证高维空间的度量信息，这个过程叫做表示学习（Present）。这个数据分布是在高维空间里的低维流行（Manifold）。</p><h2 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h2><p><img src="/2021/01/15/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210116083615697.png" alt="image-20210116083615697"></p><h2 id="传统机器学习分类"><a href="#传统机器学习分类" class="headerlink" title="传统机器学习分类"></a>传统机器学习分类</h2><p><img src="/2021/01/15/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210116083809610.png" alt="image-20210116083809610"></p><h2 id="神经网络发展"><a href="#神经网络发展" class="headerlink" title="神经网络发展"></a>神经网络发展</h2><p>由生物实验得出，哺乳动物的视觉神经是分层的。浅层只检测物体的运动等，深层才开始识别物体的分类。由此出现了感知机。</p><p>现在神经网络早已不是生物的范畴，而是工程与数学方面。</p><p>真正让神经网络发展起来的是反向传播（Back Propagation），其核心在于计算图。</p><p><img src="/2021/01/15/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210116090403146.png" alt="image-20210116090403146"></p>]]></content>
    
    
    <summary type="html">PyTorch深度学习实践Part1</summary>
    
    
    
    
    <category term="深度学习" scheme="https://nephrencake.gitee.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Butterfly主题安装和魔改</title>
    <link href="https://nephrencake.gitee.io/2021/01/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/"/>
    <id>https://nephrencake.gitee.io/2021/01/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/</id>
    <published>2020-12-31T16:00:00.000Z</published>
    <updated>2020-12-31T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装butterfly"><a href="#安装butterfly" class="headerlink" title="安装butterfly"></a>安装butterfly</h1><ol><li>运行<code>git clone https://github.com/jerryc127/hexo-theme-butterfly themes/butterfly</code></li><li>打开_config.yml找到这一行<code>theme: landspace</code>然后将landspace替换butterfly</li><li>安装插件<code>cnpm install hexo-renderer-pug hexo-renderer-stylus</code></li><li>安装插件<code>cnpm install --save hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive</code></li></ol><blockquote><p>为了以后升级方便，这里不推荐直接对主题的配置文件进行修改，而是复制配置文件进行修改。个人推荐把主題的配置文件_config.yml复制到 Hexo 工作目录下的source/_data/butterfly.yml，如果目录不存在那就创建一个。</p></blockquote><h1 id="butterfly主题魔改"><a href="#butterfly主题魔改" class="headerlink" title="butterfly主题魔改"></a>butterfly主题魔改</h1><p>自己一开始动手做的时候大部分都参考Dreamy.TZK的博客</p><p><a href="https://www.antmoe.com/posts/75a6347a/index.html">Hexo安装并使用Butterfly主题</a></p><p>但是改到后来就越来越觉得，版本问题导致的主题修改不兼容，问题实在很大。甚至到后来想要获得自己的预期效果时，已经不得不去在源代码上下手<del>，因为还没有学过前端，改的属实面目全非</del>。</p><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>这里只能为想要自己动手改的小伙伴一些建议，比如想修改文章页，可以结合浏览器的开发者工具来找到相应的参数，来修改对应的值。</p><p><img src="/2021/01/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/Untitled.png" alt="Butterfly主题安装和魔改/Untitled.png"></p><p><img src="/2021/01/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/Untitled%201.png" alt="Butterfly主题安装和魔改/Untitled%201.png"></p><h2 id="相册的使用"><a href="#相册的使用" class="headerlink" title="相册的使用"></a>相册的使用</h2><p><a href="https://blog.ahzoo.cn/2020/07/20/b7201/">https://blog.ahzoo.cn/2020/07/20/b7201/</a></p><h2 id="关于文章中插入图片"><a href="#关于文章中插入图片" class="headerlink" title="关于文章中插入图片"></a>关于文章中插入图片</h2><p>先把hexo的配置文件中的 relative_link 参数确保为false。否则会导致butterfly各分页面的链接错乱</p><p><img src="/2021/01/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/image-20201231210020980.png" alt="image-20201231210020980"></p><p>把每个文章开头部分加一个参数 relative_link: true。使每个文章部分遵从相对位置的引用，这样可以将文章的图片，不仅在typora或是在服务器上，都能够实时看到自己文章图片引用的效果。</p><p><img src="/2021/01/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/image-20201231205840731.png" alt="image-20201231205840731"></p><p><img src="/2021/01/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/image-20201231210339382.png" alt="image-20201231210339382"></p>]]></content>
    
    
    <summary type="html">Butterfly主题安装和魔改</summary>
    
    
    
    <category term="Hexo博客搭建" scheme="https://nephrencake.gitee.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
    <category term="hexo" scheme="https://nephrencake.gitee.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hexo静态博客搭建和部署</title>
    <link href="https://nephrencake.gitee.io/2021/01/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/"/>
    <id>https://nephrencake.gitee.io/2021/01/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/</id>
    <published>2020-12-31T16:00:00.000Z</published>
    <updated>2020-12-31T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><h2 id="Nodejs"><a href="#Nodejs" class="headerlink" title="Nodejs"></a>Nodejs</h2><p><a href="https://nodejs.org/en/">Node.js</a></p><ul><li><code>node -v</code>确认nodejs版本，安装成功</li></ul><h2 id="Git-Bash"><a href="#Git-Bash" class="headerlink" title="Git Bash"></a>Git Bash</h2><p><a href="https://www.git-scm.com/download/win">Downloading Git</a></p><ul><li><code>git --version</code>确认nodejs版本，安装成功</li></ul><blockquote><p>nodejs和git自己选好安装位置之后无脑下一步就行。以下都以我个人的安装目录（D:\ProgrammingKits\nodejs 和 D:\ProgrammingKits\Git）为前提，请大家各自修改为自己的路径。</p></blockquote><h1 id="Nodejs插件安装"><a href="#Nodejs插件安装" class="headerlink" title="Nodejs插件安装"></a>Nodejs插件安装</h1><ul><li><p>最新的Nodejs自带npm，但是默认安装和缓存地址不在Nodejs根目录下</p><p>  npm的默认全局模块的安装地址是 C:\Users\Administrator\AppData\Roaming\npm</p><p>  npm的默认缓存的地址是 C:\Users\Administrator\AppData\Roaming\npm_cache</p></li></ul><p>首先修改nodejs的prefix（全局）和cache（缓存）文件夹地址</p><ul><li>运行<code>npm config set cache &quot;D:\ProgrammingKits\nodejs\node_cache&quot;</code>设置缓存文件夹</li><li>运行<code>npm config set prefix &quot;D:\ProgrammingKits\nodejs\nodejs&quot;</code>设置全局模块存放路径。</li></ul><p><del>这种方法可以不用像<code>npm config set prefix &quot;D:\ProgrammingKits\nodejs\node_global&quot;</code>需要修改环境变量。</del></p><p>以后npm和cnpm安装的全局模块都会被放到 D:\ProgrammingKits\nodejs\node_modules 下，跟自带的npm模块本体在一个文件夹中。</p><h2 id="cnpm"><a href="#cnpm" class="headerlink" title="cnpm"></a>cnpm</h2><p>这里安装淘宝的cnpm包管理器，以提高下载速度。</p><ul><li>运行<code>npm install -g cnpm --registry=http://registry.npm.taobao.org</code></li><li><code>cnpm -v</code> 确认cnpm版本，安装成功</li></ul><h2 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h2><p>静态博客框架</p><ul><li>运行<code>cnpm install -g hexo-cli</code> 安装hexo框架</li><li><code>hexo -v</code>确认hexo版本，安装成功</li></ul><h1 id="Hexo框架的使用"><a href="#Hexo框架的使用" class="headerlink" title="Hexo框架的使用"></a>Hexo框架的使用</h1><ul><li>hexo常用命令<ul><li><code>hexo init</code>初始化博客</li><li><code>hexo clean</code>清理缓存文件</li><li><code>hexo g</code>生成文件</li><li><code>hexo s</code>运行本地服务器</li><li><code>hexo d</code>部署到服务器</li><li><code>hexo n &quot;MyBlog&quot;</code>创建新的文章</li></ul></li></ul><p>现在我们只需要在 D:\MyBlog\HexoBlog 下运行<code>hexo init &amp; hexo s</code>，在浏览器中输入 <a href="http://localhost:4000/">localhost:4000</a> 即为最初始的博客内容。</p><h1 id="部署博客"><a href="#部署博客" class="headerlink" title="部署博客"></a>部署博客</h1><h2 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h2><p>用来存放你的代码/网站供别人访问</p><p><img src="/2021/01/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled.png" alt="Hexo静态博客搭建和部署/Untitled.png"></p><p><img src="/2021/01/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%201.png" alt="Hexo静态博客搭建和部署/Untitled%201.png"></p><h2 id="创建部署分支"><a href="#创建部署分支" class="headerlink" title="创建部署分支"></a>创建部署分支</h2><p>master用来放代码，ph-pages用来部署网站</p><p><img src="/2021/01/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%202.png" alt="Hexo静态博客搭建和部署/Untitled%202.png"></p><p><img src="/2021/01/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%203.png" alt="Hexo静态博客搭建和部署/Untitled%203.png"></p><h2 id="开启Gitee-Pages服务"><a href="#开启Gitee-Pages服务" class="headerlink" title="开启Gitee Pages服务"></a>开启Gitee Pages服务</h2><p><img src="/2021/01/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%204.png" alt="Hexo静态博客搭建和部署/Untitled%204.png"></p><h2 id="创建公钥"><a href="#创建公钥" class="headerlink" title="创建公钥"></a>创建公钥</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;邮箱地址&quot;</span></span><br></pre></td></tr></table></figure><p>密钥对生成后默认的位置是在 C:\Users\Administrator.ssh 的目录下。</p><p>其中 id_rsa 是私钥，id_rsa.pub 是公钥。</p><p>用记事本打开并复制公钥。</p><h2 id="添加公钥"><a href="#添加公钥" class="headerlink" title="添加公钥"></a>添加公钥</h2><p><img src="/2021/01/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%205.png" alt="Hexo静态博客搭建和部署/Untitled%205.png"></p><p>将复制到的公钥粘贴进去并确定保存。</p><h2 id="安装hexo-deployer-git"><a href="#安装hexo-deployer-git" class="headerlink" title="安装hexo-deployer-git"></a>安装hexo-deployer-git</h2><ul><li>运行<code>npm install hexo-deployer-git --save</code></li></ul><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>打开D:\MyBlog\HexoBlog_config.yml查找deploy，并行修改下面这段代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line"><span class="built_in">type</span>: git</span><br><span class="line">repo: https://gitee.com/NephrenCake/NephrenCake.git</span><br><span class="line">branch: ph-pages</span><br></pre></td></tr></table></figure><h2 id="部署至云端"><a href="#部署至云端" class="headerlink" title="部署至云端"></a>部署至云端</h2><ul><li>运行<code>hexo d</code></li></ul><blockquote><p><a href="https://nephrencake.gitee.io/">https://nephrencake.gitee.io/</a> 即静态博客的地址了。</p></blockquote><ul><li>如果有网页不同步的时候<ul><li>在Gitee Pages 服务中更新部署（每次deploy之后都要手动更新）</li><li>清理浏览器缓存</li></ul></li></ul><p><img src="/2021/01/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%206.png" alt="Hexo静态博客搭建和部署/Untitled%206.png"></p>]]></content>
    
    
    <summary type="html">Hexo静态博客搭建和部署</summary>
    
    
    
    <category term="Hexo博客搭建" scheme="https://nephrencake.gitee.io/categories/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
    <category term="hexo" scheme="https://nephrencake.gitee.io/tags/hexo/"/>
    
  </entry>
  
</feed>
