<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Git基础-完结目录</title>
      <link href="/2021/03/Git%E5%9F%BA%E7%A1%80-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
      <url>/2021/03/Git%E5%9F%BA%E7%A1%80-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="Git基础——完结目录"><a href="#Git基础——完结目录" class="headerlink" title="Git基础——完结目录"></a>Git基础——完结目录</h1><p>教程视频传送门：</p><ol><li><a href="https://www.bilibili.com/video/BV1FE411P7B3?from=search&seid=14741939341268860783">【狂神说Java】Git最新教程通俗易懂</a></li><li><a href="https://www.bilibili.com/video/BV1pW411A7a5?p=15">尚硅谷GitHub基础全套完整版教程</a></li></ol><p>和Linux的一样，狂神的很精简，尚硅谷的更全。</p><table><thead><tr><th align="center"><a href="/2021/03/Git%E5%9F%BA%E7%A1%80Part1/">Part1——概述</a></th></tr></thead><tbody><tr><td align="center"><strong><a href="/2021/03/Git%E5%9F%BA%E7%A1%80Part2/">Part2——操作使用</a></strong></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Git基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git基础Part2</title>
      <link href="/2021/03/Git%E5%9F%BA%E7%A1%80Part2/"/>
      <url>/2021/03/Git%E5%9F%BA%E7%A1%80Part2/</url>
      
        <content type="html"><![CDATA[<h1 id="Git基础Part2——操作使用"><a href="#Git基础Part2——操作使用" class="headerlink" title="Git基础Part2——操作使用"></a>Git基础Part2——操作使用</h1><p>[TOC]</p><h2 id="打开命令行"><a href="#打开命令行" class="headerlink" title="打开命令行"></a>打开命令行</h2><p>在win上可以到指定位置使用Git Bash（在指定目录下-&gt;右键-&gt;Git Bash Here），个人认为比较方便。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="查看配置"><a href="#查看配置" class="headerlink" title="查看配置"></a>查看配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config -l</span><br><span class="line">git config --system --list  <span class="comment">#查看系统config</span></span><br><span class="line">git config --global  --list  <span class="comment">#查看当前用户（global）配置</span></span><br></pre></td></tr></table></figure><ol><li>Git\etc\gitconfig  ：Git 安装目录下的 gitconfig   –system 系统级</li><li>C:\Users\Administrator\ .gitconfig   只适用于当前登录用户的配置  –global 全局</li></ol><h3 id="编辑配置"><a href="#编辑配置" class="headerlink" title="编辑配置"></a>编辑配置</h3><p>每次Git提交都会使用该信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config [ --global ] user.name <span class="string">&quot;你的用户名&quot;</span>  <span class="comment"># 设置用户名</span></span><br><span class="line">git config [ --global ] user.email <span class="string">&quot;你的邮箱&quot;</span>  <span class="comment"># 设置注册github的邮箱</span></span><br></pre></td></tr></table></figure><p>使用<code>--global</code>则为全局系统级别的设置，否则为仓库级别的设置。</p><h2 id="仓库与文件"><a href="#仓库与文件" class="headerlink" title="仓库与文件"></a>仓库与文件</h2><h3 id="本地仓库搭建"><a href="#本地仓库搭建" class="headerlink" title="本地仓库搭建"></a>本地仓库搭建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init  <span class="comment"># 在当前目录新建一个Git代码库</span></span><br></pre></td></tr></table></figure><p>此命令之后目录下生成.git隐藏文件。</p><h3 id="首次配置url"><a href="#首次配置url" class="headerlink" title="首次配置url"></a>首次配置url</h3><p>建议直接 clone 远端创建好的项目，在其基础上进行开发。</p><h3 id="克隆远程仓库"><a href="#克隆远程仓库" class="headerlink" title="克隆远程仓库"></a>克隆远程仓库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> [ url ]  <span class="comment"># 克隆一个项目和它的整个代码历史(版本信息)</span></span><br></pre></td></tr></table></figure><h3 id="文件的四种状态"><a href="#文件的四种状态" class="headerlink" title="文件的四种状态"></a>文件的四种状态</h3><ul><li>Untracked: 未跟踪, 此文件在文件夹中, 但并没有加入到git库, 不参与版本控制. 通过git add 状态变为Staged.</li><li>Unmodify: 文件已经入库, 未修改, 即版本库中的文件快照内容与文件夹中完全一致. 这种类型的文件有两种去处, 如果它被修改, 而变为Modified. 如果使用git rm移出版本库, 则成为Untracked文件</li><li>Modified: 文件已修改, 仅仅是修改, 并没有进行其他的操作. 这个文件也有两个去处, 通过git add可进入暂存staged状态, 使用git checkout 则丢弃修改过, 返回到unmodify状态, 这个git checkout即从库中取出文件, 覆盖当前修改 !</li><li>Staged: 暂存状态. 执行git commit则将修改同步到库中, 这时库中的文件和本地文件又变为一致, 文件为Unmodify状态. 执行git reset HEAD filename取消暂存, 文件状态为Modified</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git status [filename]  <span class="comment">#查看指定文件状态</span></span><br><span class="line">git status  <span class="comment">#查看所有文件状态</span></span><br></pre></td></tr></table></figure><p>一般在IDE中查看即可。</p><h3 id="添加文件"><a href="#添加文件" class="headerlink" title="添加文件"></a>添加文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add fileName  <span class="comment">#指定文件</span></span><br><span class="line">git add . <span class="comment">#所有</span></span><br></pre></td></tr></table></figure><p>将工作区的文件添加到暂存区</p><h3 id="提交文件"><a href="#提交文件" class="headerlink" title="提交文件"></a>提交文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m <span class="string">&#x27;commit message&#x27;</span> [ fileName ]</span><br></pre></td></tr></table></figure><p>将暂存区内容提交到本地库</p><h3 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git rm 文件名  <span class="comment"># 删除缓存区的该文件</span></span><br><span class="line">git commit -m<span class="string">&quot;注释&quot;</span>  <span class="comment"># 将仓库文件删除</span></span><br></pre></td></tr></table></figure><h3 id="查看历史记录"><a href="#查看历史记录" class="headerlink" title="查看历史记录"></a>查看历史记录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> </span><br><span class="line">git reflog  <span class="comment"># 常用</span></span><br><span class="line">git <span class="built_in">log</span> --greph <span class="comment"># 图形显示,更直观</span></span><br><span class="line">git <span class="built_in">log</span> --pretty=oneline <span class="comment"># 漂亮一行显示(建议)</span></span><br><span class="line">git <span class="built_in">log</span> --oneline <span class="comment"># 简洁显示</span></span><br></pre></td></tr></table></figure><p>HEAD@{移动到当前版本需要多少步}</p><h3 id="前进后退"><a href="#前进后退" class="headerlink" title="前进后退"></a>前进后退</h3><p>基于索引值（推荐）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard [ commit id ]</span><br><span class="line">git reset --hard a6ace91 <span class="comment">#回到这个状态</span></span><br></pre></td></tr></table></figure><p>使用 <strong>^</strong> 符号（只能后退）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard HEAD^  <span class="comment"># 几个 ^ 表示后退几步</span></span><br></pre></td></tr></table></figure><p>使用 <strong>~</strong> 符号（只能后退）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard HEAD~n  <span class="comment"># 后退几步</span></span><br></pre></td></tr></table></figure><p>soft: </p><ul><li>仅本地库移动HEAD 指针 </li></ul><p>mixed:</p><ul><li>在本地库移动HEAD指针</li><li>重置暂存区</li></ul><p>hard:</p><ul><li>在本地库移动HEAD指针</li><li>重置暂存区</li><li>重置工作区</li></ul><h3 id="文件差异比较"><a href="#文件差异比较" class="headerlink" title="文件差异比较"></a>文件差异比较</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git diff 文件名</span><br><span class="line">git diff 哈希值 文件名  <span class="comment"># 和历史中的一个版本比较</span></span><br><span class="line">git diff  <span class="comment"># 不带文件名，则比较多个文件</span></span><br></pre></td></tr></table></figure><h3 id="忽略文件"><a href="#忽略文件" class="headerlink" title="忽略文件"></a>忽略文件</h3><p>不想把某些文件纳入版本控制中时，可以在主目录下建立”.gitignore”文件。</p><ol><li>忽略文件中的空行或以井号（#）开始的行将会被忽略。</li><li>可以使用Linux通配符。例如：星号（*）代表任意多个字符，问号（？）代表一个字符，方括号（[abc]）代表可选字符范围，大括号（{string1,string2,…}）代表可选的字符串等。</li><li>如果名称的最前面有一个感叹号（!），表示例外规则，将不被忽略。</li><li>如果名称的最前面是一个路径分隔符（/），表示要忽略的文件在此目录下，而子目录中的文件不忽略。</li><li>如果名称的最后面是一个路径分隔符（/），表示要忽略的是此目录下该名称的子目录，而非文件（默认文件或目录都忽略）。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">*.txt        <span class="comment">#忽略所有 .txt结尾的文件,这样的话上传就不会被选中！</span></span><br><span class="line">!lib.txt     <span class="comment">#但lib.txt除外</span></span><br><span class="line">/temp        <span class="comment">#仅忽略项目根目录下的TODO文件,不包括其它目录temp</span></span><br><span class="line">build/       <span class="comment">#忽略build/目录下的所有文件</span></span><br><span class="line">doc/*.txt    <span class="comment">#会忽略 doc/notes.txt 但不包括 doc/server/arch.txt</span></span><br></pre></td></tr></table></figure><p>一般可以使用JetBrain中的插件.ignore来生成。</p><h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><h3 id="分支的好处"><a href="#分支的好处" class="headerlink" title="分支的好处"></a>分支的好处</h3><ul><li>同时并行推进多个功能开发，提高开发效率</li><li>某一分支开发失败，不会对其它分支有任何影响</li></ul><p>master主分支应该非常稳定，用来发布新版本，一般情况下不允许在上面工作，工作一般情况下在新建的dev分支上工作，工作完后，比如上要发布，或者说dev分支代码稳定后可以合并到主分支master上来。</p><h3 id="分支操作"><a href="#分支操作" class="headerlink" title="分支操作"></a>分支操作</h3><ul><li>创建分支</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch [branch]  <span class="comment"># 新建一个分支，但依然停留在当前分支</span></span><br></pre></td></tr></table></figure><ul><li>查看分支</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git branch  <span class="comment"># 列出所有本地分支</span></span><br><span class="line">git branch -v </span><br><span class="line">git branch -r  <span class="comment"># 列出所有远程分支</span></span><br></pre></td></tr></table></figure><ul><li>切换分支</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout [branch]  <span class="comment"># 切换分支</span></span><br><span class="line">git checkout -b [branch]  <span class="comment"># 新建一个分支，并切换到该分支</span></span><br></pre></td></tr></table></figure><ul><li>合并分支<code>相当于把修改了的文件拉过来</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git merge [branch]  <span class="comment"># 合并指定分支到当前分支</span></span><br></pre></td></tr></table></figure><p>注意：在a分支里面修改后，要合并到master，就先切换到master，然后合并a。</p><ul><li>删除分支</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git branch -d [branch]  <span class="comment"># 删除分支，注意在删除分支时要退出该分支</span></span><br><span class="line"></span><br><span class="line">git push origin --delete [branch]</span><br><span class="line">git branch -dr [remote/branch]  <span class="comment"># 删除远程分支</span></span><br></pre></td></tr></table></figure><h3 id="解决冲突"><a href="#解决冲突" class="headerlink" title="解决冲突"></a>解决冲突</h3><ul><li>第一步：编辑，删除特殊标记<code>&lt;&lt;&lt;</code> <code>===</code></li><li>第二步：修改到满意为止，保存退出</li><li>第三步：添加到缓存区 <code>git add 文件名</code></li><li>第四步：提交到本地库 <code>git commit -m &#39;日志信息&#39;</code>  <code>注意：后面一定不能带文件名</code></li></ul>]]></content>
      
      
      <categories>
          
          <category> Git基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Git基础Part1</title>
      <link href="/2021/03/Git%E5%9F%BA%E7%A1%80Part1/"/>
      <url>/2021/03/Git%E5%9F%BA%E7%A1%80Part1/</url>
      
        <content type="html"><![CDATA[<h1 id="Git基础Part1——概述"><a href="#Git基础Part1——概述" class="headerlink" title="Git基础Part1——概述"></a>Git基础Part1——概述</h1><p>[TOC]</p><h2 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h2><p>版本控制（Revision control）是一种在开发的过程中用于管理我们对文件、目录或工程等内容的修改历史，方便查看更改历史记录，备份以便恢复以前的版本的软件工程技术。</p><ul><li>实现跨区域多人协同开发</li><li>追踪和记载一个或者多个文件的历史记录</li><li>组织和保护你的源代码和文档</li><li>统计工作量</li><li>并行开发、提高开发效率</li><li>跟踪记录整个软件的开发过程</li><li>减轻开发人员的负担，节省时间，同时降低人为错误</li></ul><p>没有进行版本控制或者版本控制本身缺乏正确的流程管理，在软件开发过程中将会引入很多问题，如软件代码的一致性、软件内容的冗余、软件过程的事物性、软件开发过程中的并发性、软件源代码的安全性，以及软件的整合等问题。</p><h2 id="常见的版本控制工具"><a href="#常见的版本控制工具" class="headerlink" title="常见的版本控制工具"></a>常见的版本控制工具</h2><ul><li><strong>Git</strong></li><li><strong>SVN</strong>（Subversion）</li><li><strong>CVS</strong>（Concurrent Versions System）</li><li><strong>VSS</strong>（Micorosoft Visual SourceSafe）</li><li><strong>TFS</strong>（Team Foundation Server）</li></ul><p>现在影响力最大且使用最广泛的是Git与SVN。</p><h2 id="版本控制分类"><a href="#版本控制分类" class="headerlink" title="版本控制分类"></a>版本控制分类</h2><p><strong>1、本地版本控制</strong></p><p>记录文件每次的更新，可以对每个版本做一个快照，或是记录补丁文件，适合个人用，如RCS。</p><p><strong>2、集中版本控制 SVN</strong></p><p>所有的版本数据都保存在服务器上，协同开发者从服务器上同步更新或上传自己的修改。</p><p>所有的版本数据都存在服务器上，用户的本地只有自己以前所同步的版本，如果不连网的话，用户就看不到历史版本，也无法切换版本验证问题，或在不同分支工作。而且，所有数据都保存在单一的服务器上，有很大的风险这个服务器会损坏，这样就会丢失所有的数据，当然可以定期备份。代表产品：SVN、CVS、VSS</p><p><strong>3、分布式版本控制 Git</strong></p><p>所有版本信息仓库全部同步到本地的每个用户，这样就可以在本地查看所有版本历史，可以离线在本地提交，只需在连网时push到相应的服务器或其他用户那里。</p><p>每个人都拥有全部的代码，是安全隐患！由于每个用户那里保存的都是所有的版本数据，只要有一个用户的设备没有问题就可以恢复所有的数据，但这增加了本地存储空间的占用。</p><h2 id="Git工作流程"><a href="#Git工作流程" class="headerlink" title="Git工作流程"></a>Git工作流程</h2><ol><li>从远程仓库中克隆 Git 资源作为本地仓库。</li><li>从本地仓库中checkout代码然后进行代码修改</li><li>在提交前先将代码提交到暂存区。</li><li>提交修改。提交到本地仓库。本地仓库中保存修改的各个历史版本。</li><li>在修改完成后，需要和团队成员共享代码时，可以将代码push到远程仓库。</li></ol><img src="/2021/03/Git%E5%9F%BA%E7%A1%80Part1/image-20210314153241990.png" alt="image-20210314153241990" style="zoom:150%;"><h2 id="fork与git-clone的区别"><a href="#fork与git-clone的区别" class="headerlink" title="fork与git clone的区别"></a>fork与git clone的区别</h2><p>1.区别</p><p>git clone 是在自己电脑直接敲命令，结果是将github仓库中的项目克隆到自己本地电脑中。</p><p>fork是直接访问github网站，在项目页面中点击fork，然后自己github项目中就会多出一个复制的项目。</p><p>2.用法</p><p>如果我们想要修改他人github项目的话，我们直接git clone代码到本地是不能pull的，所以我们使用fork，先把代码复制到自己的github仓库，然后git clone到本地修改，然后在提交pull（这里的pull是pull到自己github仓库了，我们自己的github仓库中的代码是fork源的一个分支），这时候我们想要把修改的代码提交给他人的话，就可以在自己github上pull，等其他人看到后就可以把代码做一个合并。</p>]]></content>
      
      
      <categories>
          
          <category> Git基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础-完结目录</title>
      <link href="/2021/03/Linux%E5%9F%BA%E7%A1%80-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
      <url>/2021/03/Linux%E5%9F%BA%E7%A1%80-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux基础-完结目录"><a href="#Linux基础-完结目录" class="headerlink" title="Linux基础-完结目录"></a>Linux基础-完结目录</h1><p>教程视频传送门：</p><ol><li><a href="https://www.bilibili.com/video/BV1dW411M7xL?from=search&seid=2409244161150482317">尚硅谷Linux教程(千万级学习人次，linux最新升级版)</a></li><li><a href="https://www.bilibili.com/video/BV187411y7hF?from=search&seid=7370772631947777047">【狂神说Java】Linux最通俗易懂的教程阿里云真实环境学习</a></li><li><a href="https://www.runoob.com/linux/linux-tutorial.html">Linux 教程</a></li></ol><p>狂神的比较精简，使用上差不多足够，与菜鸟教程的文本大多相同。尚硅谷的更全。</p><table><thead><tr><th align="center"><a href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part1/">Part1——概述</a></th></tr></thead><tbody><tr><td align="center"><strong><a href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part2/">Part2——基本命令</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part3/">Part3——VIM使用</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part4/">Part4——用户和用户组</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part5/">Part5——系统管理</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part6/">环境安装</a></strong></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Linux基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础Part6</title>
      <link href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part6/"/>
      <url>/2021/03/Linux%E5%9F%BA%E7%A1%80Part6/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux基础Part6——环境安装"><a href="#Linux基础Part6——环境安装" class="headerlink" title="Linux基础Part6——环境安装"></a>Linux基础Part6——环境安装</h1><p>[TOC]</p><h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><p>一般有三种方式：</p><ol><li>rpm</li><li>yum</li><li>解压缩</li></ol><h2 id="RPM-Redhat-Package-Manager"><a href="#RPM-Redhat-Package-Manager" class="headerlink" title="RPM(Redhat Package Manager)"></a>RPM(Redhat Package Manager)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rpm –qa | grep xx  <span class="comment"># 查询安装列表</span></span><br><span class="line">rpm -qi 软件包名  <span class="comment"># 查询软件包信息</span></span><br><span class="line">rpm -qf 文件全路径名  <span class="comment"># 查询文件所属的软件包</span></span><br><span class="line">rpm -e [ --nodeps ] 包名  <span class="comment"># rpm包卸载</span></span><br><span class="line">rpm -ivh RPM 包全路径名称</span><br><span class="line">rpm -ivh 包名  <span class="comment"># 安装</span></span><br></pre></td></tr></table></figure><blockquote><p>使用rpm安装jdk不需要配置环境变量，只需要重启就好了</p></blockquote><h2 id="YUM"><a href="#YUM" class="headerlink" title="YUM"></a>YUM</h2><p>Yum 是一个 Shell 前端软件包管理器。基于 RPM 包管理，能够从指定的服务器自动下载 RPM 包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包。使用 yum 的前提是可以联网。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum [ options ] [ <span class="built_in">command</span> ] [ package ... ]</span><br></pre></td></tr></table></figure><ul><li>列出所有可更新的软件清单命令：<strong>yum check-update</strong> </li><li>更新所有软件命令：<strong>yum update</strong></li><li>仅安装指定的软件命令：<strong>yum install <package_name></package_name></strong></li><li>仅更新指定的软件命令：<strong>yum update <package_name></package_name></strong></li><li>列出所有可安裝的软件清单命令：<strong>yum list</strong></li><li>删除软件包命令：<strong>yum remove <package_name></package_name></strong></li><li>查找软件包命令：**yum search <key_word> **</key_word></li><li>清除缓存命令:<ul><li><strong>yum clean packages</strong>: 清除缓存目录下的软件包</li><li><strong>yum clean headers</strong>: 清除缓存目录下的 headers</li><li><strong>yum clean oldheaders</strong>: 清除缓存目录下旧的 headers</li><li><strong>yum clean, yum clean all (= yum clean packages; yum clean oldheaders)</strong> :清除缓存目录下的软件包及旧的 headers</li></ul></li><li>-y：自动确认所有提示</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础Part5</title>
      <link href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part5/"/>
      <url>/2021/03/Linux%E5%9F%BA%E7%A1%80Part5/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux基础Part5——系统管理"><a href="#Linux基础Part5——系统管理" class="headerlink" title="Linux基础Part5——系统管理"></a>Linux基础Part5——系统管理</h1><p>[TOC]</p><h2 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h2><p>任务调度：是指系统在某个时间执行的特定的命令或程序。</p><ol><li><p>系统工作：有些重要的工作必须周而复始地执行。如病毒扫描等</p><p>设置任务调度文件：/etc/crontab</p></li><li><p>用户工作：个别用户可能希望执行某些程序，比如对 mysql 数据库的备份</p><p>执行 <code>crontab –e</code> 命令，输入任务到调度文件</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crontab [ -erl ]</span><br><span class="line">service crond restart：[ 重启任务调度 ]</span><br></pre></td></tr></table></figure><ul><li>-e：编辑定时crontab任务</li><li>-l：查询crontab任务</li><li>-r：删除当前用户所有的crontab任务</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*/1 * * * * ls –l /etc/ &gt; /tmp/to.txt  <span class="comment"># 每小时的每分钟执行 ls –l /etc/ &gt; /tmp/to.txt 命令</span></span><br></pre></td></tr></table></figure><ul><li>第1个*：一小时中的第几分钟。0~59。</li><li>第2个*：一天中的第几小时。0~23。</li><li>第3个*：一个月中的第几天。1~31。</li><li>第4个*：一年中的第几月。1~12。</li><li>第5个*：一周中的星期几。0~7。(0、7都表示周日)</li><li>*：代表任何时间</li><li>,：代表不连续的时间</li><li>-：代表连续的时间范围</li><li>*/n：代表每隔多久执行一次</li></ul><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">先编写一个文件 /home/mytask1.sh</span><br><span class="line">date &gt;&gt; /tmp/mydate</span><br><span class="line">给 mytask1.sh 一个可以执行权限chmod 744 /home/mytask1.sh</span><br><span class="line">crontab -e</span><br><span class="line">*/1 * * * * /home/mytask1.sh</span><br><span class="line"></span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># vim task1.sh  # 先编写一个shell脚本</span></span><br><span class="line">date &gt;&gt; /tmp/mydate</span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># chmod 744 task1.sh  # 给脚本一个可执行权限</span></span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># crontab -e  # 编辑任务</span></span><br><span class="line">*/1 * * * * /home/mytask1.sh</span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># cd /tmp/</span></span><br><span class="line">[root@VM-1-14-centos tmp]<span class="comment"># cat mydate  # 查看输出</span></span><br><span class="line">Sat Mar 13 13:35:01 CST 2021</span><br><span class="line">Sat Mar 13 13:36:01 CST 2021</span><br><span class="line">Sat Mar 13 13:37:01 CST 2021</span><br></pre></td></tr></table></figure><h2 id="磁盘管理"><a href="#磁盘管理" class="headerlink" title="磁盘管理"></a>磁盘管理</h2><h3 id="分区基础知识"><a href="#分区基础知识" class="headerlink" title="分区基础知识"></a>分区基础知识</h3><p>分区的方式：</p><ol><li>mbr 分区:<ol><li>最多支持四个主分区</li><li>系统只能安装在主分区</li><li>扩展分区要占一个主分区</li><li>MBR 最大只支持 2TB，但拥有最好的兼容性</li></ol></li><li>gpt 分区:<ol><li>支持无限多个主分区（但操作系统可能限制，比如 windows 下最多 128 个分区）</li><li>最大支持 18EB 的大容量（1EB=1024 PB，1PB=1024 TB ）</li><li>windows7 64 位以后支持 gpt</li></ol></li></ol><h3 id="Linux分区"><a href="#Linux分区" class="headerlink" title="Linux分区"></a>Linux分区</h3><ol><li>原理：<ol><li>Linux 来说无论有几个分区，分给哪一目录使用，它归根结底就只有一个根目录，一个独立且唯一的文件结构 , Linux 中每个分区都是用来组成整个文件系统的一部分。</li><li>Linux 采用了一种叫“载入”的处理方法，它的整个文件系统中包含了一整套的文件和目录， 且将一个分区和一个目录联系起来。这时要载入的一个分区将使它的存储空间在一个目录下获得。</li></ol></li><li>硬盘说明：<ol><li>Linux 硬盘分 IDE 硬盘和 SCSI 硬盘，目前基本上是 SCSI 硬盘</li><li>对于 IDE 硬盘，驱动器标识符为“hdx<del>”,其中“hd”表明分区所在设备的类型，这里是指 IDE 硬盘了。“x”为盘号（a 为基本盘，b 为基本从属盘，c 为辅助主盘，d 为辅助从属盘）,“</del>”代表分区，前四个分区用数字 1 到 4 表示，它们是主分区或扩展分区，从 5 开始就是逻辑分区。例，hda3 表示为第一个 IDE 硬盘上的第三个主分区或扩展分区,hdb2 表示为第二个 IDE 硬盘上的第二个主分区或扩展分区。</li><li>对于 SCSI 硬盘则标识为“sdx~”，SCSI 硬盘是用“sd”来表示分区所在设备的类型的，其余则和 IDE 硬盘的表示方法一样</li></ol></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos tmp]<span class="comment"># lsblk  # 老色比裂开 查看系统分区和挂载情况</span></span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sr0     11:0    1  111M  0 rom              <span class="comment"># 光驱</span></span><br><span class="line">vda    253:0    0   50G  0 disk             <span class="comment"># 硬盘</span></span><br><span class="line">└─vda1 253:1    0   50G  0 part /           <span class="comment"># 分区1</span></span><br><span class="line">[root@VM-1-14-centos tmp]<span class="comment"># lsblk -f</span></span><br><span class="line">NAME   FSTYPE  LABEL    UUID                                 MOUNTPOINT</span><br><span class="line">sr0    iso9660 config-2 2021-03-06-19-49-34-00               </span><br><span class="line">vda                                                          </span><br><span class="line">└─vda1 ext4             4b499d76-769a-40a0-93dc-4a31a59add28 /</span><br><span class="line"><span class="comment"># 分区名 分区类型          格式化产生的40位唯一标识分区id         挂载点</span></span><br></pre></td></tr></table></figure><h3 id="在虚拟机上挂载一块硬盘"><a href="#在虚拟机上挂载一块硬盘" class="headerlink" title="在虚拟机上挂载一块硬盘"></a>在虚拟机上挂载一块硬盘</h3><ol><li><p><strong>虚拟机添加硬盘</strong></p></li><li><p><strong>分区</strong> <code>fdisk /dev/sdb</code></p></li><li><p><strong>格式化</strong> <code>mkfs -t ext4 /dev/sdb1</code></p></li><li><p><strong>临时挂载</strong><br>先创建目录 <code>/home/newdisk</code><br>挂载指令：<code>mount /dev/sdb1 /home/newdisk</code><br>卸载指令：<code>umount 挂载目录或硬盘目录</code></p></li><li><p><strong>永久挂载</strong><br>修改设置：<code>vim /etc/fstab</code><br>添加一行：<code>/dev/sdb1 /home/newdisk ext4 defaults 0 0</code></p><p>生效：<code>mount -a</code></p></li></ol><h2 id="磁盘查询"><a href="#磁盘查询" class="headerlink" title="磁盘查询"></a>磁盘查询</h2><h3 id="df-：查询系统磁盘总体占用情况"><a href="#df-：查询系统磁盘总体占用情况" class="headerlink" title="df ：查询系统磁盘总体占用情况"></a>df ：查询系统磁盘总体占用情况</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df [ -ahikHTm ] [ 目录或文件名 ]</span><br></pre></td></tr></table></figure><ul><li>-a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统</li><li>-h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示</li><li>-H ：以 M=1000K 取代 M=1024K 的进位方式</li><li>-T ：显示文件系统类型, 连同该 partition 的 filesystem 名称 (例如 ext3) 也列出</li><li>-i ：不用硬盘容量，而以 inode 的数量来显示</li><li>[ 目录或文件名 ]：指定挂载目录</li></ul><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos tmp]<span class="comment"># df -h</span></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">devtmpfs        3.9G     0  3.9G   0% /dev</span><br><span class="line">tmpfs           3.9G   24K  3.9G   1% /dev/shm</span><br><span class="line">tmpfs           3.9G  428K  3.9G   1% /run</span><br><span class="line">tmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup</span><br><span class="line">/dev/vda1        50G  3.3G   44G   8% /</span><br><span class="line">tmpfs           783M     0  783M   0% /run/user/0</span><br></pre></td></tr></table></figure><h3 id="du：查询指定目录的磁盘占用情况"><a href="#du：查询指定目录的磁盘占用情况" class="headerlink" title="du：查询指定目录的磁盘占用情况"></a>du：查询指定目录的磁盘占用情况</h3><p>与 df 不一样的是，du 这个命令会直接到文件系统内去搜寻所有的文件数据。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">du [ -ahskm ] 文件或目录名称</span><br></pre></td></tr></table></figure><ul><li>-a ：列出所有的文件与目录容量，因为默认仅统计目录底下的文件夹而已</li><li>-h ：以人们较易读的容量格式 (G/M) 显示</li><li>-s ：列出总量而已，而不列出每个各别的目录占用容量</li><li>-S ：不包括子目录下的总计，与 -s 有点差别</li><li>–max-depth=1 子目录深度</li></ul><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos tmp]<span class="comment"># du -h  # 默认为当前目录下文件夹(包含隐藏文件夹)的磁盘占用情况</span></span><br><span class="line">4.0K./.XIM-unix</span><br><span class="line">4.0K./.font-unix</span><br><span class="line">4.0K./.X11-unix</span><br><span class="line">4.0K./systemd-private-93e24fc7bc744033a007e9d37393f18d-ntpd.service-qBqeOq/tmp</span><br><span class="line">8.0K./systemd-private-93e24fc7bc744033a007e9d37393f18d-ntpd.service-qBqeOq</span><br><span class="line">4.0K./.ICE-unix</span><br><span class="line">13M./d0ba2854fb7784799d5cfe6838f4f3a6</span><br><span class="line">4.0K./.Test-unix</span><br><span class="line">13M.                              <span class="comment"># 当前目录下文件夹的总量</span></span><br><span class="line">[root@VM-1-14-centos tmp]<span class="comment"># du -ach</span></span><br><span class="line">4.0K./virtio_blk_affinity.log</span><br><span class="line">4.0K./.XIM-unix</span><br><span class="line">......</span><br><span class="line">4.0K./.Test-unix</span><br><span class="line">13M.</span><br><span class="line">13Mtotal</span><br></pre></td></tr></table></figure><h3 id="实用指令"><a href="#实用指令" class="headerlink" title="实用指令"></a>实用指令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos ~]<span class="comment"># ls -l | grep &quot;^d&quot; | wc -l  # 统计当前目录下文件夹数量</span></span><br><span class="line">2</span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># ls -l | grep &quot;^-&quot; | wc -l  # 统计当前目录下文件数量</span></span><br><span class="line">5</span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># ls -lr | grep &quot;^-&quot; | wc -l  # 递归统计当前目录下文件数量</span></span><br><span class="line">5</span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># yum install tree  # 安装tree</span></span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># tree /  # 以树状显示目录结构</span></span><br></pre></td></tr></table></figure><h2 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos ~]<span class="comment"># ifconfig  # 显示或设置网络设备</span></span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># ping www.baidu.com  # 测试网络连通</span></span><br></pre></td></tr></table></figure><p><a href="https://www.bilibili.com/video/BV1dW411M7xL?p=46&spm_id_from=pageDriver">主要是虚拟机用</a></p><h2 id="线程管理"><a href="#线程管理" class="headerlink" title="线程管理"></a>线程管理</h2><h3 id="进程的基本介绍"><a href="#进程的基本介绍" class="headerlink" title="进程的基本介绍"></a>进程的基本介绍</h3><ol><li>在 LINUX 中，每个执行的程序（代码）都称为一个进程。每一个进程都分配一个 ID 号（进程号PID）。</li><li>每一个进程，都会对应一个父进程（父进程号PPID），而这个父进程可以复制多个子进程例如 www 服务器。</li><li>每个进程都可能以两种方式存在的。前台与后台，所谓前台进程就是用户目前的屏幕上可以进行操作的。后台进程（守护进程）则是实际在操作，但屏幕上无法看到的进程。</li><li>一般系统的服务都是以后台进程的方式存在，而且都会常驻在系统中，直到关机才才结束。</li></ol><h3 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps [ -aux ]</span><br></pre></td></tr></table></figure><ul><li>-a：显示当前终端所有进程信息</li><li>-u：以用户的格式显示进程</li><li>-x：显示后台进程运行的参数</li><li>-e：显示所有进程</li><li>-f：全格式</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos ~]<span class="comment"># ps -aux | grep xxx  # 查看是否有xxx进程</span></span><br><span class="line">root     16906  0.0  0.0 112812   972 pts/0    S+   19:19   0:00 grep --color=auto xxx</span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># ps -aux</span></span><br><span class="line">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">root         1  0.0  0.0 191036  4036 ?        Ss   Mar06   0:59 /usr/lib/systemd/systemd --switched-root --system --dese</span><br><span class="line">......</span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># ps -ef</span></span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">root         1     0  0 Mar06 ?        00:01:00 /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">......</span><br></pre></td></tr></table></figure><ul><li><p>USER：用户名称</p></li><li><p>PID：进程号</p></li><li><p>%CPU：进程占用 CPU 的百分比</p></li><li><p>%MEM：进程占用物理内存的百分比</p></li><li><p>VSZ：进程占用的虚拟内存大小（单位：KB）</p></li><li><p>RSS：进程占用的物理内存大小（单位：KB）</p></li><li><p>TTY：终端名称</p></li><li><p>STAT：进程状态，其中 S-睡眠，s-表示该进程是会话的先导进程，N-表示进程拥有比普通优先级更低的优先级，R-正在运行，D-短期等待，Z-僵死进程，T-被跟踪或者被停止等等</p></li><li><p>START：进程的启动时间</p></li><li><p>TIME：CPU 时间，即进程使用 CPU 的总时间</p></li><li><p>COMMAND：启动进程所用的命令和参数，如果过长会被截断显示</p></li><li><p>C：CPU 用于计算执行优先级的因子。数值越大，表明进程是 CPU 密集型运算，执行优先级会降低；数值越小，表明进程是 I/O 密集型运算，执行优先级会提高</p></li><li><p>STIME：进程启动的时间</p></li><li><p>TTY：完整的终端名称</p></li><li><p>TIME：CPU 时间</p></li><li><p>CMD：启动进程所用的命令和参数</p></li></ul><h3 id="结束进程"><a href="#结束进程" class="headerlink" title="结束进程"></a>结束进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> [ -9 ] 进程号</span><br><span class="line">killall 进程名</span><br></pre></td></tr></table></figure><ul><li>9：强制终止</li></ul><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos ~]<span class="comment"># kill 1590  # 结束/usr/sbin/sshd进程，终止远程登录服务</span></span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># killall gedit  # 结束所有gedit进程</span></span><br></pre></td></tr></table></figure><h3 id="进程树"><a href="#进程树" class="headerlink" title="进程树"></a>进程树</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pstree [ -pu ]</span><br></pre></td></tr></table></figure><ul><li>-p :显示进程的 PID</li><li>-u :显示进程的所属用户</li></ul><h3 id="服务（守护进程）管理"><a href="#服务（守护进程）管理" class="headerlink" title="服务（守护进程）管理"></a>服务（守护进程）管理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service 服务名 [ start | stop | restart | reload | status ]</span><br></pre></td></tr></table></figure><h3 id="查看服务列表"><a href="#查看服务列表" class="headerlink" title="查看服务列表"></a>查看服务列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -l /etc/init.d/</span><br></pre></td></tr></table></figure><h3 id="chkconfig指令"><a href="#chkconfig指令" class="headerlink" title="chkconfig指令"></a>chkconfig指令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chkconfig --list | grep 服务名</span><br><span class="line">chkconfig --level n 服务名 on/off</span><br></pre></td></tr></table></figure><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  案例 1： 请显示当前系统所有服务的各个运行级别的运行状态</span></span><br><span class="line">chkconfig --list</span><br><span class="line"><span class="comment">#  案例 2 ：请查看 sshd 服务的运行状态</span></span><br><span class="line">service sshd status</span><br><span class="line"><span class="comment">#  案例 3： 将 sshd 服务在运行级别 5 下设置为不自动启动</span></span><br><span class="line">chkconfig --level 5 sshd off</span><br><span class="line"><span class="comment">#  案例 4： 当运行级别为 5 时，关闭防火墙。</span></span><br><span class="line">chkconfig --level 5 iptables off</span><br><span class="line"><span class="comment">#  案例 5： 在所有运行级别下，关闭防火墙</span></span><br><span class="line">chkconfig iptables off</span><br><span class="line"><span class="comment">#  案例 6： 在所有运行级别下，开启防火墙</span></span><br><span class="line">chkconfig iptables on</span><br></pre></td></tr></table></figure><h3 id="动态监控进程"><a href="#动态监控进程" class="headerlink" title="动态监控进程"></a>动态监控进程</h3><p>top 与 ps 命令很相似，都用来显示正在执行的进程。但不同在于 top 在执行一段时间可以更新正在运行的的进程。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top [ -dip ]</span><br></pre></td></tr></table></figure><ul><li>-d：秒数，指定top命令间隔多少秒更新，默认3秒</li><li>-i：使top不显示闲置或僵死进程</li><li>-p：通过指定进程PID来显示相应进程状态</li></ul><p>交互操作：</p><ul><li>P：以CPU使用率排序（默认此项）</li><li>M：以内存使用率来排序</li><li>N：以PID排序</li><li>q：退出top</li><li>k：杀死进程</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">top - 21:15:35 up 7 days,  1:25,  1 user,  load average: 0.00, 0.02, 0.05        <span class="comment"># 当前时间 运行时间 登录用户数 负载均衡</span></span><br><span class="line">Tasks:  93 total,   2 running,  91 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  0.3 us,  0.2 sy,  0.0 ni, 99.5 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st  <span class="comment"># cpu使用情况</span></span><br><span class="line">KiB Mem :  8009084 total,  5870600 free,   217104 used,  1921380 buff/cache      <span class="comment"># 内存使用情况</span></span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.  7481244 avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                             </span><br><span class="line"> 7164 root      20   0  161996   2176   1552 R   0.3  0.0   0:00.16 top                                                 </span><br><span class="line">24146 root      20   0  160712   9412   2020 S   0.3  0.1   4:59.89 barad_agent                                         </span><br><span class="line">24147 root      20   0  607352  15888   2200 S   0.3  0.2  25:44.34 barad_agent  </span><br><span class="line">......</span><br></pre></td></tr></table></figure><h3 id="查看网络状态"><a href="#查看网络状态" class="headerlink" title="查看网络状态"></a>查看网络状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat [ -anp ]</span><br></pre></td></tr></table></figure><ul><li>-an 按一定顺序排列输出</li><li>-p 显示哪个进程在调用</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos ~]<span class="comment"># netstat -anp | grep sshd</span></span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1590/sshd           </span><br><span class="line">tcp        0     52 172.17.1.14:22          114.222.3.200:9702      ESTABLISHED 4553/sshd: root@pts </span><br><span class="line">unix  3      [ ]         STREAM     CONNECTED     17005    1590/sshd            </span><br><span class="line">unix  2      [ ]         DGRAM                    11566495 4553/sshd: root@pts  </span><br></pre></td></tr></table></figure><h2 id="指定运行级别"><a href="#指定运行级别" class="headerlink" title="指定运行级别"></a>指定运行级别</h2><p>运行级别说明：</p><ol start="0"><li>关机</li><li>单用户(在实体机上开机时进入单用户模式，找回root密码)</li><li>多用户无网络</li><li>多用户有网络</li><li>保留级别</li><li>图形界面</li><li>系统重启</li></ol><p>修改默认运行级别可以修改文件 <code>/etc/inittab</code> 中的 <code>id:5:initdefault: </code></p><p>或者使用命令：<code>init [ 0123456 ]</code></p><p>开机流程：</p><ol><li>开机</li><li>BIOS</li><li>/boot</li><li>init进程1</li><li>运行级别</li><li>运行级别对应的服务</li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础Part4</title>
      <link href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part4/"/>
      <url>/2021/03/Linux%E5%9F%BA%E7%A1%80Part4/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux基础Part4——用户和用户组"><a href="#Linux基础Part4——用户和用户组" class="headerlink" title="Linux基础Part4——用户和用户组"></a>Linux基础Part4——用户和用户组</h1><p>[TOC]</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。</p><p>用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。</p><p>每个用户账号都拥有一个唯一的用户名和各自的口令。</p><p>用户在登录时键入正确的用户名和口令后，就能够进入系统和自己的主目录。</p><p>实现用户账号的管理，要完成的工作主要有如下几个方面：</p><ul><li>用户账号的添加、删除与修改。</li><li>用户口令的管理。</li><li>用户组的管理。</li></ul><h2 id="用户管理"><a href="#用户管理" class="headerlink" title="用户管理"></a>用户管理</h2><h3 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd [ -cdgGsu ] 用户名</span><br></pre></td></tr></table></figure><ul><li>-c comment 指定一段注释性描述</li><li>-d 目录 指定用户主目录，如果此目录不存在，则一并创建</li><li>-g 用户组 指定用户所属的用户组 <code>useradd -g group user</code></li><li>-G 用户组，用户组 指定用户所属的附加组(SentOS每个用户只有一个附加组)</li><li>-s Shell文件 指定用户的登录Shell</li><li>-u 用户号 指定用户的用户号，如果同时有-o选项，则可以重复使用其他用户的标识号</li></ul><p>如果不使用参数，则默认在 /home 新建一个  /home/用户名 ，注意不要提前自己新建目录，因为还需要生成一系列隐藏文件。</p><h3 id="口令管理"><a href="#口令管理" class="headerlink" title="口令管理"></a>口令管理</h3><p>用户账号刚创建时没有口令，被系统锁定，无法使用，必须为其指定口令后才可以使用，即使是指定空口令。</p><p>指定和修改用户口令的Shell命令是<code>passwd</code>。超级用户可以为自己和其他用户指定口令，普通用户只能用它修改自己的口令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd [ -ludf ] 用户名</span><br></pre></td></tr></table></figure><ul><li>-l 锁定口令，即禁用账号</li><li>-u 口令解锁</li><li>-d 使账号无口令</li><li>-f 强迫用户下次登录时修改口令</li></ul><p>如果默认用户名，则修改当前用户的口令</p><p>普通用户修改自己的口令时，要求先输入原口令；而超级用户为用户指定口令时，就不需要原口令。</p><h3 id="删除用户"><a href="#删除用户" class="headerlink" title="删除用户"></a>删除用户</h3><p>如果一个用户的账号不再使用，可以从系统中删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">userdel [ -r ] 用户名</span><br></pre></td></tr></table></figure><ul><li> -r 把用户相关文件一起删除，包括用户在系统文件中（主要是/etc/passwd, /etc/shadow, /etc/group等）的记录，同时删除用户的主目录。</li></ul><p>一般公司员工离职会删除用户但保存家目录，因为其贡献的代码都在家目录中。</p><h3 id="修改用户"><a href="#修改用户" class="headerlink" title="修改用户"></a>修改用户</h3><p>修改用户账号就是根据实际情况更改用户的有关属性，如用户号、主目录、用户组、登录Shell等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usermod [ -cdgGsu ] 用户名</span><br></pre></td></tr></table></figure><p>常用的选项包括 -c, -d, -m, -g, -G, -s, -u, -o 等，这些选项的意义同 <code>useradd</code> ，可以为用户指定新的资源值。</p><p>另外，有些系统可以使用选项：-l 将原来的用户名改为新的用户名。</p><h3 id="查询用户"><a href="#查询用户" class="headerlink" title="查询用户"></a>查询用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">id 用户名</span><br></pre></td></tr></table></figure><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos tt]<span class="comment"># id root</span></span><br><span class="line">uid=0(root) gid=0(root) groups=0(root)</span><br><span class="line">[root@VM-1-14-centos tt]<span class="comment"># id xu</span></span><br><span class="line">id: xu: no such user</span><br></pre></td></tr></table></figure><h3 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su 用户名</span><br></pre></td></tr></table></figure><p>高权限用户到低权限用户不需要输入密码，反之需要</p><p>可以通过 <code>exit</code> 返回原用户</p><h3 id="查看当前用户"><a href="#查看当前用户" class="headerlink" title="查看当前用户"></a>查看当前用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">whoami</span><br></pre></td></tr></table></figure><p>返回当前登录用户名</p><h2 id="用户组管理"><a href="#用户组管理" class="headerlink" title="用户组管理"></a>用户组管理</h2><p>每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。</p><p>组的增加、删除和修改实际上就是对/etc/group文件的更新。</p><h3 id="新增用户组"><a href="#新增用户组" class="headerlink" title="新增用户组"></a>新增用户组</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupadd [ -go ] 用户组</span><br></pre></td></tr></table></figure><ul><li>-g GID 指定新用户组的组标识号（GID）。</li><li>-o 一般与-g选项同时使用，表示新用户组的GID可以与系统已有用户组的GID相同。</li></ul><p>不指定GID则默认在当前已有的最大组标识号的基础上加1。</p><h3 id="删除用户组"><a href="#删除用户组" class="headerlink" title="删除用户组"></a>删除用户组</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupdel 用户组</span><br></pre></td></tr></table></figure><h3 id="修改用户组属性"><a href="#修改用户组属性" class="headerlink" title="修改用户组属性"></a>修改用户组属性</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupmod [ -gon ] 用户组</span><br></pre></td></tr></table></figure><ul><li>-g GID 为用户组指定新的组标识号。</li><li>-o 与-g选项同时使用，用户组的新GID可以与系统已有用户组的GID相同。</li><li>-n新用户组 将用户组的名字改为新名字</li></ul><p>demo；</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupmod –g 10000 -n group3 group2  <span class="comment"># 将组group2的标识号改为10000，组名修改为group3</span></span><br></pre></td></tr></table></figure><h3 id="切换用户组"><a href="#切换用户组" class="headerlink" title="切换用户组"></a>切换用户组</h3><p>如果一个用户同时属于多个用户组，那么用户可以在用户组之间切换，以便具有其他用户组的权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">newgrp 用户组</span><br></pre></td></tr></table></figure><h2 id="用户和用户组的系统文件"><a href="#用户和用户组的系统文件" class="headerlink" title="用户和用户组的系统文件"></a>用户和用户组的系统文件</h2><p>管理用户和用户组实际上都是对有关的系统文件进行修改，包括/etc/passwd, /etc/shadow, /etc/group等。</p><h3 id="etc-passwd"><a href="#etc-passwd" class="headerlink" title="/etc/passwd"></a>/etc/passwd</h3><p>Linux系统中的每个用户都在/etc/passwd文件中有一个对应的记录行，它记录了这个用户的一些基本属性。</p><p>这个文件对所有用户都是可读的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos home]<span class="comment"># cat /etc/passwd</span></span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell</span><br></pre></td></tr></table></figure><h4 id="用户名"><a href="#用户名" class="headerlink" title="用户名"></a>用户名</h4><p>“用户名”是代表用户账号的字符串。通常长度不超过8个字符，并且由大小写字母和/或数字组成。</p><p>登录名中不能有冒号(:)，因为冒号在这里是分隔符。为了兼容起见，登录名中最好不要包含点字符(.)，并且不使用连字符(-)和加号(+)打头。</p><h4 id="口令"><a href="#口令" class="headerlink" title="口令"></a>口令</h4><p>虽然这个字段存放的只是用户口令的加密串，不是明文，但是由于/etc/passwd文件对所有用户都可读，所以这仍是一个安全隐患。</p><p>因此，现在许多Linux 系统都使用了shadow技术，把真正的加密后的用户口令字存放到/etc/shadow文件中，而在/etc/passwd文件的口令字段中只存放一个特殊的字符，例如“x”或者“*”。</p><h4 id="用户标识号"><a href="#用户标识号" class="headerlink" title="用户标识号"></a>用户标识号</h4><p>一般情况下它与用户名是一一对应的。如果几个用户名对应的用户标识号是一样的，系统内部将把它们视为同一个用户，但是它们可以有不同的口令、不同的主目录以及不同的登录Shell等。</p><p>通常用户标识号的取值范围是0～65 535。0是超级用户root的标识号，1～99由系统保留，作为管理账号，普通用户的标识号从100开始。在Linux系统中，这个界限是500。</p><h4 id="组标识号"><a href="#组标识号" class="headerlink" title="组标识号"></a>组标识号</h4><p>它对应着/etc/group文件中的一条记录。</p><h4 id="注释性描述"><a href="#注释性描述" class="headerlink" title="注释性描述"></a>注释性描述</h4><p>例如用户的真实姓名、电话、地址等，这个字段并没有什么实际的用途。格式也没有统一。在许多Linux系统中，用做finger命令的输出。没有就为空。</p><h4 id="主目录"><a href="#主目录" class="headerlink" title="主目录"></a>主目录</h4><p>是用户在登录到系统之后所处的家目录。各用户的主目录都被放在 /home 下，通常来说用户主目录的名称就是该用户的登录名。</p><p>各用户对自己的主目录有读、写、执行权限，其他用户对此目录的访问权限则根据具体情况设置。</p><h4 id="登录Shell"><a href="#登录Shell" class="headerlink" title="登录Shell"></a>登录Shell</h4><p>Shell是用户与Linux系统之间的接口。Linux的Shell有许多种，每种都有不同的特点。常用的有sh(Bourne Shell), csh(C Shell), ksh(Korn Shell), tcsh(TENEX/TOPS-20 type C Shell), bash(Bourne Again Shell)等。</p><p>系统管理员可以根据系统情况和用户习惯为用户指定某个Shell。如果不指定Shell，那么系统使用sh为默认的登录Shell，即这个字段的值为/bin/sh。</p><p>用户的登录Shell也可以指定为某个特定的程序（此程序不是一个命令解释器）。</p><p>利用这一特点，我们可以限制用户只能运行指定的应用程序，在该应用程序运行结束后，用户就自动退出了系统。有些Linux 系统要求只有那些在系统中登记了的程序才能出现在这个字段中。</p><h4 id="伪用户-pseudo-users"><a href="#伪用户-pseudo-users" class="headerlink" title="伪用户(pseudo users)"></a>伪用户(pseudo users)</h4><p>这些用户在/etc/passwd文件中也占有一条记录，但是不能登录，因为它们的登录Shell为空。它们的存在主要是方便系统管理，满足相应的系统进程对文件属主的要求。</p><ul><li>bin 拥有可执行的用户命令文件 </li><li>sys 拥有系统文件 </li><li>adm 拥有帐户文件 </li><li>uucp UUCP使用 </li><li>lp lp或lpd子系统使用 </li><li>nobody NFS使用</li></ul><h3 id="etc-shadow"><a href="#etc-shadow" class="headerlink" title="/etc/shadow"></a>/etc/shadow</h3><p>与/etc/passwd记录一一对应，由pwconv命令根据/etc/passwd中的数据自动产生。但只有超级用户才拥有该文件读权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos home]<span class="comment"># cat /etc/shadow</span></span><br><span class="line">root:$1$ltOhBJKQ<span class="variable">$X</span>/KwY82TfyngQn/GVxk..1:18692:0:99999:7:::</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志</span><br></pre></td></tr></table></figure><ol><li>“登录名”是与/etc/passwd文件中的登录名相一致的用户账号</li><li>“口令”字段存放的是加密后的用户口令字，长度为13个字符。如果为空，则对应用户没有口令，登录时不需要口令；如果含有不属于集合 { ./0-9A-Za-z }中的字符，则对应的用户不能登录。</li><li>“最后一次修改时间”表示的是从某个时刻起，到用户最后一次修改口令时的天数。时间起点对不同的系统可能不一样。</li><li>“最小时间间隔”指的是两次修改口令之间所需的最小天数。</li><li>“最大时间间隔”指的是口令保持有效的最大天数。</li><li>“警告时间”字段表示的是从系统开始警告用户到用户密码正式失效之间的天数。</li><li>“不活动时间”表示的是用户没有登录活动但账号仍能保持有效的最大天数。</li><li>“失效时间”字段给出的是一个绝对的天数，如果使用了这个字段，那么就给出相应账号的生存期。期满后，该账号就不再是一个合法的账号，也就不能再用来登录了。</li></ol><h3 id="etc-group"><a href="#etc-group" class="headerlink" title="/etc/group"></a>/etc/group</h3><p>用户组的所有信息都存放在 /etc/group 文件中。</p><p>将用户分组是 Linux 系统中对用户进行管理及控制访问权限的一种手段。</p><p>每个用户都属于某个用户组；一个组中可以有多个用户，一个用户也可以属于不同的组。</p><p>当一个用户同时是多个组中的成员时，在 /etc/passwd 文件中记录的是用户所属的主组，也就是登录时所属的默认组，而其他组称为附加组。</p><p>用户要访问属于附加组的文件时，必须首先使用 <code>newgrp</code> 命令使自己成为所要访问的组中的成员。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos home]<span class="comment"># cat /etc/shadow</span></span><br><span class="line">root:$1$ltOhBJKQ<span class="variable">$X</span>/KwY82TfyngQn/GVxk..1:18692:0:99999:7:::</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">组名:口令:组标识号:组内用户列表</span><br></pre></td></tr></table></figure><ol><li>“组名”是用户组的名称，由字母或数字构成。与/etc/passwd中的登录名一样，组名不应重复。</li><li>“口令”字段存放的是用户组加密后的口令字。一般Linux 系统的用户组都没有口令，即这个字段一般为空，或者是*。</li><li>“组标识号”与用户标识号类似，也是一个整数，被系统内部用来标识组。</li><li>“组内用户列表”是属于这个组的所有用户的列表，不同用户之间用逗号(,)分隔。这个用户组可能是用户的主组，也可能是附加组。</li></ol><p>口令和主组内用户列表一般是看不到的</p><h2 id="添加批量用户"><a href="#添加批量用户" class="headerlink" title="添加批量用户"></a>添加批量用户</h2><h3 id="（1）先编辑一个文本用户文件"><a href="#（1）先编辑一个文本用户文件" class="headerlink" title="（1）先编辑一个文本用户文件"></a>（1）先编辑一个文本用户文件</h3><p>每一列按照<code>/etc/passwd</code>密码文件的格式书写，要注意每个用户的用户名、UID、宿主目录都不可以相同，其中密码栏可以留做空白或输入x号。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">user001::600:100:user:/home/user001:/bin/bash</span><br><span class="line">user002::601:100:user:/home/user002:/bin/bash</span><br><span class="line">user003::602:100:user:/home/user003:/bin/bash</span><br><span class="line">user004::603:100:user:/home/user004:/bin/bash</span><br><span class="line">user005::604:100:user:/home/user005:/bin/bash</span><br><span class="line">user006::605:100:user:/home/user006:/bin/bash</span><br></pre></td></tr></table></figure><h3 id="（2）以root身份执行-usr-sbin-newusers并导入user-txt"><a href="#（2）以root身份执行-usr-sbin-newusers并导入user-txt" class="headerlink" title="（2）以root身份执行 /usr/sbin/newusers并导入user.txt"></a>（2）以root身份执行 <code>/usr/sbin/newusers</code>并导入<code>user.txt</code></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># newusers &lt; user.txt</span><br></pre></td></tr></table></figure><p>然后可以执行命令 <code>vipw</code> 或 <code>vi /etc/passwd</code> 检查 <code>/etc/passwd</code> 文件是否已经出现这些用户的数据，并且用户的宿主目录是否已经创建。</p><h3 id="（3）执行命令-usr-sbin-pwunconv"><a href="#（3）执行命令-usr-sbin-pwunconv" class="headerlink" title="（3）执行命令/usr/sbin/pwunconv"></a>（3）执行命令/usr/sbin/pwunconv</h3><p>将 <code>/etc/shadow</code> 产生的 <code>shadow</code> 密码解码，然后回写到 <code>/etc/passwd</code> 中，并将<code>/etc/shadow</code>的<code>shadow</code>密码栏删掉。这是为了方便下一步的密码转换工作，即先取消 <code>shadow password</code> 功能。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pwunconv</span></span><br></pre></td></tr></table></figure><h3 id="（4）编辑每个用户的密码对照文件"><a href="#（4）编辑每个用户的密码对照文件" class="headerlink" title="（4）编辑每个用户的密码对照文件"></a>（4）编辑每个用户的密码对照文件</h3><p>格式为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户名:密码</span><br></pre></td></tr></table></figure><p>实例文件 <code>passwd.txt</code> 内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">user001:123456</span><br><span class="line">user002:123456</span><br><span class="line">user003:123456</span><br><span class="line">user004:123456</span><br><span class="line">user005:123456</span><br><span class="line">user006:123456</span><br></pre></td></tr></table></figure><h3 id="（5）以-root-身份执行命令-usr-sbin-chpasswd"><a href="#（5）以-root-身份执行命令-usr-sbin-chpasswd" class="headerlink" title="（5）以 root 身份执行命令 /usr/sbin/chpasswd"></a>（5）以 root 身份执行命令 <code>/usr/sbin/chpasswd</code></h3><p>创建用户密码，<code>chpasswd</code> 会将经过 <code>/usr/bin/passwd</code> 命令编码过的密码写入 <code>/etc/passwd</code> 的密码栏。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># chpasswd &lt; passwd.txt</span></span><br></pre></td></tr></table></figure><h3 id="（6）确定密码经编码写入-etc-passwd的密码栏后"><a href="#（6）确定密码经编码写入-etc-passwd的密码栏后" class="headerlink" title="（6）确定密码经编码写入/etc/passwd的密码栏后"></a>（6）确定密码经编码写入/etc/passwd的密码栏后</h3><p>执行命令 <code>/usr/sbin/pwconv</code> 将密码编码为 <code>shadow password</code>，并将结果写入 <code>/etc/shadow</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pwconv</span></span><br></pre></td></tr></table></figure><p>这样就完成了大量用户的创建了，之后您可以到/home下检查这些用户宿主目录的权限设置是否都正确，并登录验证用户密码是否正确。</p>]]></content>
      
      
      <categories>
          
          <category> Linux基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础Part3</title>
      <link href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part3/"/>
      <url>/2021/03/Linux%E5%9F%BA%E7%A1%80Part3/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux基础Part3——VIM使用"><a href="#Linux基础Part3——VIM使用" class="headerlink" title="Linux基础Part3——VIM使用"></a>Linux基础Part3——VIM使用</h1><p>[TOC]</p><h2 id="VIM简介"><a href="#VIM简介" class="headerlink" title="VIM简介"></a>VIM简介</h2><p>Vim是从 vi 发展出来的一个文本编辑器。具有代码补完、编译及错误跳转等丰富的编程功能。</p><p><img src="/2021/03/Linux%E5%9F%BA%E7%A1%80Part3/vi-vim-cheat-sheet-sch.gif" alt="img"></p><h2 id="vi-vim-的使用"><a href="#vi-vim-的使用" class="headerlink" title="vi/vim 的使用"></a>vi/vim 的使用</h2><p>基本上 vi/vim 共分为三种模式，分别是<strong>命令模式（Command mode）</strong>、<strong>输入模式（Insert mode）</strong>、<strong>底线命令模式（Last line mode）</strong>。 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim 文件名</span><br></pre></td></tr></table></figure><ul><li>这里的文件可以是存在的，也可以是不存在的</li></ul><h3 id="命令-一般模式"><a href="#命令-一般模式" class="headerlink" title="命令/一般模式"></a>命令/一般模式</h3><p>用户刚刚启动 vi/vim，便进入了命令模式。</p><p>此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。</p><ul><li><strong>i</strong> 切换到输入模式，以输入字符。tip：用 i, I, o, O, a, A, r, R 都可以进入编辑模式。</li><li><strong>x</strong> 删除当前光标所在处的字符。</li><li><strong>:</strong> 切换到底线命令模式，以在最底一行输入命令。</li></ul><p>命令模式只有一些最基本的命令，因此仍要依靠底线命令模式输入更多命令。</p><h3 id="输入-编辑模式"><a href="#输入-编辑模式" class="headerlink" title="输入/编辑模式"></a>输入/编辑模式</h3><p>在命令模式下按下i就进入了输入模式。</p><p>在输入模式中，可以使用以下按键：</p><ul><li><strong>字符按键以及Shift组合</strong>，输入字符</li><li><strong>ENTER</strong>，回车键，换行</li><li><strong>BACK SPACE</strong>，退格键，删除光标前一个字符</li><li><strong>DEL</strong>，删除键，删除光标后一个字符</li><li><strong>方向键</strong>，在文本中移动光标</li><li><strong>HOME</strong>/<strong>END</strong>，移动光标到行首/行尾</li><li><strong>Page Up</strong>/<strong>Page Down</strong>，上/下翻页</li><li><strong>Insert</strong>，切换光标为输入/替换模式，光标将变成竖线/下划线</li><li><strong>ESC</strong>，退出输入模式，切换到命令模式</li></ul><h3 id="底线命令模式"><a href="#底线命令模式" class="headerlink" title="底线命令模式"></a>底线命令模式</h3><p>在命令模式下按下:（英文冒号）就进入了底线命令模式。</p><ul><li>q 不保存，退出程序</li><li>w 保存文件</li><li>qw 保存并退出</li><li>q! 强制退出</li><li>ESC 退出底线命令模式。</li></ul><p><img src="/2021/03/Linux%E5%9F%BA%E7%A1%80Part3/vim-vi-workmodel.png" alt="img"></p><h2 id="vi-vim-按键说明"><a href="#vi-vim-按键说明" class="headerlink" title="vi/vim 按键说明"></a>vi/vim 按键说明</h2><h3 id="第一部分：一般模式可用的光标移动、复制粘贴、搜索替换等"><a href="#第一部分：一般模式可用的光标移动、复制粘贴、搜索替换等" class="headerlink" title="第一部分：一般模式可用的光标移动、复制粘贴、搜索替换等"></a>第一部分：一般模式可用的光标移动、复制粘贴、搜索替换等</h3><table><thead><tr><th align="left">移动光标的方法</th><th></th></tr></thead><tbody><tr><td align="left">h 或 向左箭头键(←)</td><td>光标向左移动一个字符</td></tr><tr><td align="left">j 或 向下箭头键(↓)</td><td>光标向下移动一个字符</td></tr><tr><td align="left">k 或 向上箭头键(↑)</td><td>光标向上移动一个字符</td></tr><tr><td align="left">l 或 向右箭头键(→)</td><td>光标向右移动一个字符</td></tr><tr><td align="left">如果你将右手放在键盘上的话，你会发现 hjkl 是排列在一起的，因此可以使用这四个按钮来移动光标。 如果想要进行多次移动的话，例如向下移动 30 行，可以使用 “30j” 或 “30↓” 的组合按键， 亦即加上想要进行的次数(数字)后，按下动作即可！</td><td></td></tr><tr><td align="left">[Ctrl] + [f]</td><td>屏幕『向下』移动一页，相当于 [Page Down]按键 (常用)</td></tr><tr><td align="left">[Ctrl] + [b]</td><td>屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用)</td></tr><tr><td align="left">[Ctrl] + [d]</td><td>屏幕『向下』移动半页</td></tr><tr><td align="left">[Ctrl] + [u]</td><td>屏幕『向上』移动半页</td></tr><tr><td align="left">+</td><td>光标移动到非空格符的下一行</td></tr><tr><td align="left">-</td><td>光标移动到非空格符的上一行</td></tr><tr><td align="left">n<space></space></td><td>那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20<space> 则光标会向后面移动 20 个字符距离。</space></td></tr><tr><td align="left">0 或功能键[Home]</td><td>这是数字『 0 』：移动到这一行的最前面字符处 (常用)</td></tr><tr><td align="left">$ 或功能键[End]</td><td>移动到这一行的最后面字符处(常用)</td></tr><tr><td align="left">H</td><td>光标移动到这个屏幕的最上方那一行的第一个字符</td></tr><tr><td align="left">M</td><td>光标移动到这个屏幕的中央那一行的第一个字符</td></tr><tr><td align="left">L</td><td>光标移动到这个屏幕的最下方那一行的第一个字符</td></tr><tr><td align="left">G</td><td>移动到这个档案的最后一行(常用)</td></tr><tr><td align="left">nG</td><td>n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu)</td></tr><tr><td align="left">gg</td><td>移动到这个档案的第一行，相当于 1G 啊！ (常用)</td></tr><tr><td align="left">n<Enter></Enter></td><td>n 为数字。光标向下移动 n 行(常用)</td></tr><tr><td align="left">搜索替换</td><td></td></tr><tr><td align="left">/word</td><td>向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用)</td></tr><tr><td align="left">?word</td><td>向光标之上寻找一个字符串名称为 word 的字符串。</td></tr><tr><td align="left">n</td><td>这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！</td></tr><tr><td align="left">N</td><td>这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。</td></tr><tr><td align="left">使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！</td><td></td></tr><tr><td align="left">:n1,n2s/word1/word2/g</td><td>n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则： 『:100,200s/vbird/VBIRD/g』。(常用)</td></tr><tr><td align="left"><strong>:1,$s/word1/word2/g</strong> 或 <strong>:%s/word1/word2/g</strong></td><td>从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用)</td></tr><tr><td align="left"><strong>:1,$s/word1/word2/gc</strong> 或 <strong>:%s/word1/word2/gc</strong></td><td>从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用)</td></tr><tr><td align="left">删除、复制与贴上</td><td></td></tr><tr><td align="left">x, X</td><td>在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用)</td></tr><tr><td align="left">nx</td><td>n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。</td></tr><tr><td align="left">dd</td><td>删除游标所在的那一整行(常用)</td></tr><tr><td align="left">ndd</td><td>n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用)</td></tr><tr><td align="left">d1G</td><td>删除光标所在到第一行的所有数据</td></tr><tr><td align="left">dG</td><td>删除光标所在到最后一行的所有数据</td></tr><tr><td align="left">d$</td><td>删除游标所在处，到该行的最后一个字符</td></tr><tr><td align="left">d0</td><td>那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符</td></tr><tr><td align="left">yy</td><td>复制游标所在的那一行(常用)</td></tr><tr><td align="left">nyy</td><td>n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用)</td></tr><tr><td align="left">y1G</td><td>复制游标所在行到第一行的所有数据</td></tr><tr><td align="left">yG</td><td>复制游标所在行到最后一行的所有数据</td></tr><tr><td align="left">y0</td><td>复制光标所在的那个字符到该行行首的所有数据</td></tr><tr><td align="left">y$</td><td>复制光标所在的那个字符到该行行尾的所有数据</td></tr><tr><td align="left">p, P</td><td>p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用)</td></tr><tr><td align="left">J</td><td>将光标所在行与下一行的数据结合成同一行</td></tr><tr><td align="left">c</td><td>重复删除多个数据，例如向下删除 10 行，[ 10cj ]</td></tr><tr><td align="left">u</td><td>复原前一个动作。(常用)</td></tr><tr><td align="left">[Ctrl]+r</td><td>重做上一个动作。(常用)</td></tr><tr><td align="left">这个 u 与 [Ctrl]+r 是很常用的指令！一个是复原，另一个则是重做一次～ 利用这两个功能按键，你的编辑，嘿嘿！很快乐的啦！</td><td></td></tr><tr><td align="left">.</td><td>不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用)</td></tr></tbody></table><h3 id="第二部分：一般模式切换到编辑模式的可用的按钮说明"><a href="#第二部分：一般模式切换到编辑模式的可用的按钮说明" class="headerlink" title="第二部分：一般模式切换到编辑模式的可用的按钮说明"></a>第二部分：一般模式切换到编辑模式的可用的按钮说明</h3><table><thead><tr><th align="left">进入输入或取代的编辑模式</th><th></th></tr></thead><tbody><tr><td align="left">i, I</td><td>进入输入模式(Insert mode)： i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用)</td></tr><tr><td align="left">a, A</td><td>进入输入模式(Insert mode)： a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用)</td></tr><tr><td align="left">o, O</td><td>进入输入模式(Insert mode)： 这是英文字母 o 的大小写。o 为在目前光标所在的下一行处输入新的一行； O 为在目前光标所在的上一行处输入新的一行！(常用)</td></tr><tr><td align="left">r, R</td><td>进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次；R会一直取代光标所在的文字，直到按下 ESC 为止；(常用)</td></tr><tr><td align="left">上面这些按键中，在 vi 画面的左下角处会出现『–INSERT–』或『–REPLACE–』的字样。 由名称就知道该动作了吧！！特别注意的是，我们上面也提过了，你想要在档案里面输入字符时， 一定要在左下角处看到 INSERT 或 REPLACE 才能输入喔！</td><td></td></tr><tr><td align="left">[Esc]</td><td>退出编辑模式，回到一般模式中(常用)</td></tr></tbody></table><h3 id="第三部分：一般模式切换到指令行模式的可用的按钮说明"><a href="#第三部分：一般模式切换到指令行模式的可用的按钮说明" class="headerlink" title="第三部分：一般模式切换到指令行模式的可用的按钮说明"></a>第三部分：一般模式切换到指令行模式的可用的按钮说明</h3><table><thead><tr><th align="left">指令行的储存、离开等指令</th><th></th></tr></thead><tbody><tr><td align="left">:w</td><td>将编辑的数据写入硬盘档案中(常用)</td></tr><tr><td align="left">:w!</td><td>若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关</td></tr><tr><td align="left">:q</td><td>离开 vi (常用)</td></tr><tr><td align="left">:q!</td><td>若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。</td></tr><tr><td align="left">注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有『强制』的意思～</td><td></td></tr><tr><td align="left">:wq</td><td>储存后离开，若为 :wq! 则为强制储存后离开 (常用)</td></tr><tr><td align="left">ZZ</td><td>这是大写的 Z 喔！如果修改过，保存当前文件，然后退出！效果等同于(保存并退出)</td></tr><tr><td align="left">ZQ</td><td>不保存，强制退出。效果等同于 **:q!**。</td></tr><tr><td align="left">:w [filename]</td><td>将编辑的数据储存成另一个档案（类似另存新档）</td></tr><tr><td align="left">:r [filename]</td><td>在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面</td></tr><tr><td align="left">:n1,n2 w [filename]</td><td>将 n1 到 n2 的内容储存成 filename 这个档案。</td></tr><tr><td align="left">:! command</td><td>暂时离开 vi 到指令行模式下执行 command 的显示结果！例如 『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！</td></tr><tr><td align="left">vim 环境的变更</td><td></td></tr><tr><td align="left">:set nu</td><td>显示行号，设定之后，会在每一行的前缀显示该行的行号</td></tr><tr><td align="left">:set nonu</td><td>与 set nu 相反，为取消行号！</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Linux基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础Part2</title>
      <link href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part2/"/>
      <url>/2021/03/Linux%E5%9F%BA%E7%A1%80Part2/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux基础Part2——基本命令"><a href="#Linux基础Part2——基本命令" class="headerlink" title="Linux基础Part2——基本命令"></a>Linux基础Part2——基本命令</h1><p>[TOC]</p><h2 id="查看帮助"><a href="#查看帮助" class="headerlink" title="查看帮助"></a>查看帮助</h2><h3 id="help"><a href="#help" class="headerlink" title="help"></a>help</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令 --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><h3 id="man"><a href="#man" class="headerlink" title="man"></a>man</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">man 命令</span><br></pre></td></tr></table></figure><ul><li>空格/→：下一页</li><li>回车/↓：下一行</li><li>b/←：上一页</li><li>f/↑：上一行</li><li>q：退出</li></ul><h2 id="目录管理"><a href="#目录管理" class="headerlink" title="目录管理"></a>目录管理</h2><h3 id="处理目录常用命令"><a href="#处理目录常用命令" class="headerlink" title="处理目录常用命令"></a>处理目录常用命令</h3><ul><li>ls: 列出目录</li><li>cd：切换目录</li><li>pwd：显示目前的目录</li><li>mkdir：创建一个新的目录</li><li>rmdir：删除一个空的目录</li><li>cp: 复制文件或目录</li><li>rm: 移除文件或目录</li><li>mv: 移动文件与目录，或修改文件与目录的名称</li></ul><h3 id="ls-列出目录"><a href="#ls-列出目录" class="headerlink" title="ls 列出目录"></a>ls 列出目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls [ -al ] [ 目录名称 ]</span><br></pre></td></tr></table></figure><ul><li>-a 列出全部文件，包括隐藏文件（开头为 . 的文件）</li><li>-l 列出长数据串，包括文件的属性和权限，不包括隐藏文件</li></ul><h3 id="cd-切换目录"><a href="#cd-切换目录" class="headerlink" title="cd 切换目录"></a>cd 切换目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> [ 相对路径或绝对路径 ]</span><br></pre></td></tr></table></figure><ul><li>绝对路径：由 根目录<code>/</code> 写起。</li><li>相对路径：由 父级目录<code>../</code> 或者 当前目录(可省略)<code>./</code> 写起。</li></ul><h3 id="pwd-显示当前目录"><a href="#pwd-显示当前目录" class="headerlink" title="pwd 显示当前目录"></a>pwd 显示当前目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pwd</span> [ -P ]</span><br></pre></td></tr></table></figure><ul><li>-P 显示出真实路径，而非使用连接(link)路径。</li></ul><h3 id="mkdir-创建目录"><a href="#mkdir-创建目录" class="headerlink" title="mkdir 创建目录"></a>mkdir 创建目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir  [ -mp ] 目录名称</span><br></pre></td></tr></table></figure><ul><li>-m ：配置文件的权限</li><li>-p ：递归创建多级目录</li></ul><h3 id="rmdir-删除目录"><a href="#rmdir-删除目录" class="headerlink" title="rmdir 删除目录"></a>rmdir 删除目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmdir [ -p ] 目录名称</span><br></pre></td></tr></table></figure><ul><li>-p ：递归删除上一级空白目录</li></ul><p>rmdir只能删除空白目录，rm删除非空目录。</p><h3 id="cp-复制文件-目录"><a href="#cp-复制文件-目录" class="headerlink" title="cp 复制文件/目录"></a>cp 复制文件/目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp [ -adfilprsu ] 来源(<span class="built_in">source</span>) [ source1 source2 source3 ] 目标(destination)</span><br></pre></td></tr></table></figure><ul><li>-a：等效于 -pdr </li><li>-p：连同文件的属性一起复制过去，而非使用默认属性</li><li>-d：若来源档为连结档的属性(link file)，则复制连结档属性而非文件本身</li><li>-r：多级目录的递归复制</li><li>-f：为强制(force)的意思，若目标文件已经存在且无法开启，则移除后再尝试一次</li><li>-i：若目标档(destination)已经存在时，在覆盖时会先询问动作的进行</li><li>-l：进行硬式连结(hard link)的连结档创建，而非复制文件本身</li><li>-s：复制成为符号连结档 (symbolic link)，即『捷径』文件</li><li>-u：若 destination 比 source 旧才升级 destination ！</li></ul><h3 id="rm-移除文件-目录"><a href="#rm-移除文件-目录" class="headerlink" title="rm 移除文件/目录"></a>rm 移除文件/目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm [ -fir ] 文件/目录</span><br></pre></td></tr></table></figure><p>选项与参数：</p><ul><li>-f ：(force)忽略不存在的文件，不会出现警告信息，强制删除</li><li>-i ：互动模式，在删除前会询问使用者是否删除</li><li>-r ：递归删除目录</li></ul><h3 id="mv-移动文件-目录，或修改名称"><a href="#mv-移动文件-目录，或修改名称" class="headerlink" title="mv 移动文件/目录，或修改名称"></a>mv 移动文件/目录，或修改名称</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv [ -fiu ] <span class="built_in">source</span> [ source2 source3 ] destination</span><br></pre></td></tr></table></figure><p>选项与参数：</p><ul><li>-f ：强制移动。若存在，则直接替换</li><li>-i ：若目标文件 (destination) 已经存在时，询问是否覆盖</li><li>-u ：若存在，则保留最后更新时间最新的文件</li></ul><h3 id="touch-创建文件"><a href="#touch-创建文件" class="headerlink" title="touch 创建文件"></a>touch 创建文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch 文件名1 文件名2</span><br></pre></td></tr></table></figure><p>可以一次创建多个文件</p><p>可以随意指定文件类型</p><h3 id="gt-输出重定向和-gt-gt-追加"><a href="#gt-输出重定向和-gt-gt-追加" class="headerlink" title="&gt;输出重定向和&gt;&gt;追加"></a>&gt;输出重定向和&gt;&gt;追加</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ls -l&gt;文件  <span class="comment"># 列表内容覆盖写入目标文件夹</span></span><br><span class="line">ls -al&gt;&gt;文件   <span class="comment"># 列表内容追加到文件末尾</span></span><br><span class="line">cat 文件1&gt;文件2  <span class="comment"># 用文件1覆盖文件2</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;内容&quot;</span>&gt;&gt;文件  <span class="comment"># 将内容追加到文件末尾</span></span><br></pre></td></tr></table></figure><p><code>&gt;</code>, <code>&gt;&gt;</code>左右可空格也可不空格</p><h2 id="基本属性"><a href="#基本属性" class="headerlink" title="基本属性"></a>基本属性</h2><p>Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。</p><blockquote><p>每个文件都有自己的所有者、所有组。</p><p>对于这个文件的属主(所有者/创建者)、属组(所有组)、其他用户(除了属主和属组)，有不同的权限。</p><p>一个文件夹要有x的权限，用户才能进入这个目录；并且要有r的权限，才能查看；只有w的权限，才能修改文件夹。</p></blockquote><h3 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h3><p>使用<code>ll</code>或者<code>ls –l</code>来显示一个文件的属性以及文件所属的用户和组</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos /]<span class="comment"># ls -ll</span></span><br><span class="line">total 64</span><br><span class="line">lrwxrwxrwx.   1 root root     7 Mar  7  2019 bin -&gt; usr/bin</span><br><span class="line">dr-xr-xr-x.   5 root root  4096 Mar  6 19:50 boot</span><br><span class="line">drwxr-xr-x    2 root root  4096 Nov  5  2019 data</span><br><span class="line">drwxr-xr-x   20 root root  3020 Mar  6 19:49 dev</span><br><span class="line">drwxr-xr-x.  90 root root  4096 Mar  6 19:50 etc</span><br><span class="line">drwxr-xr-x.   2 root root  4096 Apr 11  2018 home</span><br><span class="line">lrwxrwxrwx.   1 root root     7 Mar  7  2019 lib -&gt; usr/lib</span><br><span class="line">lrwxrwxrwx.   1 root root     9 Mar  7  2019 lib64 -&gt; usr/lib64</span><br><span class="line">drwx------.   2 root root 16384 Mar  7  2019 lost+found</span><br><span class="line">drwxr-xr-x.   2 root root  4096 Apr 11  2018 media</span><br><span class="line">drwxr-xr-x.   2 root root  4096 Apr 11  2018 mnt</span><br><span class="line">drwxr-xr-x.   4 root root  4096 Aug  5  2020 opt</span><br><span class="line">dr-xr-xr-x  104 root root     0 Mar  6 19:49 proc</span><br><span class="line">dr-xr-x---.   7 root root  4096 Mar 10 19:45 root</span><br><span class="line">drwxr-xr-x   25 root root   900 Mar  8 16:10 run</span><br><span class="line">lrwxrwxrwx.   1 root root     8 Mar  7  2019 sbin -&gt; usr/sbin</span><br><span class="line">drwxr-xr-x.   2 root root  4096 Apr 11  2018 srv</span><br><span class="line">dr-xr-xr-x   13 root root     0 Mar  9 22:52 sys</span><br><span class="line">drwxrwxrwt.   9 root root  4096 Mar 10 19:46 tmp</span><br><span class="line">drwxr-xr-x.  13 root root  4096 Mar  7  2019 usr</span><br><span class="line">drwxr-xr-x.  19 root root  4096 Apr 22  2020 var</span><br><span class="line"><span class="comment"># 权限   文件个数 属主 属组   大小 修改日期 时间 文件名</span></span><br></pre></td></tr></table></figure><p>第1个字符代表这个文件的类型(目录、文件或链接文件等)：</p><ul><li>d：表示目录</li><li>-：表示文件</li><li>l：档 (link file)</li><li>b：件里面的可供储存的接口设备 ( 可随机存取装置 )</li><li>c：件里面的串行端口设备，例如键盘、鼠标 ( 一次性读取装置 )</li></ul><p>第2-10个字符中，以三个为一组，且均为『rwx』 的三个参数的组合。</p><ul><li>r：代表可读(read)</li><li>w：代表可写(write)</li><li>x：代表可执行(execute)</li><li>-：代表无权限</li></ul><p><img src="/2021/03/Linux%E5%9F%BA%E7%A1%80Part2/363003_1227493859FdXT.png" alt="363003_1227493859FdXT"></p><h3 id="属主和属组"><a href="#属主和属组" class="headerlink" title="属主和属组"></a>属主和属组</h3><p>对于文件来说，它都有一个特定的所有者，也就是对该文件具有所有权的用户。</p><p>同时，在Linux系统中，用户是按组分类的，一个用户属于一个或多个组。</p><p>文件所有者以外的用户又可以分为文件所有者的同组用户和其他用户。</p><p>因此，Linux系统按文件所有者、文件所有者同组用户和其他用户来规定了不同的文件访问权限。</p><p>对于 root 用户来说，一般情况下，文件的权限对其不起作用。</p><h3 id="数字修改文件属性"><a href="#数字修改文件属性" class="headerlink" title="数字修改文件属性"></a>数字修改文件属性</h3><p><strong>1、chgrp：更改文件属组</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chgrp [ -R ] 属组名 文件名</span><br></pre></td></tr></table></figure><ul><li>-R：递归更改文件属组，就是在更改某个目录文件的属组时，如果加上-R的参数，那么该目录下的所有文件的属组都会更改。</li></ul><p><strong>2、chown：更改文件属主，也可以同时更改文件属组</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown [ –R ] 属主名 文件名</span><br><span class="line">chown [ -R ] 属主名:属组名 文件名</span><br></pre></td></tr></table></figure><p><strong>3、chmod：更改文件9个属性</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod [ -R ] xyz 文件/目录</span><br></pre></td></tr></table></figure><ul><li>-R：连同目录下的所有文件都进行递归变更</li></ul><p>Linux文件的基本权限有九个，分别是owner/group/others三种身份各有自己的read/write/execute权限。</p><p>各权限的分数对照：</p><ul><li>r = 4</li><li>w = 2</li><li>x = 1</li></ul><p>举例：</p><ul><li>可读可写可执行：rwx = 4+2+1 = 7</li><li>可读可写不可执行：rw- = 4+2+0 = 6</li><li>不可读/写/执行：— = 0+0+0 = 0</li></ul><h3 id="符号修改文件属性"><a href="#符号修改文件属性" class="headerlink" title="符号修改文件属性"></a>符号修改文件属性</h3><p>使用 u、g、o、a 代表 user用户、group组、others其他、all全部 身份</p><p>使用 r、w、x 代表读写权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod [ u/g/o/a ] [ +(加入) -(除去) =(设定) ] [ r w x ] 文件或目录</span><br></pre></td></tr></table></figure><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  touch test1    // 创建 test1 文件</span></span><br><span class="line"><span class="comment"># ls -al test1    // 查看 test1 默认权限</span></span><br><span class="line">-rw-r--r-- 1 root root 0 Nov 15 10:32 test1</span><br><span class="line"><span class="comment"># chmod u=rwx,g=rx,o=r  test1    // 修改 test1 权限</span></span><br><span class="line"><span class="comment"># ls -al test1</span></span><br><span class="line">-rwxr-xr-- 1 root root 0 Nov 15 10:32 test1</span><br><span class="line"><span class="comment">#  chmod  a-x test1</span></span><br><span class="line"><span class="comment"># ls -al test1</span></span><br><span class="line">-rw-r--r-- 1 root root 0 Nov 15 10:32 test1</span><br></pre></td></tr></table></figure><h2 id="内容查看"><a href="#内容查看" class="headerlink" title="内容查看"></a>内容查看</h2><h3 id="查看文件内容"><a href="#查看文件内容" class="headerlink" title="查看文件内容"></a>查看文件内容</h3><ul><li>cat 由第一行开始显示文件内容</li><li>tac 从最后一行开始显示</li><li>nl  显示的时候同时输出行号</li><li>more 一页一页的显示文件内容</li><li>less 与 more 类似，但是比 more 更好的是，他可以往前翻页！</li><li>head 只看开头几行</li><li>tail 只看末尾几行</li></ul><h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat [ -AbEnTv ]</span><br></pre></td></tr></table></figure><ul><li>-A：等价于 -vET ，可列出一些特殊字符而不是空白而已</li><li>-b：列出行号，仅针对非空白行做行号显示，空白行不标行号！</li><li>-E：将结尾的断行字节 $ 显示出来；</li><li>-n：列印出行号，连同空白行也会有行号，与 -b 的选项不同；</li><li>-T：将 [tab] 按键以 ^I 显示出来；</li><li>-v：列出一些看不出来的特殊字符</li></ul><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos network-scripts]<span class="comment"># cat ifcfg-eth0 </span></span><br><span class="line"><span class="comment"># Created by cloud-init on instance boot automatically, do not edit.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">BOOTPROTO=dhcp</span><br><span class="line">DEVICE=eth0</span><br><span class="line">HWADDR=52:54:00:23:6f:89</span><br><span class="line">ONBOOT=yes</span><br><span class="line">PERSISTENT_DHCLIENT=yes</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">USERCTL=no</span><br></pre></td></tr></table></figure><h3 id="tac"><a href="#tac" class="headerlink" title="tac"></a>tac</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tac [ -AbEnTv ]</span><br></pre></td></tr></table></figure><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos network-scripts]<span class="comment"># tac ifcfg-eth0 </span></span><br><span class="line">USERCTL=no</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">PERSISTENT_DHCLIENT=yes</span><br><span class="line">ONBOOT=yes</span><br><span class="line">HWADDR=52:54:00:23:6f:89</span><br><span class="line">DEVICE=eth0</span><br><span class="line">BOOTPROTO=dhcp</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Created by cloud-init on instance boot automatically, do not edit.</span></span><br></pre></td></tr></table></figure><h3 id="nl"><a href="#nl" class="headerlink" title="nl"></a>nl</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nl [ -bnw ] 文件</span><br></pre></td></tr></table></figure><ul><li>-b：指定行号指定的方式，主要有两种：-b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)；-b t ：如果有空行，空的那一行不要列出行号(默认值)；</li><li>-n：列出行号表示的方法，主要有三种：-n ln ：行号在荧幕的最左方显示；-n rn ：行号在自己栏位的最右方显示，且不加 0 ；-n rz ：行号在自己栏位的最右方显示，且加 0 ；</li><li>-w：行号栏位的占用的位数。</li></ul><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos network-scripts]<span class="comment"># nl ifcfg-eth0 </span></span><br><span class="line">     1<span class="comment"># Created by cloud-init on instance boot automatically, do not edit.</span></span><br><span class="line">     2<span class="comment">#</span></span><br><span class="line">     3BOOTPROTO=dhcp</span><br><span class="line">     4DEVICE=eth0</span><br><span class="line">     5HWADDR=52:54:00:23:6f:89</span><br><span class="line">     6ONBOOT=yes</span><br><span class="line">     7PERSISTENT_DHCLIENT=yes</span><br><span class="line">     8TYPE=Ethernet</span><br><span class="line">     9USERCTL=no</span><br></pre></td></tr></table></figure><h3 id="more"><a href="#more" class="headerlink" title="more"></a>more</h3><ul><li>空白键 (space)：代表向下翻一页；</li><li>Enter：代表向下翻『一行』；</li><li>/字串：代表在这个显示的内容当中，向下搜寻『字串』这个关键字；</li><li>:f：立刻显示出档名以及目前显示的行数；</li><li>q：代表立刻离开 more ，不再显示该文件内容。</li><li>b 或 [ctrl]-b：代表往回翻页，不过这动作只对文件有用，对管线无用。</li></ul><h3 id="less"><a href="#less" class="headerlink" title="less"></a>less</h3><ul><li>空白键：向下翻动一页；</li><li>[pagedown]↑：向下翻动一页；</li><li>[pageup]↓：向上翻动一页；</li><li>/字串：向下搜寻『字串』的功能；</li><li>?字串：向上搜寻『字串』的功能；</li><li>n：重复前一个搜寻 (与 / 或 ? 有关！)</li><li>N：反向的重复前一个搜寻 (与 / 或 ? 有关！)</li><li>q：离开 less 这个程序；</li></ul><h3 id="echo"><a href="#echo" class="headerlink" title="echo"></a>echo</h3><p>输出内容到控制台</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> [ 内容 ]</span><br></pre></td></tr></table></figure><p>demo：输出环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos ~]<span class="comment"># echo $PATH</span></span><br><span class="line">/usr/<span class="built_in">local</span>/sbin:/usr/<span class="built_in">local</span>/bin:/usr/sbin:/usr/bin:/root/bin</span><br></pre></td></tr></table></figure><h3 id="head"><a href="#head" class="headerlink" title="head"></a>head</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">head [ -n number ] 文件</span><br></pre></td></tr></table></figure><ul><li>-n：后面接数字，表示显示几行，默认为10行</li></ul><h3 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail [ -n number ] 文件</span><br></pre></td></tr></table></figure><ul><li>-n ：后面接数字，表示显示几行，默认为10行</li></ul><h2 id="日期指令"><a href="#日期指令" class="headerlink" title="日期指令"></a>日期指令</h2><h3 id="date"><a href="#date" class="headerlink" title="date"></a>date</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">date</span><br><span class="line">date +%Y</span><br><span class="line">date +<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span></span><br><span class="line">date -s <span class="string">&quot;2021-03-12 15:06:35&quot;</span></span><br></pre></td></tr></table></figure><ul><li>-s 设置当前系统时间</li></ul><p><code>+&quot;%Y-%m-%d %H:%M:%S&quot;</code>或<code>&quot;+%Y-%m-%d %H:%M:%S&quot;</code>是等效的，但必须要有一个 <code>+</code> ，多出来的+将会被当做字符输出。</p><p>输出单个数值不需要 <code>&quot;&quot;</code>。</p><h3 id="cal"><a href="#cal" class="headerlink" title="cal"></a>cal</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cal [ 选项 ]  <span class="comment"># 默认显示当前月份</span></span><br><span class="line">cal 2021  <span class="comment"># 显示2021年整年日历</span></span><br></pre></td></tr></table></figure><h2 id="查找指令"><a href="#查找指令" class="headerlink" title="查找指令"></a>查找指令</h2><h3 id="find"><a href="#find" class="headerlink" title="find"></a>find</h3><p>find 指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件或者目录显示在终端。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find [ 搜索范围 ] [ 选项 ]</span><br></pre></td></tr></table></figure><ul><li>-name 按指定文件名查找 <code>-name 文件名</code></li><li>-user 查找属于指定用户的所有文件 <code>-user 用户名</code></li><li>-size 查找指定大小的文件(+n 大于 -n 小于 n 等于) <code>-size +20M</code></li></ul><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find / *.txt  <span class="comment"># 查找所有.txt文件</span></span><br></pre></td></tr></table></figure><h3 id="locate"><a href="#locate" class="headerlink" title="locate"></a>locate</h3><p>locate 指令无需遍历整个文件系统，利用事先建立的系统中所有文件名称及路径的 locate 数据库，实现快速定位给定的文件路径。</p><p>第一次运行前，必须使用 <code>updatedb</code> 指令创建 locate 数据库，同时也需要定期更新 locate 时刻，以保证查找准确性。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">locate 文件名</span><br></pre></td></tr></table></figure><h3 id="grep和管道符号"><a href="#grep和管道符号" class="headerlink" title="grep和管道符号|"></a>grep和管道符号|</h3><p>grep 过滤查找 。</p><p>管道符 <code>|</code> 表示将前一个命令的处理结果输出传递给后面的命令处理。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep [ 选项 ] 内容 文件</span><br></pre></td></tr></table></figure><ul><li>-n 显示匹配行和行号</li><li>-i 忽略大小写</li></ul><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos ~]<span class="comment"># grep -n main hello.java </span></span><br><span class="line">2:public static void main(String args[])&#123;</span><br><span class="line">[root@VM-1-14-centos ~]<span class="comment"># cat hello.java | grep -n main</span></span><br><span class="line">2:public static void main(String args[])&#123;</span><br></pre></td></tr></table></figure><h2 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h2><h3 id="gzip-gunzip"><a href="#gzip-gunzip" class="headerlink" title="gzip/gunzip"></a>gzip/gunzip</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gzip 文件名  <span class="comment"># 将文件压缩成*.gz文件</span></span><br><span class="line">gunzip 文件名  <span class="comment"># 解压*.gz文件</span></span><br></pre></td></tr></table></figure><p>使用 <code>gzip/gunzip</code> 将不会保留源文件。</p><h3 id="zip-unzip"><a href="#zip-unzip" class="headerlink" title="zip/unzip"></a>zip/unzip</h3><p>经常使用于项目打包发布中。</p><p>会保留源文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zip [ - ] 文件名.zip 需要压缩的目录或文件</span><br><span class="line">unzip [ - ] 文件名.zip</span><br></pre></td></tr></table></figure><ul><li>-r：递归压缩</li><li>-d &lt;目录&gt; ：指定解压后文件的存放目录</li></ul><h3 id="tar"><a href="#tar" class="headerlink" title="tar"></a>tar</h3><p>tar是打包指令，即可以压缩也可以解压，最后打包后的文件是 .tar.gz 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar[ 选项 ] XXX.tar.gz 打包的内容</span><br></pre></td></tr></table></figure><ul><li>-c：产生.tar打包文件</li><li>-v：显示详细信息</li><li>-f：指定压缩后的文件名</li><li>-z：打包同时压缩</li><li>-x：解包.tar文件</li><li>-t：列出档案文件的内容，查看已经备份了哪些文件</li></ul><p>对于解压的选项，f必须放最后，其他的位置任意，但是建议按照规范</p><p>解压到指定某个目录必须加上-C选项，不然会出错。</p><p>demo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zcvf txt.tar.gz t1.txt t2.txt  <span class="comment"># 这里将t1.txt和t2.txt打包压缩为txt.tar.gz文件</span></span><br><span class="line">tar -zcvf home.tar.gz /home/  <span class="comment"># 将/home/目录打包</span></span><br><span class="line">tar -zxvf num.tar.gz -C ~  <span class="comment"># 这里将num.tar.gz解压到~这个目录下，这个目录必须存在</span></span><br></pre></td></tr></table></figure><h2 id="软-硬链接"><a href="#软-硬链接" class="headerlink" title="软/硬链接"></a>软/硬链接</h2><p>Linux 链接分两种，一种被称为硬链接（Hard Link），另一种被称为符号链接（Symbolic Link）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln [ -s ] 源文件 目标文件</span><br></pre></td></tr></table></figure><h3 id="硬连接"><a href="#硬连接" class="headerlink" title="硬连接"></a>硬连接</h3><ul><li>硬连接指通过索引节点号(Inode Index)来进行连接。即可以有两个文件名A和B指向同一个索引节点号，一个文件拥有多个有效路径名，A和B对文件系统来说是完全平等的。</li><li>删除其中任何一个都不会影响另外一个的访问，只有两个都被删除才算是真正删除了文件数据。</li><li>通常用于备份以防误删。</li></ul><h3 id="软连接"><a href="#软连接" class="headerlink" title="软连接"></a>软连接</h3><ul><li>符号连接（Symbolic Link），也叫软连接。</li><li>软链接文件实际是一个特殊的文件。内容的是另一文件的位置信息，相当于快捷方式。</li><li>A 和 B 指向的是两个不同的 inode 节点，即他们本体在磁盘的存储是两个文件。</li><li>但是 A 的数据块中存放的只是 B 的路径名（可以根据这个找到 B 的目录项）。如果 B 被删除了，A 仍然存在（因为两个是不同的文件），但指向的是一个无效的地址。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础Part1</title>
      <link href="/2021/03/Linux%E5%9F%BA%E7%A1%80Part1/"/>
      <url>/2021/03/Linux%E5%9F%BA%E7%A1%80Part1/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux基础Part1——概述"><a href="#Linux基础Part1——概述" class="headerlink" title="Linux基础Part1——概述"></a>Linux基础Part1——概述</h1><p>[TOC]</p><h2 id="Linux概述"><a href="#Linux概述" class="headerlink" title="Linux概述"></a>Linux概述</h2><h3 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h3><p>Linux 内核最初只是由芬兰人林纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。</p><p>Linux 是一套免费使用和自由传播的类 Unix 操作系统，是一个基于 POSIX（可移植操作系统接口） 和 UNIX 的多用户、多任务、支持多线程和多 CPU 的操作系统。</p><p>Linux 能运行主要的 UNIX 工具软件、应用程序和网络协议。它支持 32 位和 64 位硬件。Linux 继承了 Unix 以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。</p><h3 id="发行版"><a href="#发行版" class="headerlink" title="发行版"></a>发行版</h3><p>Linux 的发行版说简单点就是将 Linux 内核与应用软件做一个打包。</p><p>目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debian、Fedora、SuSE、OpenSUSE、Arch Linux、SolusOS 等。（Kali Linux安全渗透测试）</p><p>Mac无缝兼容Linux指令，mac也同时具备linux的优秀工具。</p><blockquote><p>作为一个优秀的程序员，应当重视安全性！</p></blockquote><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>通常服务器使用 LAMP（Linux + Apache + MySQL + PHP）或 LNMP（Linux + Nginx+ MySQL + PHP）组合。</p><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p><a href="https://www.bilibili.com/video/BV187411y7hF?p=3">暂时不详细记录，本人使用服务器</a></p><h3 id="本地虚拟机"><a href="#本地虚拟机" class="headerlink" title="本地虚拟机"></a>本地虚拟机</h3><p>VMware+CentOS</p><p>ctrl+alt: 退出虚拟机</p><h3 id="云端服务器"><a href="#云端服务器" class="headerlink" title="云端服务器"></a>云端服务器</h3><p>CentOS 7.0</p><ol><li><p>服务器就是一个远程电脑</p></li><li><p>访问出错基本都是端口放行问题，包括Linux的防火墙和服务器安全组</p><p>21-ftp；22-ssh；443-https；80-http；8888-39000/40000-宝塔；3306-MySQL；8080-Tomcat；9000-测试；6379/6380-redis</p></li><li><p>安全组开放端口授权对象：0.0.0.0/0（允许所有人访问）</p></li></ol><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="开机"><a href="#开机" class="headerlink" title="开机"></a>开机</h3><p>开机会启动许多程序。它们在Windows叫做”服务”（service），在Linux就叫做”守护进程”（daemon）。</p><p>用户的登录方式有三种：1.命令行登录；2.ssh登录；3.图形界面登录</p><p>root是最高权限，其下有更多子用户</p><h3 id="关机"><a href="#关机" class="headerlink" title="关机"></a>关机</h3><p>服务器大多数情况不会关机。</p><p>不管是重启系统还是关闭系统，首先要运行 <code>sync</code>命令，把内存中的数据写到磁盘中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sync <span class="comment"># 将数据由内存同步到硬盘中。</span></span><br><span class="line"></span><br><span class="line">shutdown <span class="comment"># 关机</span></span><br><span class="line"></span><br><span class="line">shutdown –h 10 <span class="comment"># 这个命令告诉大家，计算机将在10分钟后关机</span></span><br><span class="line"></span><br><span class="line">shutdown –h now <span class="comment"># 立马关机</span></span><br><span class="line"></span><br><span class="line">shutdown –h 20:25 <span class="comment"># 系统会在今天20:25关机</span></span><br><span class="line"></span><br><span class="line">shutdown –h +10 <span class="comment"># 十分钟后关机</span></span><br><span class="line"></span><br><span class="line">shutdown –r now <span class="comment"># 系统立即重启</span></span><br><span class="line"></span><br><span class="line">shutdown –r +10 <span class="comment"># 系统十分钟后重启</span></span><br><span class="line"></span><br><span class="line">reboot <span class="comment"># 重启，等效于 shutdown –r now</span></span><br><span class="line"></span><br><span class="line">halt <span class="comment"># 关机，等效于 shutdown –h now 和 poweroff</span></span><br></pre></td></tr></table></figure><blockquote><p>Linux中，在输入指令过后，没有输出即代表指令执行成功</p></blockquote><h2 id="Linux系统目录结构"><a href="#Linux系统目录结构" class="headerlink" title="Linux系统目录结构"></a>Linux系统目录结构</h2><blockquote><ol><li>一切皆文件</li><li>所有文件都挂载在根目录 <code>/</code> 下</li></ol></blockquote><p><strong>系统启动必须：</strong></p><ul><li><p><strong>/boot：</strong>存放的启动Linux 时使用的内核文件，包括连接文件以及镜像文件。</p></li><li><p><strong>/etc：</strong>存放<strong>所有</strong>的系统需要的<strong>配置文件</strong>和<strong>子目录列表，</strong>更改目录下的文件可能会导致系统不能启动。</p></li><li><p><strong>/lib</strong>：存放基本代码库（比如c++库），其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。</p></li><li><p><strong>/sys</strong>： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。该文件系统是内核设备树的一个直观反映。当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中</p></li></ul><p><strong>指令集合：</strong></p><ul><li><p><strong>/bin：</strong>存放着最常用的程序和指令</p></li><li><p><strong>/sbin：</strong>只有系统管理员能使用的程序和指令。</p></li></ul><p><strong>外部文件管理：</strong></p><ul><li><p><strong>/dev ：</strong>Device(设备)的缩写, 存放的是Linux的外部设备。<strong>注意：</strong>在Linux中访问设备和访问文件的方式是相同的。</p></li><li><p><strong>/media</strong>：类windows的<strong>其他设备，</strong>例如U盘、光驱等等，识别后linux会把设备放到这个目录下。</p></li><li><p><strong>/mnt</strong>：临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。</p></li></ul><p><strong>临时文件：</strong></p><ul><li><p><strong>/run</strong>：是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。</p></li><li><p><strong>/lost+found</strong>：一般情况下为空的，系统非法关机后，这里就存放一些文件。</p></li><li><p><strong>/tmp</strong>：这个目录是用来存放一些临时文件的。比如：安装包。</p></li></ul><p><strong>账户：</strong></p><ul><li><p><strong>/root</strong>：系统管理员的用户主目录。</p></li><li><p><strong>/home</strong>：目录下有各个用户以账号名命名的家目录。用户登录时，会自动进入自己的家目录。</p></li><li><p><strong>/usr</strong>： usr 是 unix shared resources(共享资源) 的缩写，这是一个非常重要的目录。用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。</p></li><li><p><strong>/usr/bin：</strong>系统用户使用的应用程序与指令。</p></li><li><p><strong>/usr/sbin：</strong>超级用户使用的比较高级的管理程序和系统守护程序。</p></li><li><p><strong>/usr/src：</strong>内核源代码默认的放置目录。</p></li></ul><p><strong>运行过程中要用：</strong></p><ul><li><p><strong>/var</strong>：存放经常修改的数据，比如程序运行的日志文件（/var/log 目录下）。</p></li><li><p><strong>/proc</strong>：管理<strong>内存空间！</strong>虚拟的目录，是系统内存的映射，我们可以直接访问这个目录来，获取系统信息。这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件来做修改。</p></li></ul><p><strong>扩展用的：</strong></p><ul><li><p><strong>/opt</strong>：默认是空的，我们安装额外软件可以放在这个里面。</p></li><li><p><strong>/srv</strong>：存放服务启动后需要提取的数据<strong>（不用服务器就是空）</strong></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程-完结目录</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程-完结目录"><a href="#浙江大学机器学习课程-完结目录" class="headerlink" title="浙江大学机器学习课程-完结目录"></a>浙江大学机器学习课程-完结目录</h1><p>教程视频传送门：<a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=1">浙江大学研究生机器学习课程</a></p><p>对于本人来说是看完一些机器学习入门教程之后的理论补充。这个教程包含大量具体详细的理论推导，有一定难度，但是对于想要熟悉理论的还是比较推荐。本人在以后会把推导不足的部分补全。</p><table><thead><tr><th align="center"><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart1/">Part1——课程概论</a></th></tr></thead><tbody><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/">Part2——支持向量机</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/">Part3——人工神经网络</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/">Part4——深度学习</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/">Part5——强化学习</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart6/">Part6——传统的机器学习</a></strong></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part6</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart6/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart6/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part6——传统的机器学习"><a href="#浙江大学机器学习课程Part6——传统的机器学习" class="headerlink" title="浙江大学机器学习课程Part6——传统的机器学习"></a>浙江大学机器学习课程Part6——传统的机器学习</h1><p>[TOC]</p><h2 id="特征选择与特征提取"><a href="#特征选择与特征提取" class="headerlink" title="特征选择与特征提取"></a>特征选择与特征提取</h2><p>特征选择与特征提取(Feature Selection and Extraction)</p><p>特征选择是一个”物理”过程，不会产生新特征；特征提取是一个”化学”过程，会产生新特征。</p><ol><li>特征提取：主成分分析(Principle Component Analysis)</li><li>特征选择：自适应提升(AdaBoost)</li></ol><h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><h3 id="主成分分析与神经网络"><a href="#主成分分析与神经网络" class="headerlink" title="主成分分析与神经网络"></a>主成分分析与神经网络</h3><p>多层的神经网络，其本质也是一个特征提取器。但是，主成分分析主要目标是减少计算量。</p><p>主成分分析：构造一个A，b使：Y<del>(M*1)</del> = A<del>(M*N)</del> * X<del>(N*1)</del> + b<del>(M*1)</del></p><p>主成分分析可以看成是一个一层的，有m个神经元的神经网络。</p><h3 id="主成分分析的过程"><a href="#主成分分析的过程" class="headerlink" title="主成分分析的过程"></a>主成分分析的过程</h3><p>主成分分析：寻找方差最大的方向，并在该方向投影。在降维的同时保存最大的区分度。</p><p>这里方差最大方向指的是投影之后方差和最大，因为如果投影之后点都汇集在一起的话，那么可以近似成一个点，就区分不出来了。</p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=38">主成分分析详细推导P38</a></p><ol><li><p>最大化寻找的特征方向上的方差和。</p><p>max： a<del>1</del>∑a<del>1</del>^T^=λ；s.t.：a<del>1</del>*a<del>1</del>^T^=||a<del>1</del>||^2^=1 (a<del>i</del>是A的行向量)</p></li><li><p>寻找下方差和最大化的特征方向，并且需要与刚才的方向正交。</p><p>max： a<del>2</del>∑a<del>2</del>^T^=λ；s.t.：a<del>2</del>*a<del>2</del>^T^=||a<del>2</del>||^2^=1、a<del>1</del>*a<del>2</del>^T^=a<del>2</del>*a<del>1</del>^T^=0</p></li><li><p>a<del>2</del>是∑的特征向量，λ是∑的第二大特征值</p></li><li><p>loop</p></li></ol><h3 id="PCA算法全程"><a href="#PCA算法全程" class="headerlink" title="PCA算法全程"></a>PCA算法全程</h3><ol><li><p>求 ∑=∑<del>i=1</del>^i^(X<del>i</del>-E(x))^T^</p></li><li><p>求∑的特征值并从大到小排序[λ<del>1</del>, λ<del>1</del>, λ<del>2</del>,…, λ<del>M</del>, λ<del>M+1</del>,… ]</p><p>对应特征向量[a<del>1</del>^T^, a<del>2</del>^T^, …, a<del>M</del>^T^, a<del>M+1</del>^T^, …]</p></li><li><p>归一化所有a<del>i</del>，使a<del>i</del>a<del>i</del>^T^=1</p></li><li><p>A=[[–a<del>1</del>–],[–a<del>2</del>–],…,[–a<del>m</del>–]]</p></li><li><p>Y<del>i</del>=A(X<del>i</del>-E(X))</p></li></ol><h3 id="其他算法"><a href="#其他算法" class="headerlink" title="其他算法"></a>其他算法</h3><p>奇异值分解SVD(Singular Value Decomposation)快速求出特征值来完成PCA算法。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在特征数比较多，样本数又比较少的情况下，采用PCA，效果不会差。</p><h2 id="自适应提升-AdaBoost"><a href="#自适应提升-AdaBoost" class="headerlink" title="自适应提升(AdaBoost)"></a>自适应提升(AdaBoost)</h2><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>从N个特征中选择M个使识别率更高。</p><p>（启发式方法，如模拟退火、基因算法）①递增法②递减法。不常用，因为神经网络已经做了这些事情，相关性不高的连线之间w会变得很小。</p><h3 id="自适应提升"><a href="#自适应提升" class="headerlink" title="自适应提升"></a>自适应提升</h3><p>针对大规模冗余的特征样本时，是一个非常好的算法。</p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=39&spm_id_from=pageDriver">详细推导P39</a></p><ol><li>初始化采样权值</li><li>用D<del>m</del>采样N个样本（错的样本出现更多，对的样本出现更少），获得弱分类器</li><li>计算加权错误率</li><li>更新权值分布</li><li>-&gt; 2. loop </li><li>最终识别器</li></ol><p>定理：随着M增加，AdaBoost最终分类器在训练集上错误率越来越小。</p><p>AdaBoost过拟合速度不会上升太快。</p><h2 id="概率分类法-重点复习"><a href="#概率分类法-重点复习" class="headerlink" title="概率分类法==重点复习=="></a>概率分类法==重点复习==</h2><p>一定要特别注意先验概率！！！</p><h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart6/image-20210213210453656.png" alt="image-20210213210453656"></p><p>限制条件：</p><ol><li>每个维度都是离散的</li><li>每个维度互相独立</li></ol><p>对于每个P(特征|类别)有：<br>$$<br>P(w|C_j)={count(w,C_j)+1\over{\sum\limits_{w∈V}cout(w,C_j)+|v|}}<br>$$<br>其中，C<del>j</del>指某个类别j，V指特征集合，|v|指特征数。</p><h3 id="高斯概率密度函数"><a href="#高斯概率密度函数" class="headerlink" title="高斯概率密度函数"></a>高斯概率密度函数</h3><p>正态分布（Normal distribution）又名高斯分布（Gaussian distribution）。</p><h4 id="多维高斯分布："><a href="#多维高斯分布：" class="headerlink" title="多维高斯分布："></a>多维高斯分布：</h4><p>$$<br>P(X|C)= {1\over{\sqrt[]{(1π)^d|∑|}}}exp[-{1\over2}(x-μ)^T∑^{-1}(x-μ)]<br>$$</p><p>已知{X<del>i</del>}<del>i={1-N}</del> ，求待求参数：∑(d×d矩阵)、μ(d×1向量)</p><p>构造目标函数（极大似然法Maximum Likedihood）<br>$$<br>E(μ,∑)=\sum\limits_{i=1}^NlnP(X_i|C)<br>$$<br>假设：①所有{X<del>i</del>}<del>i={1-N}</del>独立同分布 undependent and identical distribution ( i.i.d. )；②设定μ<del>1</del>、∑<del>1</del>使出现{X<del>i</del>}<del>i={1-N}</del>概率最大。</p><p>先是概率累乘作为似然函数，取对数方便运算，连乘就变成求和了</p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=43">详细推导p43</a></p><h3 id="EM算法-Expectation-Maximization"><a href="#EM算法-Expectation-Maximization" class="headerlink" title="EM算法(Expectation-Maximization)"></a>EM算法(Expectation-Maximization)</h3><h4 id="混合高斯模型-Gaussian-Mixture-Model"><a href="#混合高斯模型-Gaussian-Mixture-Model" class="headerlink" title="混合高斯模型(Gaussian Mixture Model)"></a>混合高斯模型(Gaussian Mixture Model)</h4><p>叠加多个高斯分布拟合整个复杂的分布。</p><p>这是一个非凸问题，只能求局部极值，不能求全局极值。</p><p>求局部极值的一种方法，而且只对某一类局部极值问题可解。</p><p>优点：①不需要调任何参数，必定收敛②编程简单③理论优美</p><p>步骤：</p><ol><li>随机化，先假设类别</li><li>E-step 计算第n个样本在k个高斯的概率</li><li>M-step 更新所有N个样本中有多少个属于第k个高斯</li><li>-&gt;2 loop</li></ol><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=45">详细推导p45</a></p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=48&spm_id_from=pageDriver">EM算法收敛性推导</a></p><h4 id="k-均值聚类-K-means-Clustering"><a href="#k-均值聚类-K-means-Clustering" class="headerlink" title="k-均值聚类(K-means Clustering)"></a>k-均值聚类(K-means Clustering)</h4><ol><li>随机化μ<del>1</del>、…、μ<del>k</del></li><li>E-step 离哪个类近，重新归属于哪一类</li><li>M-step n个样本中有多少个属于第k类，重新分配第k类的均值μ<del>k</del></li><li>-&gt;2 loop</li></ol><h2 id="GMM在说话人识别中的应用"><a href="#GMM在说话人识别中的应用" class="headerlink" title="GMM在说话人识别中的应用"></a>GMM在说话人识别中的应用</h2><ol><li>去除静音(将不说话的低能量片段去除，但保留同为低能量的辅音(使用过零率判别))</li><li>提取的特征：MEL倒谱系数 （Mel-frequency Cepstrum Coefficients, MFCC）</li></ol><p>缺点：对噪声要求严苛，因为加了噪声就相当于改变分布。</p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part5</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part5——强化学习"><a href="#浙江大学机器学习课程Part5——强化学习" class="headerlink" title="浙江大学机器学习课程Part5——强化学习"></a>浙江大学机器学习课程Part5——强化学习</h1><p>[TOC]</p><h2 id="增强学习与监督学习的区别"><a href="#增强学习与监督学习的区别" class="headerlink" title="增强学习与监督学习的区别"></a>增强学习与监督学习的区别</h2><ol><li>训练数据中没有标签，只有奖励函数（Reward Function）。</li><li>训练数据不是现成给定，而是由行为（Action）获得。</li><li>现在的行为（Action）不仅影响后续训练数据的获得，也影响奖励函数（Reward Function）的取值。</li><li>训练的目的是构建一个“<strong>状态-&gt;行为</strong>”*(内部状态和外部状态，外部状态不由我们的行为控制)*的函数，其中状态（State）描述了目前内部和外部的环境，在此情况下，要使一个智能体（Agent）在某个特定的状态下，通过这个函数，决定此时应该采取的行为。希望采取这些行为后，最终获得最大化的奖励函数值。</li></ol><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ol><li>R<del>t</del>：t时刻的奖励函数</li><li>S<del>t</del>：t时刻的状态</li><li>A<del>t</del>：t时刻的行为</li></ol><h2 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h2><ol><li><p>马尔科夫假设：P[S<del>t+1</del>|S<del>t</del>]=P[S<del>t+1</del>|S1,…,S<del>t</del>]</p></li><li><p>下一个时刻的状态只与这一时刻的状态以及这一时刻的行为有关：</p><p>P<del>SS’</del>^a^=P[S<del>t+1</del>=s’|S<del>t</del>=s, A<del>t</del>=a]</p></li><li><p>下一时刻的奖励函数值值域这一时刻的状态以及这一时刻的行为有关：</p><p>P<del>S</del>^a^=E[R<del>t+1</del>|S<del>t</del>=s, A<del>t</del>=a]</p></li></ol><h2 id="Markov-decision-Process-MDP"><a href="#Markov-decision-Process-MDP" class="headerlink" title="Markov decision Process (MDP)"></a>Markov decision Process (MDP)</h2><ol><li>在t=0时候，环境给出一个初始状态 s ~ p(s<del>0</del>)</li><li>循环：<pre><code> -- 智能体选择行为：a~t~ -- 环境采样奖励函数：r~t~ ~ R( . |s~t~, a~t~) -- 环境产生下一个状态：s~t+1~ ~ R( . |s~t~, a~t~) -- 智能体获得奖励函数 r~t~ 和下一个状态 s~t+1~</code></pre></li><li>我们需要学习一个策略（Policy）π^*^(s<del>t</del>,a<del>t</del>)=P(a<del>t</del>|s<del>t</del>)   , 这是一个从状态到行为的映射函数，使得最大化累积的奖励。</li></ol><h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><ol><li>增强学习中已经知道的的函数是：P<del>S</del>^a^=E[R<del>t+1</del>|S<del>t</del>=s, A<del>t</del>=a]</li><li>需要学习的函数是：P<del>SS’</del>^a^=P[S<del>t+1</del>=s’|S<del>t</del>=s, A<del>t</del>=a]</li><li>根据一个决策机制（Policy），我们可以获得一条路径：s<del>0</del> -&gt; a<del>0</del> -&gt; r<del>0</del> -&gt; s<del>1</del> -&gt; a<del>1</del> -&gt; r<del>1</del> …</li><li>定义1：估值函数（Value Function）是衡量某个状态最终能获得多少累积奖励的函数：<img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/image-20210210225702878.png" alt="image-20210210225702878"></li><li>定义2：Q函数是衡量某个状态下采取某个行为后，最终能获得多少累积奖励的函数：<img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/image-20210210225720116.png" alt="image-20210210225720116"></li><li>在s状态下做出行为a的概率，和这种情况下的奖励，得到估值 V^π^(s)=∑<del>a∈A</del>P(a|s)Q^π^(s,a)</li></ol><h3 id="劣势"><a href="#劣势" class="headerlink" title="劣势"></a>劣势</h3><ol><li>对于状态数和行为数很多时，使Q函数非常复杂，难以收敛。例如：①对一个ATARI游戏，状态数是相邻几帧所有像素的取值组合，这是一个天文数字；②图像方面的应用，状态数是(像素值取值范围数)^(像素个数)</li><li>很多程序，如下棋程序等，REWARD是最后获得（输或赢），不需要对每一个中间步骤都计算REWARD。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><p>目前强化学习的发展状况：在一些特定的任务上达到人的水平或胜过人，但在一些相对复杂的任务上，例如自动驾驶等，和人存在差距。</p></li><li><p>和真人的差距，可能不完全归咎于算法。传感器、机械的物理限制等，也是决定性因素。</p></li><li><p>机器和人的另一差距是：人有一些基本的概念，依据这些概念，人能只需要很少的训练就能学会很多，但机器只有通过大规模数据，才能学会。</p></li><li><p>但是，机器速度快，机器永不疲倦，只要有源源不断的数据，在特定的任务上，机器做得比人好，是可以期待的。</p></li></ol><h2 id="Alpha-Go"><a href="#Alpha-Go" class="headerlink" title="Alpha Go"></a>Alpha Go</h2><p>围棋有必胜策略</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/image-20210210235412124.png" alt="image-20210210235412124"></p><p><a href="https://github.com/Rochester-NRT/RocAlphaGo">Alpha Go 开源地址</a></p><p>每隔一定的轮次，训练过后的网络将和训练之前的网络对抗，已获得更多的样本数据继续训练。</p><p>另外有一个更加简单的深度策略网络(Rollout Policy Network)，牺牲准确率来换取速度，在对局后期通过不断演算，将赢的落子概率增加，输的概率减少。</p><p>蒙特卡洛树搜索 （Monte Carlo Tree Search）：多次模拟未来棋局，然后选择在模拟中选择次数最多的走法。</p><p>同时要采用多样化的步骤来增加随机性。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><p>Reinforcement Learning an introduction. R. S. Sutton and A. G. Barto, 2005</p></li><li><p><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">UCL Course on RL</a></p></li><li><p>例程程序</p><p><a href="http://karpathy.github.io/2016/05/31/rl/?_utm_source=1-2-2">http://karpathy.github.io/2016/05/31/rl/?_utm_source=1-2-2</a><br><a href="https://gym.openai.com/https://github.com/openai/gym">https://gym.openai.com/https://github.com/openai/gym</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part4</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part4——深度学习"><a href="#浙江大学机器学习课程Part4——深度学习" class="headerlink" title="浙江大学机器学习课程Part4——深度学习"></a>浙江大学机器学习课程Part4——深度学习</h1><p>[TOC]</p><h2 id="多层神经网络的优劣"><a href="#多层神经网络的优劣" class="headerlink" title="多层神经网络的优劣"></a>多层神经网络的优劣</h2><h3 id="多层神经网络的优势"><a href="#多层神经网络的优势" class="headerlink" title="多层神经网络的优势"></a>多层神经网络的优势</h3><ol><li>基本单元简单，多个基本单元可扩展为非常复杂的非线性函数。因此易于构建，同时模型有很强的表达能力。</li><li>训练和测试的计算并行性非常好，有利于在分布式系统上的应用。</li><li>模型构建来源于对人脑的仿生，话题丰富，各种领域的研究人员都有兴趣，都能做贡献。</li></ol><h3 id="多层神经网络的劣势"><a href="#多层神经网络的劣势" class="headerlink" title="多层神经网络的劣势"></a>多层神经网络的劣势</h3><ol><li>数学不漂亮，优化算法只能获得局部极值，算法性能与初始值有关。</li><li>不可解释。训练神经网络获得的参数与实际任务的关联性非常模糊。</li><li>模型可调整的参数很多 （网络层数、每层神经元个数、非线性函数、学习率、优化方法、终止条件等等），使得训练神经网络变成了一门“艺术”。</li><li>如果要训练相对复杂的网络，需要大量的训练样本。</li></ol><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><ol><li>Mnist：手写数字数据库（LeCun 在1998年创造）</li><li>ImageNet：（Fei-fei Li等 2007年创造）</li></ol><h2 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h2><h3 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h3><p>神经网络是目前处理大数据最优的算法。</p><p>模拟退火和遗传算法还处于沉寂期。</p><h3 id="转机"><a href="#转机" class="headerlink" title="转机"></a>转机</h3><p>2006年是深度学习的起始年，Hinton在SCIENCE上发文，提出一种叫做自动编码机（Auto-encoder）的方法，部分解决了神经网络参数初始化的问题。</p><p>但是目前为止，自动编码机并没有什么用。</p><h2 id="卷积神经网络-CNN"><a href="#卷积神经网络-CNN" class="headerlink" title="卷积神经网络(CNN)"></a>卷积神经网络(CNN)</h2><h3 id="发展"><a href="#发展" class="headerlink" title="发展"></a>发展</h3><p>是深度学习神经网络流行起来最大的因素。</p><p>由手工设计卷积核变成自动学习卷积核。</p><h3 id="如何卷积"><a href="#如何卷积" class="headerlink" title="如何卷积"></a>如何卷积</h3><blockquote><p>一个讲解比较清晰的视频：<a href="https://www.bilibili.com/video/BV1JX4y1K7Dr">什么是卷积？</a></p></blockquote><p>详细图解原理在另一份笔记中，不重复记述：<a href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/">PyTorch深度学习实践Part10</a></p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210160012660.png" alt="image-20210210160012660"></p><p>卷积核的数量等于输出通道数。卷积核的长度等于输入通道数。</p><p>卷积过后得到的叫做特征图。</p><p>在边缘可能会丢失数据的时候，用padding补零。</p><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210160114566.png" alt="image-20210210160114566"></p><p>卷积神经网络与全连接的区别。</p><ol><li>局部感受野</li><li>权值共享</li></ol><p>卷积虽然复杂，但是计算量更少。</p><h3 id="激活"><a href="#激活" class="headerlink" title="激活"></a>激活</h3><p>在卷积神经网络中，最常用的非线性函数为ReLu。</p><h3 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h3><p>导数会平均反向传播。</p><blockquote><p>整个网络的计算速度取决于卷积层，整个网络的参数个数取决于全连接层。</p><p>即：如果要加速神经网络，则在卷积层做文章；如果要让其内存占用更少的话，则在全连接层做文章。</p></blockquote><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h3 id="改进1——ReLU"><a href="#改进1——ReLU" class="headerlink" title="改进1——ReLU"></a>改进1——ReLU</h3><p>以ReLU函数代替sigmoid或tanh函数。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210163727240.png" alt="image-20210210163727240"></p><p>实践证明，这样做能使网络训练以更快速度收敛。</p><blockquote><p>如果数据是靠近0的正态分布，则每次只激活一半的神经元。</p></blockquote><h3 id="改进2——MaxPooling"><a href="#改进2——MaxPooling" class="headerlink" title="改进2——MaxPooling"></a>改进2——MaxPooling</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210163808698.png" alt="image-20210210163808698"></p><p>在AlexNet中，提出了最大池化(Max Pooling)的概念，即对每一个邻近像素组成的“池子”，选取像素最大值作为输出。在LeNet中，池化的像素是不重叠的；而在AlexNet中进行的是有重叠的池化。实践表明，有重叠的最大池化能够很好的克服过拟合问题，提升系统性能。</p><p>在反向传播时，只传播最大值，其他都为0。</p><blockquote><p>MaxPooling不仅是一个降采样的操作，同时还是一个非线性操作。只采用最大的神经元，有效降低了激活的神经元个数，从而加快了收敛的速率。</p></blockquote><h3 id="改进3——Dropout"><a href="#改进3——Dropout" class="headerlink" title="改进3——Dropout"></a>改进3——Dropout</h3><p>随机丢弃（Dropout）。为了避免系统参数更新过快导致过拟合，每次利用训练样本更新参数时候，随机的“丢弃”一定比例的神经元，被丢弃的神经元将不参加训练过程，输入和输出该神经元的权重系数也不做更新。这样每次训练时，训练的网络架构都不一样，而这些不同的网络架构却分享共同的权重系数。实验表明，随机丢弃技术减缓了网络收敛速度，也以大概率避免了过拟合的发生。</p><blockquote><p>道理和改进2一样，每次训练都只激活有限个神经元，而不要让整个网络同时改变所有的参数，导致整个网络不稳定。</p></blockquote><h3 id="改进4——增加训练样本"><a href="#改进4——增加训练样本" class="headerlink" title="改进4——增加训练样本"></a>改进4——增加训练样本</h3><p>增加训练样本。尽管ImageNet的训练样本数量有超过120万幅图片，但相对于6亿待估计参数来说，训练图像仍然不够。Alex等人采用了多种方法增加训练样本，包括：1. 将原图水平翻转；2. 将256×256的图像随机选取224×224的片段作为输入图像。运用上面两种方法的组合可以将一幅图像变为2048幅图像。还可以对每幅图片引入一定的噪声，构成新的图像。这样做可以较大规模增加训练样本，避免由于训练样本不够造成的性能损失。</p><h3 id="改进5——GPU加速"><a href="#改进5——GPU加速" class="headerlink" title="改进5——GPU加速"></a>改进5——GPU加速</h3><p>用GPU加速训练过程。采用2片GTX 580 GPU对训练过程进行加速，由于GPU强大的并行计算能力，使得训练过程的时间缩短数十倍，哪怕这样，训练时间仍然用了六天。</p><h2 id="近年来流行的网络结构"><a href="#近年来流行的网络结构" class="headerlink" title="近年来流行的网络结构"></a>近年来流行的网络结构</h2><h3 id="VGGNet-（Simonyan-and-Zisserman-2014）"><a href="#VGGNet-（Simonyan-and-Zisserman-2014）" class="headerlink" title="VGGNet: （Simonyan and Zisserman, 2014）"></a>VGGNet: （Simonyan and Zisserman, 2014）</h3><p>3个叠到一起的3 * 3卷积核，感受野（Receptive Field）是7 * 7,大致可以替代7 * 7卷积核的作用。但这样做可以使参数更少 ，参数比例大致为27:49。</p><p>但是，运算速度会因为卷积核数量的增加而大幅下降。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210173031450.png" alt="image-20210210173031450"></p><h3 id="GoogLeNet-（Szegedy-2014）"><a href="#GoogLeNet-（Szegedy-2014）" class="headerlink" title="GoogLeNet: （Szegedy, 2014）"></a>GoogLeNet: （Szegedy, 2014）</h3><p>inception 结构，用一些1<em>1, 3</em>3和5*5的小卷积核用固定方式组合到一起，来代替大的卷积核，从而达到增加感受野和减少参数的目的。500万参数，比ALEXNET小了12倍。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210173440780.png" alt="image-20210210173440780"></p><h3 id="残差网络Residual-Net-ResNet-（He-et-al-2015）"><a href="#残差网络Residual-Net-ResNet-（He-et-al-2015）" class="headerlink" title="残差网络Residual Net(ResNet): （He et al, 2015）"></a>残差网络Residual Net(ResNet): （He et al, 2015）</h3><p>加入了前向输入机制，将前面层获得的特征图作为监督项输入到后面层。用这样的方法使深层网络训练能够更好地收敛。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210183347601.png" alt="image-20210210183347601"></p><h2 id="迁移学习-Transfer-Learning"><a href="#迁移学习-Transfer-Learning" class="headerlink" title="迁移学习(Transfer Learning )"></a>迁移学习(Transfer Learning )</h2><p>迁移学习是指，从一个domain训练好的神经网络，加入新的domain的样本进行调优，从而获得一个更好的识别效果。</p><p>例如，训练好的 Alex Net ，在最后的1k个分类后，外加一层全连接神经网络，输出十几种水果的分类，用十种水果进行调优。虽然这原本1k个分类中或许没有很明显的这十几种水果分类，但这样它能很容易排除除了这些水果以外的物品，同时经过调优之后会更容易贴合我们想要分出的十几种水果。</p><h2 id="目标检测与分割"><a href="#目标检测与分割" class="headerlink" title="目标检测与分割"></a>目标检测与分割</h2><p>这里其实算深度学习（</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211213322900.png" alt="image-20210211213322900"></p><h3 id="目标定位与识别"><a href="#目标定位与识别" class="headerlink" title="目标定位与识别"></a>目标定位与识别</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211213742143.png" alt="image-20210211213742143"></p><h3 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h3><p>在一幅图片中可能有多种类别，不能确定数量。</p><p>现在这个问题基本解决，有三篇逐步递进的三篇论文。</p><h4 id="Regional-CNN-R-CNN"><a href="#Regional-CNN-R-CNN" class="headerlink" title="Regional CNN(R-CNN)"></a>Regional CNN(R-CNN)</h4><p>目标候选区域(Region Proposal)：先用传统方法或者图像处理的方法，确定可能有物体的地方候选项，再放在卷积神经网络检测。</p><h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><ol><li>Selective Search：产生RP</li><li>CNN：检测这些候选区域</li><li>SVM：分类</li></ol><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211222650735.png" alt="image-20210211222650735"></p><h5 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h5><ol><li>High cost to perform Selective Search (~5s per image)</li><li>Too many passes to CNN (~2000 proposals per image)</li><li>Lead to unacceptable test time (~50s per image)</li><li>High space cost to train SVM (millions of 1024-d features)</li></ol><h4 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h4><p>用 Regions of Interest(RoIs) 把不同长宽的候选区域，在Pooling层归一化成同一形状。最后依然是一路预测label，一路预测Bounding-box regressors。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211224649852.png" alt="image-20210211224649852"></p><h5 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h5><p>主要通过小的Region Proposal Network产生粗略位置，来代替Selective Search</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211221620685.png" alt="image-20210211221620685"></p><h3 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h3><h4 id="全卷积网络FCN"><a href="#全卷积网络FCN" class="headerlink" title="全卷积网络FCN"></a>全卷积网络FCN</h4><p>全卷积网络(Fully Convolutional Networks)</p><p>先训练前面一半，再训练后一半。先降采样，后升采样。</p><p>卷积层的上采样（Upsampling）也叫反卷积（Deconvolution）或 转置卷积（Transpose Convolution）。</p><p>全卷积网络也可以用于边缘提取。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211230604746.png" alt="image-20210211230604746"></p><h2 id="隐含马尔科夫过程-HMM-与递归神经网络-RNN"><a href="#隐含马尔科夫过程-HMM-与递归神经网络-RNN" class="headerlink" title="隐含马尔科夫过程(HMM)与递归神经网络(RNN)"></a>隐含马尔科夫过程(HMM)与递归神经网络(RNN)</h2><p>对连续信息的判断有几个问题：不知道如何划分每一个状态，因为①持续时间可能各不相同；②对于语音模型的建模是以一个音节而不是一个单词为基础。</p><h3 id="Hidden-Markov-Models-复习"><a href="#Hidden-Markov-Models-复习" class="headerlink" title="Hidden Markov Models==复习=="></a>Hidden Markov Models==复习==</h3><p>一个HMM模型是由三部分组成：λ = {A, B, π}。其中，A为状态转移矩阵，B为观测概率，π为状态先验概率。</p><ol><li>π(S<del>i</del>)表示一开始在S<del>i</del>状态的概率。</li><li>A是一个P×P的矩阵。马尔科夫链假设是强假设，后一个时刻状态和前一个(或者多个)时刻状态有关（注意这是固定假设 a<del>i,j</del>=P(q<del>t+1</del>=S<del>j</del>|q<del>t</del>=S<del>i</del>)。当t时刻的q样本是状态为S<del>i</del>类，则在t+1时刻q样本转变为状态为S<del>j</del>类的概率）</li><li>B={b<del>j</del>(0)} 若输入向量O属于S<del>j</del>，则它的概率分布用b<del>j</del>(0)表示。</li></ol><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=50">详细推导p50</a></p><h3 id="大词汇量连续语音识别（LVCSR）"><a href="#大词汇量连续语音识别（LVCSR）" class="headerlink" title="大词汇量连续语音识别（LVCSR）"></a>大词汇量连续语音识别（LVCSR）</h3><p>大词汇量连续语音识别（Large-scale Vocabulary Continuous Speech Recognition, LVCSR）</p><ol><li>每一个HMM模型所表达的“单词”是什么？英语中有效的Triphone个数大致在55000左右，模型过多而训练样本不足，所以需要多个Triphone 合并（Tying）、多个Triphone 联合训练（Tying）</li><li>在识别流程中如何对测试声音文件做时间轴的划分，使每一个分段（SEGMENT）对应一个“单词”？如何搜索最佳的“单词”组合？VITERBI搜索（有多种形式，如Two-Level Dynamic Programming）、A*搜索、随机搜索</li><li>如何构造语言模型 (Language Model)? 定义(N-gram): 一个单词出现的概率，只与它前面的N个单词相关。</li></ol><h3 id="结合深度网络模型的语音识别"><a href="#结合深度网络模型的语音识别" class="headerlink" title="结合深度网络模型的语音识别"></a>结合深度网络模型的语音识别</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214220419460.png" alt="image-20210214220419460"></p><h3 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h3><h4 id="输入与输出多对多："><a href="#输入与输出多对多：" class="headerlink" title="输入与输出多对多："></a>输入与输出多对多：</h4><p>大词汇连续语音识别、机器翻译</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214215442390.png" alt="image-20210214215442390"></p><h4 id="输入与输出多对一："><a href="#输入与输出多对一：" class="headerlink" title="输入与输出多对一："></a>输入与输出多对一：</h4><p>动作识别、行为识别、单词量有限的语音识别</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214215758906.png" alt="image-20210214215758906"></p><h4 id="输入与输出一对多："><a href="#输入与输出一对多：" class="headerlink" title="输入与输出一对多："></a>输入与输出一对多：</h4><p>文本生成、图像文字标注</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214215916422.png" alt="image-20210214215916422"></p><h3 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long-Short Term Memory (LSTM)"></a>Long-Short Term Memory (LSTM)</h3><p>相比VANILLA RNN， LSTM的误差反向传播更方便和直接，梯度更新不存在RNN中的暴涨或消失现象。因此，建议涉及RNN的应用都用LSTM或LSTM相关的变种。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214221615263.png" alt="image-20210214221615263"></p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part3</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part3——人工神经网络"><a href="#浙江大学机器学习课程Part3——人工神经网络" class="headerlink" title="浙江大学机器学习课程Part3——人工神经网络"></a>浙江大学机器学习课程Part3——人工神经网络</h1><p>[TOC]</p><h2 id="神经网络的生物及数学模型"><a href="#神经网络的生物及数学模型" class="headerlink" title="神经网络的生物及数学模型"></a>神经网络的生物及数学模型</h2><ol><li>硬件算力的提升</li><li>数据样本的增加</li><li>但是，<strong>其最基本的神经元模型至今没有重大改变</strong></li></ol><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210209144613134.png" alt="image-20210209144613134"></p><h2 id="感知器算法-Perceptron-Algorithm"><a href="#感知器算法-Perceptron-Algorithm" class="headerlink" title="感知器算法(Perceptron Algorithm)"></a>感知器算法(Perceptron Algorithm)</h2><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210209144950398.png" alt="image-20210209144950398"></p><ul><li>效果比SVM差得多，但是是机器学习最早提出的算法。</li><li>感知器每次只取单个样本，SVM从全局样本考虑。</li></ul><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=20">收敛性证明看p20</a></p><h2 id="多层神经网络-Multi-Layer-Neural-Networks"><a href="#多层神经网络-Multi-Layer-Neural-Networks" class="headerlink" title="多层神经网络(Multi-Layer Neural Networks)"></a>多层神经网络(Multi-Layer Neural Networks)</h2><p>线性不可分的数据集困扰了早期神经网络算法长达十年之久。</p><p>因此，我们需要用非线性的函数集合来分开非线性可分的数据集。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210209160042159.png" alt="image-20210209160042159"></p><p>二层神经网络模型举例。</p><h2 id="三层神经网络模拟任意决策面"><a href="#三层神经网络模拟任意决策面" class="headerlink" title="三层神经网络模拟任意决策面"></a>三层神经网络模拟任意决策面</h2><p>我个人认为：过了第一层，数据样本基本不会保持原貌，但是保留了特征，或者说特点。</p><p>在老师画的示例中，实际的数学意义为：①第一层是：坐标(x,y)进入多个函数输入相对位置，进入激活函数输出0-1；②第二层：根据上一层判断的0-1，进入函数判断是否为某一块图形内部，返回0-1；③第三层：根据上一层返回的是否在某图形内部的0-1，进入函数判断该图形是否为class1，通过或关系，输出最终结果0-1.</p><p>详情见<a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=23">p23</a></p><h2 id="后向传播算法-Back-Propagation"><a href="#后向传播算法-Back-Propagation" class="headerlink" title="后向传播算法(Back Propagation)"></a>后向传播算法(Back Propagation)</h2><blockquote><p>在对于某种问题，我们究竟应该选择哪一种参数组合，如何搭建完美的网络结构。这依然是一个至今为止不完备的，对于一种问题，最好的方法还是不断试验。</p></blockquote><p>主要思想就是梯度向下法(Gradient Descent Method)来求局部极值。</p><p>常用的激活函数：sigmoid、tanh、ReLu(Rectified Linear Units)、LeakReLu</p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=24">==BP推导P24==</a></p><h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><ol><li>不用每输入一个样本就去变换参数，而是输入一批样本（叫做一个BATCH或MINI-BATCH），求出这些样本的梯度平均值后，根据这个平均值改变参数。</li><li>在神经网络训练中，BATCH的样本数大致设置为50-200不等。  </li></ol><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210210140226760.png" alt="image-20210210140226760"></p><h3 id="梯度消失与归一化"><a href="#梯度消失与归一化" class="headerlink" title="梯度消失与归一化"></a>梯度消失与归一化</h3><ol><li>当样本都非常大或者非常小，在sigmoid函数中就可以发现，他们的梯度将会非常小，这样意味着他们更加符合我们想要得到的预测结果，即二分类。但是这并不是我们在训练过程中想要的，因为梯度太小而导致参数无法更新以进行训练。</li><li>防止梯度消失，那么就需要使样本更加靠近在0附近，更加具有像线性模型一样的特性。因此我们经常对<strong>初</strong>始数据以及在<strong>训练过程中</strong>进行归一化Batch Normalization。</li></ol><h3 id="目标函数选择"><a href="#目标函数选择" class="headerlink" title="目标函数选择"></a>目标函数选择</h3><ol><li><p>SOFTMAX函数</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210210131234229.png" alt="image-20210210131234229"> 处理多分类</p></li><li><p>交叉熵(Cross Entropy)</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210210131306125.png" alt="image-20210210131306125"> 处理二分类</p></li></ol><h3 id="参数更新策略"><a href="#参数更新策略" class="headerlink" title="参数更新策略"></a>参数更新策略</h3><p>优化器不一定是MSE，因为MSE的更新通常容易出现z字路径，一般可以选择使用其他更加平滑的优化器，比如AdaGrad、RMSProp。</p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part2</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part2——支持向量机"><a href="#浙江大学机器学习课程Part2——支持向量机" class="headerlink" title="浙江大学机器学习课程Part2——支持向量机"></a>浙江大学机器学习课程Part2——支持向量机</h1><p>[TOC]</p><h2 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h2><h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><p>对样本数较少的时候，都会得到一个比较好的结果。</p><p>如何在两种训练集上画一条直线来分类：</p><ol><li>线性可分(Linear Separable)训练样本集</li><li>非线性可分(Non-Linear Separable)样本集</li></ol><p>将线(在多维特征下是超平面Hyperplane)向两边移动直到擦到样本点，其中间隔(Margin)最大，且线在d/2处。</p><h3 id="数学描述"><a href="#数学描述" class="headerlink" title="数学描述"></a>数学描述</h3><ol><li>将平行线擦到的向量称作支持向量(Support Vector)</li><li>训练数据及标签：(Xn, Yn)…… X为特征向量，Y为标签。Y取+1或-1来表示，方便推导。</li><li>线性模型：(W, b) 超平面：Wt * X + b = 0</li></ol><h3 id="机器学习过程"><a href="#机器学习过程" class="headerlink" title="机器学习过程"></a>机器学习过程</h3><ol><li>用复杂的函数来限定模型框架</li><li>留出待定参数</li><li>用训练样本来确定参数取值</li></ol><h3 id="线性可分的定义和条件"><a href="#线性可分的定义和条件" class="headerlink" title="线性可分的定义和条件"></a>线性可分的定义和条件</h3><p>{(Xi, Yi)}i = 1<del>N, 存在(W, b), 则对任意 i = 1</del>N. 有：</p><p>①若Yi = +1, 则 Wt * X + b &gt; 0</p><p>②若Yi = -1, 则 Wt * X + b &lt; 0</p><p>为什么要取Y=±1？因为可以得到①②等价于 Yi ( Wt * X + b ) &gt; 0</p><h2 id="优化问题-优化目标函数和限制条件"><a href="#优化问题-优化目标函数和限制条件" class="headerlink" title="优化问题(优化目标函数和限制条件)"></a>优化问题(优化目标函数和限制条件)</h2><h3 id="两个要点"><a href="#两个要点" class="headerlink" title="两个要点"></a>两个要点</h3><ol><li>点最小化(Minimize / min): 1 / 2 * ||W||² 。<strong>这里1/2只是为了求导方便</strong></li><li>限制条件(Subject to / s.t.): Y<del>i</del> ( W^T^ * X + b ) ≥ 1, (i=1~N)</li></ol><h3 id="两个事实"><a href="#两个事实" class="headerlink" title="两个事实"></a>两个事实</h3><p>事实1：W^T^ * X + b = 0 与 a * W^T^ * X + a * b = 0, (a∈R+) 是同一个平面。</p><p>​           即: 若 (W, b) 满足 W^T^ * X + b = 0 , 则 (a * W, a * b) 也满足 W^T^ * X + b = 0 </p><p>事实2：向量到超平面(点到平面)的距离公式。d = | Wt * X0 + b | / || W || *<em>1.这里的X0代表的是包含多个维度的坐标[x,y,z…]，而Y0是分类标签，不能与坐标混为一谈；2.Wt</em>X0结果是一个数而不是矩阵向量**</p><p>​           其中，模 || W || = √(W1²+W2²…+Wn²)</p><h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><ol><li>用a去缩放超平面参数：(W, b) &lt;=&gt; ( a * W, a * b )。根据事实1，这两组不同的参数代表的是同一个平面。</li><li>最终使在支持向量上：| W^T^ * X<del>0</del> + b | = 1。<strong>这里的W和b都是经过系数a缩放过后的，目的是为了凑出1(当然你可以凑出任意常数)，而a具体是多少，我们不需要关注。需要注意的是，这里只是带入支持向量的值，并不是指支持向量的那个点在超平面上，否则值为0。</strong></li><li>由<em>推导2</em>和<em>事实2</em>可得：d = | W^T^ * X<del>0</del> + b | / || W || = 1 / || W ||。可得<em>要点1</em>：最小化 ||W|| 即最大化 d 。</li><li>其他不是支持向量的点到超平面的距离，则大于 1 / || W || 。可得 | W^T^ * X<del>0</del> + b | &gt; 1 。可得<em>要点2</em>：限制条件 Y<del>i</del> * ( W^T^ * X + b ) ≥ 1</li></ol><p>举例：原来平面是 W^T^ * X + b = 0 , 假设带入X<del>0</del>后的值 | W^T^ * X<del>0</del> + b | = M , 现在把超平面缩放为 a * W^T^ * X + a * b = 0 , 其中a是1/M, 那么把X<del>0</del>带入则 | a * W^T^ * X<del>0</del> + a * b | = M/M = 1。与此同时，计算d的时候，因为分子分母同乘a=1/M，a不需要求出，所以我们不需要关心a的取值，只是为了凑一个 | W^T^ * X<del>0</del> + b | = 1 ，当然你可以凑出任意常数。</p><h3 id="二次优化问题-Quadratic-Programming"><a href="#二次优化问题-Quadratic-Programming" class="headerlink" title="二次优化问题(Quadratic Programming)"></a>二次优化问题(Quadratic Programming)</h3><p>二次优化问题属于凸优化问题</p><ol><li>目标函数(Obejective Function)是二次项</li><li>限制条件是一次项</li></ol><p>要么无解，要么只有唯一极值。即局部极值就是全局极值。</p><h3 id="SVM处理非线性"><a href="#SVM处理非线性" class="headerlink" title="SVM处理非线性"></a>SVM处理非线性</h3><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207133321372.png" alt="image-20210207133321372" style="zoom: 200%;"><p>处理这种问题的一种方案就是加上正则项(Regulation Term)（结构损失函数），在解集合中挑选出一组参数（解），使经验损失和结构损失都较低。</p><p>c是正则化的强度，是事先设定好的超参数。</p><p>ζ是松弛变量(Slack Variable)。</p><p>放到场景中就是，样本数小于参数量，在只优化经验误差函数的时候很容易发生过拟合。这个公式依然可以适用于处理线性Linear SVM。</p><h2 id="低维到高维映射"><a href="#低维到高维映射" class="headerlink" title="低维到高维映射"></a>低维到高维映射</h2><p>在低维空间中，一些线性不可分的数据集，在高维空间中，更有可能被分开。因此可以把Xi通过函数φ(x)变换映射到高维空间，通过泛函分析满足某种条件，把核函数W*φ(x)拆成两个高维向量的内积。</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>我们可以不在意无限维映射φ(x)的显示表达，我们只要知道一个核函数(kernel Function)，<img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207142313276.png" alt="image-20210207142313276">，则优化问题仍然可解。</p><p>线性内核相当于没有用核。多项式核的待定系数d取越大，则越复杂。高斯核是无限维度，分类效果最高，待定系数为σ。Tanh核的待定参数是β和b。这些待定参数的选取只能不停地试。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210206144627709.png" alt="image-20210206144627709"></p><h3 id="充要条件"><a href="#充要条件" class="headerlink" title="充要条件"></a>充要条件</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207142313276.png" alt="image-20210207142313276">能够成立的充要条件：(Mercer’s Theorem)</p><ol><li>K(X<del>1</del>, X<del>2</del>) = K(X<del>2</del>, X<del>1</del>) <strong>(交换性)</strong></li><li>对任意 常数C<del>i</del>, 向量X<del>i</del> (i=1<del>N)，有 ∑(i=1</del>N) ∑(j=1<del>N) C</del>i~ * C<del>j</del> * K(X<del>i</del>, X<del>j</del>) ≥ 0 <strong>(半正定性)</strong></li></ol><h2 id="原问题和对偶问题-重点复习"><a href="#原问题和对偶问题-重点复习" class="headerlink" title="原问题和对偶问题==(重点复习)=="></a>原问题和对偶问题==(重点复习)==</h2><h3 id="优化理论"><a href="#优化理论" class="headerlink" title="优化理论"></a>优化理论</h3><p>优化理论(运筹学)是工程里最本质的问题</p><h4 id="推荐书籍"><a href="#推荐书籍" class="headerlink" title="推荐书籍"></a>推荐书籍</h4><ol><li>Convex optimization - Stephen Boyd - b站吴立德</li><li>Nonlinear Programming</li></ol><h3 id="原问题-Prime-Problem"><a href="#原问题-Prime-Problem" class="headerlink" title="原问题(Prime Problem)"></a>原问题(Prime Problem)</h3><p>min: f(w)</p><p>s.t. : g<del>i</del>(w)≤0 (i=1<del>K) , h</del>i<del>(w)=0 (i=1</del>M)</p><h3 id="对偶问题-Dual-Problem"><a href="#对偶问题-Dual-Problem" class="headerlink" title="对偶问题(Dual Problem)"></a>对偶问题(Dual Problem)</h3><ol><li><p>定义：<img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207153808814.png" alt="image-20210207153808814"></p><p>x是前面的w，α是一个K维的向量，β是一个M维的向量。</p><p>拉格朗日对偶问题是运筹学基础知识。*(KKT条件求解、拉格朗日传乘数法、弱对偶性定理)*</p></li><li><p>对偶问题定义：</p><p>max: <img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207154702680.png" alt="image-20210207154702680">(inf:求最小值)</p><p>s.t. : λ<del>i</del> ≥ 0 (i=1~K)</p></li></ol><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207161614446.png" alt="p11-19:17"></p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207162438113.png" alt="p11-22:56"></p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207162758286.png" alt="p11-26:14"></p><p>这里可以去看《Convex optimization》前150页内容，学习推导过程。</p><p>结论可推出：</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207163413689.png" alt="image-20210207163413689"></p><h2 id="支持向量机原问题转化为对偶问题-重点复习"><a href="#支持向量机原问题转化为对偶问题-重点复习" class="headerlink" title="支持向量机原问题转化为对偶问题==(重点复习)=="></a>支持向量机原问题转化为对偶问题==(重点复习)==</h2><p>凸函数的定义。w可能是高维向量，这个代数表达在高维依然适用。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207164852602.png" alt="image-20210207164852602"></p><p>原问题证明建议重看<a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=12">p12-23:37</a></p><h2 id="支持向量机的应用——兵王问题"><a href="#支持向量机的应用——兵王问题" class="headerlink" title="支持向量机的应用——兵王问题"></a>支持向量机的应用——兵王问题</h2><h3 id="n折交叉验证"><a href="#n折交叉验证" class="headerlink" title="n折交叉验证"></a>n折交叉验证</h3><p>对于<strong>每一组超参数</strong>，进行<strong>n折交叉验证</strong>求损失，最终选取的是损失最小的那一组超参数。</p><h3 id="测试结果中的支持向量"><a href="#测试结果中的支持向量" class="headerlink" title="测试结果中的支持向量"></a>测试结果中的支持向量</h3><p>当支持向量占比非常高，甚至是几乎等于训练样本。则表明这次训练失败，或者数据集本身没有规律，或者SVM 没法找到他的规律。</p><h3 id="评判模型好坏"><a href="#评判模型好坏" class="headerlink" title="评判模型好坏"></a>评判模型好坏</h3><h4 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h4><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210208230203359.png" alt="image-20210208230203359"></p><p>TP: 将正样本识别为正样本的数量（或概率）</p><p>FN: 将正样本识别为负样本的数量（或概率）</p><p>FP: 将负样本识别为正样本的数量（或概率）</p><p>TN: 将负样本识别为负样本的数量（或概率）</p><p>FN减少 &lt;=&gt; TP增加 &lt;=&gt; FP增加 &lt;=&gt; TN减少</p><blockquote><p>mAP: mean Average Precision, 即各类别AP的平均值</p><p>AP: PR曲线下面积，后文会详细讲解</p><p>PR曲线: Precision-Recall曲线</p><p>Precision: TP / (TP + FP)</p><p>Recall: TP / (TP + FN)</p><p>TP: IoU&gt;0.5的检测框数量（同一Ground Truth只计算一次）</p><p>FP: IoU&lt;=0.5的检测框，或者是检测到同一个GT的多余检测框的数量</p><p>FN: 没有检测到的GT的数量</p></blockquote><h4 id="ROC曲线-Receiver-Operating-Character"><a href="#ROC曲线-Receiver-Operating-Character" class="headerlink" title="ROC曲线(Receiver Operating Character)"></a>ROC曲线(Receiver Operating Character)</h4><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210209123230240.png" alt="image-20210209123230240"></p><p>四条线表示四个不同的系统</p><p>等错误率 (Equal Error Rate, EER)是两类错误FP和FN相等时候的错误率，这时错误率越小，表示系统性能约好。</p><p>AUC(Area Under Curve)曲线右下角的一块面积，面积的大小也能体现模型的好坏。</p><blockquote><p>判别模型的好坏要看具体的应用，并不是准确率越高，模型高就越好</p></blockquote><h3 id="兵王问题的Python实现-补充"><a href="#兵王问题的Python实现-补充" class="headerlink" title="兵王问题的Python实现==(补充)=="></a>兵王问题的Python实现==(补充)==</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="处理多分类问题"><a href="#处理多分类问题" class="headerlink" title="处理多分类问题"></a>处理多分类问题</h2><ol><li>改造优化的目标函数和限制条件，使之能处理多类<br>论文 SVM-Multiclass Multi-class Support Vector Machine</li><li>一类 VS 其他类</li><li>一类 VS 另一类</li></ol><p>其中，①方法通常不是很好，因为SVM是针对二分类问题开发的。</p><p>在n类问题分类中，②方法主要构造n个SVM，③方法要构造 n * (n-1) / 2 个SVM。</p><p>通常来说，③方法分类噢效果更好，但是也更复杂。</p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part1</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart1/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart1/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part1——课程概论"><a href="#浙江大学机器学习课程Part1——课程概论" class="headerlink" title="浙江大学机器学习课程Part1——课程概论"></a>浙江大学机器学习课程Part1——课程概论</h1><p>[TOC]</p><h2 id="推荐书籍-amp-课程"><a href="#推荐书籍-amp-课程" class="headerlink" title="推荐书籍&amp;课程"></a>推荐书籍&amp;课程</h2><ol><li>机器学习， 周志华，清华大学出版社，2016</li><li>统计学习方法，李航，清华大学出版社，2012</li><li>Machine Learning in Action, P. Harrington,人民邮电出版社</li><li>Pattern Recognition and Machine Learning (模式识别与机器学习)，Christopher M. Bishop, 2006</li><li>Machine Learning: A Probabilistic Perspective, K. P. Murphy, </li><li>Machine Learning (机器学习), Tom M. Mitchell, 机械工业出版社，2003年</li><li>Deep Learning, I. Goodfellow, Y. Bengio and A. Courville, 2016</li><li><a href="https://www.coursera.org/course/ml">Stanfrod Web course by Andrew Ng</a></li><li><a href="https://cs231n.stanford.edu/">Stanfrod Web course by Fei-fei Li</a></li></ol><h2 id="机器学习算法分类"><a href="#机器学习算法分类" class="headerlink" title="机器学习算法分类"></a>机器学习算法分类</h2><ol><li>Supervised learning 监督式学习 – SVM, NEURAL NETWORKS</li><li>Unsupervised learning 无监督式学习 – CLUSTERING, EM ALGORITHM, PCA</li><li>Semi-Supervised Learning 半监督式学习</li><li>Reinforcement learning 增强学习</li></ol><p>以预测标签为导向的分类：监督式学习、无监督式学习、半监督式学习。</p><p>不注重过程而关心结果的分类：增强学习。例如：智能驾驶（在不违反交通规则的情况下最快到达目的地）、下棋对战AI（为了最终的输赢有很多种不同的下法）</p><p>Supervised learning:  The machine learning task of inferring a function from labeled training data. Supervised learning can be further divided into </p><ol><li>classification 分类（二分类/多分类） - 离散值标签</li><li>regression 回归（单响应/多响应） - 连续值标签</li></ol><p>两者没有明确界限，以至于有些分类算法也能做回归算法。</p><h2 id="机器学习两大难点"><a href="#机器学习两大难点" class="headerlink" title="机器学习两大难点"></a>机器学习两大难点</h2><ol><li>维度</li><li>标准</li></ol><h2 id="没有免费午餐定理-No-Free-Lunch-Theorem"><a href="#没有免费午餐定理-No-Free-Lunch-Theorem" class="headerlink" title="没有免费午餐定理(No Free Lunch Theorem)"></a>没有免费午餐定理(No Free Lunch Theorem)</h2><p>任何一个预测函数，如果在一些训练样本上表现好，那么必然在另一些训练样本上表现不好，表现好与表现不好的情况一样多。</p><p>如果我们不对特征空间有先验假设，则所有算法的平均表现是一样的。</p><p>我们认为：特征差距小的样本更有可能是同一类。但是，在没有任何先前给定特征意义的情况下，我们都不能确定预测的下一个是什么。</p><p>在这个领域没有最好的算法，但是有公认的好方法。</p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践-完结目录</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践"><a href="#PyTorch深度学习实践" class="headerlink" title="PyTorch深度学习实践"></a>PyTorch深度学习实践</h1><p>教程视频传送门：<a href="https://www.bilibili.com/video/BV1Y7411d7Ys?p=13">《PyTorch深度学习实践》完结合集</a></p><p>总之就是非常推荐，很适合新手入门，原理循序渐进，建议认真听，认真做。</p><table><thead><tr><th align="center"><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/">Part1——概论</a></th></tr></thead><tbody><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/">Part2——线性模型</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/">Part3——梯度下降算法</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/">Part4——反向传播</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/">Part5——线性回归</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/">Part6——逻辑斯蒂回归</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/">Part7——处理多维特征的输入</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/">Part8——加载数据集</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/">Part9——多分类问题</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/">Part10——卷积神经网络（基础篇）</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/">Part11——卷积神经网络（高级篇）</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/">Part12——循环神经网络（基础篇）</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/">Part13——循环神经网络（高级篇）</a></strong></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Python </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part13</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part13——循环神经网络（高级篇）"><a href="#PyTorch深度学习实践Part13——循环神经网络（高级篇）" class="headerlink" title="PyTorch深度学习实践Part13——循环神经网络（高级篇）"></a>PyTorch深度学习实践Part13——循环神经网络（高级篇）</h1><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210121201135070.png" alt="image-20210121201135070"></p><p>双向循环神经网络</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122101740766.png" alt="image-20210122101740766"></p><h2 id="人名处理"><a href="#人名处理" class="headerlink" title="人名处理"></a>人名处理</h2><ol><li>切分字符串</li><li>转ASCII码</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122103511843.png" alt="image-20210122103511843"></p><ol start="3"><li>填充</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122103548168.png" alt="image-20210122103548168"></p><ol start="4"><li>转置</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122103627700.png" alt="image-20210122103627700"></p><ol start="5"><li>排序</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122103701821.png" alt="image-20210122103701821"></p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line">HIDDEN_SIZE = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line">N_LAYER = <span class="number">2</span></span><br><span class="line">N_EPOCHS = <span class="number">100</span></span><br><span class="line">N_CHARS = <span class="number">128</span></span><br><span class="line">USE_GPU = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NameDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, is_train_set=<span class="literal">True</span></span>):</span></span><br><span class="line">        filename = <span class="string">&#x27;../names_train.csv.gz&#x27;</span> <span class="keyword">if</span> is_train_set <span class="keyword">else</span> <span class="string">&#x27;../names_test.csv.gz&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            reader = csv.reader(f)</span><br><span class="line">            rows = <span class="built_in">list</span>(reader)</span><br><span class="line">        self.names = [row[<span class="number">0</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows]</span><br><span class="line">        self.<span class="built_in">len</span> = <span class="built_in">len</span>(self.names)</span><br><span class="line">        self.countries = [row[<span class="number">1</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows]</span><br><span class="line">        self.country_list = <span class="built_in">list</span>(<span class="built_in">sorted</span>(<span class="built_in">set</span>(self.countries)))</span><br><span class="line">        self.country_dict = self.getCountryDict()</span><br><span class="line">        self.country_num = <span class="built_in">len</span>(self.country_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.names[index], self.country_dict[self.countries[index]]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getCountryDict</span>(<span class="params">self</span>):</span></span><br><span class="line">        country_dict = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> idx, country_name <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.country_list, <span class="number">0</span>):</span><br><span class="line">            country_dict[country_name] = idx</span><br><span class="line">        <span class="keyword">return</span> country_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">idx2country</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.country_list[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getCountriesNum</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.country_num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">trainset = NameDataset(is_train_set=<span class="literal">True</span>)</span><br><span class="line">trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line">testset = NameDataset(is_train_set=<span class="literal">False</span>)</span><br><span class="line">testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=<span class="literal">False</span>)</span><br><span class="line">N_COUNTRY = trainset.getCountriesNum()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNClassifier</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, n_layers=<span class="number">1</span>, bidirectional=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNNClassifier, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.n_directions = <span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.embedding = torch.nn.Embedding(input_size, hidden_size)</span><br><span class="line">        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers,</span><br><span class="line">                                bidirectional=bidirectional)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hidden</span>(<span class="params">self, batch_size</span>):</span></span><br><span class="line">        hidden = torch.zeros(self.n_layers * self.n_directions,</span><br><span class="line">                             batch_size, self.hidden_size)</span><br><span class="line">        <span class="keyword">return</span> create_tensor(hidden)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, seq_lengths</span>):</span></span><br><span class="line">        <span class="comment"># input shape : B x S -&gt; S x B</span></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.t()</span><br><span class="line">        batch_size = <span class="built_in">input</span>.size(<span class="number">1</span>)</span><br><span class="line">        hidden = self._init_hidden(batch_size)</span><br><span class="line">        embedding = self.embedding(<span class="built_in">input</span>)</span><br><span class="line">        <span class="comment"># pack them up</span></span><br><span class="line">        gru_input = pack_padded_sequence(embedding, seq_lengths)</span><br><span class="line">        output, hidden = self.gru(gru_input, hidden)</span><br><span class="line">        <span class="keyword">if</span> self.n_directions == <span class="number">2</span>:</span><br><span class="line">            hidden_cat = torch.cat([hidden[-<span class="number">1</span>], hidden[-<span class="number">2</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden_cat = hidden[-<span class="number">1</span>]</span><br><span class="line">        fc_output = self.fc(hidden_cat)</span><br><span class="line">        <span class="keyword">return</span> fc_output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name2list</span>(<span class="params">name</span>):</span></span><br><span class="line">    arr = [<span class="built_in">ord</span>(c) <span class="keyword">for</span> c <span class="keyword">in</span> name]</span><br><span class="line">    <span class="keyword">return</span> arr, <span class="built_in">len</span>(arr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tensor</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="keyword">if</span> USE_GPU:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        tensor = tensor.to(device)</span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_tensors</span>(<span class="params">names, countries</span>):</span></span><br><span class="line">    sequences_and_lengths = [name2list(name) <span class="keyword">for</span> name <span class="keyword">in</span> names]</span><br><span class="line">    name_sequences = [sl[<span class="number">0</span>] <span class="keyword">for</span> sl <span class="keyword">in</span> sequences_and_lengths]</span><br><span class="line">    seq_lengths = torch.LongTensor([sl[<span class="number">1</span>] <span class="keyword">for</span> sl <span class="keyword">in</span> sequences_and_lengths])</span><br><span class="line">    countries = countries.long()</span><br><span class="line">    <span class="comment"># make tensor of name, BatchSize x SeqLen</span></span><br><span class="line">    seq_tensor = torch.zeros(<span class="built_in">len</span>(name_sequences), seq_lengths.<span class="built_in">max</span>()).long()</span><br><span class="line">    <span class="keyword">for</span> idx, (seq, seq_len) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(name_sequences, seq_lengths), <span class="number">0</span>):</span><br><span class="line">        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)</span><br><span class="line">    <span class="comment"># sort by length to use pack_padded_sequence</span></span><br><span class="line">    seq_lengths, perm_idx = seq_lengths.sort(dim=<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    seq_tensor = seq_tensor[perm_idx]</span><br><span class="line">    countries = countries[perm_idx]</span><br><span class="line">    <span class="keyword">return</span> create_tensor(seq_tensor), create_tensor(seq_lengths), create_tensor(countries)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_since</span>(<span class="params">since</span>):</span></span><br><span class="line">    s = time.time() - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;%dm %ds&#x27;</span> % (m, s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainModel</span>():</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, (names, countries) <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">1</span>):</span><br><span class="line">        inputs, seq_lengths, target = make_tensors(names, countries)</span><br><span class="line">        output = classifier(inputs, seq_lengths)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f&#x27;[<span class="subst">&#123;time_since(start)&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            print(<span class="string">f&#x27;[<span class="subst">&#123;i * <span class="built_in">len</span>(inputs)&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(trainset)&#125;</span>] &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            print(<span class="string">f&#x27;loss=<span class="subst">&#123;total_loss / (i * <span class="built_in">len</span>(inputs))&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testModel</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="built_in">len</span>(testset)</span><br><span class="line">    print(<span class="string">&quot;evaluating trained model ...&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (names, countries) <span class="keyword">in</span> <span class="built_in">enumerate</span>(testloader, <span class="number">1</span>):</span><br><span class="line">            inputs, seq_lengths, target = make_tensors(names, countries)</span><br><span class="line">            output = classifier(inputs, seq_lengths)</span><br><span class="line">            pred = output.<span class="built_in">max</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()</span><br><span class="line">        percent = <span class="string">&#x27;%.2f&#x27;</span> % (<span class="number">100</span> * correct / total)</span><br><span class="line">        print(<span class="string">f&#x27;Test set: Accuracy <span class="subst">&#123;correct&#125;</span>/<span class="subst">&#123;total&#125;</span> <span class="subst">&#123;percent&#125;</span>%&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> correct / total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)</span><br><span class="line">    <span class="keyword">if</span> USE_GPU:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        classifier.to(device)</span><br><span class="line"></span><br><span class="line">    criterion = torch.nn.CrossEntropyLoss()  <span class="comment"># 做的是分类问题，用交叉熵</span></span><br><span class="line">    optimizer = torch.optim.Adam(classifier.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    print(<span class="string">&quot;Training for %d epochs...&quot;</span> % N_EPOCHS)</span><br><span class="line">    acc_list = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N_EPOCHS + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Train cycle</span></span><br><span class="line">        trainModel()</span><br><span class="line">        acc = testModel()</span><br><span class="line">        acc_list.append(acc)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part12</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part12——循环神经网络（基础篇）"><a href="#PyTorch深度学习实践Part12——循环神经网络（基础篇）" class="headerlink" title="PyTorch深度学习实践Part12——循环神经网络（基础篇）"></a>PyTorch深度学习实践Part12——循环神经网络（基础篇）</h1><h2 id="循环神经网络RNN"><a href="#循环神经网络RNN" class="headerlink" title="循环神经网络RNN"></a>循环神经网络RNN</h2><p>之前一开始用的是稠密网络DNN，因为是全连接，所以对每个元素都有相应的权重，因此其计算量是远大于看似复杂但是具有权重共享特性的CNN的。而RNN就是延续权重共享理念的网络。</p><p>RNN主要处理有序列连接的数据，比如自然语言、天气、股市、视频等。</p><p>RNN本质是一个线性层，与DNN不同是RNN Cell是共享的。</p><p>从图像到文本的转换：CNN+FC+RNN。</p><p>循环神经网络的激活函数更常用tanh。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121142256148.png" alt="image-20210121142256148"></p><p>可以选择使用RNN Cell自己构建循环，也可以使用RNN。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121145441952.png" alt="image-20210121145441952"></p><h2 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h2><p>处理文本使用独热编码</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121151229487.png" alt="image-20210121151229487"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121151654777.png" alt="image-20210121151654777"></p><h2 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h2><p>独热编码的缺点：</p><ol><li>维度高</li><li>稀疏</li><li>硬编码</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121153403068.png" alt="image-20210121153403068"></p><p>使用Embedding改善优化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare dataset</span></span><br><span class="line">num_class = <span class="number">4</span></span><br><span class="line">input_size = <span class="number">4</span></span><br><span class="line">hidden_size = <span class="number">8</span></span><br><span class="line">embedding_size = <span class="number">10</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">idx2char = [<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>]</span><br><span class="line">x_data = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]]  <span class="comment"># (batch, seq_len)</span></span><br><span class="line">y_data = [<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]  <span class="comment"># (batch * seq_len)</span></span><br><span class="line">inputs = torch.LongTensor(x_data)</span><br><span class="line">labels = torch.LongTensor(y_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># design model using class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.emb = torch.nn.Embedding(input_size, embedding_size)  <span class="comment"># matrix of Embedding:[input_size, embedding_size]</span></span><br><span class="line">        self.rnn = torch.nn.RNN(input_size=embedding_size,</span><br><span class="line">                                hidden_size=hidden_size,</span><br><span class="line">                                num_layers=num_layers,</span><br><span class="line">                                batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_size, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        hidden = torch.zeros(num_layers, x.size(<span class="number">0</span>), hidden_size)</span><br><span class="line">        x = self.emb(</span><br><span class="line">            x)  <span class="comment"># 这里输入需要是长整型longtensor，输出为(𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒆𝒎𝒃𝒆𝒅𝒅𝒊𝒏𝒈𝑺𝒊𝒛𝒆)，注意batch_first=True</span></span><br><span class="line">        x, _ = self.rnn(x, hidden)  <span class="comment"># 输出(𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛e)</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, num_class)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># training cycle forward, backward, update</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    _, idx = outputs.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">    idx = idx.data.numpy()</span><br><span class="line">    print(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27;&#x27;</span>.join([idx2char[x] <span class="keyword">for</span> x <span class="keyword">in</span> idx]), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;, Epoch [%d/15] loss = %.3f&#x27;</span> % (epoch + <span class="number">1</span>, loss.item()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><p>使用LSTM和GRU训练模型</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121192740594.png" alt="image-20210121192740594"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121192834591.png" alt="image-20210121192834591"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part11</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part10——卷积神经网络（高级篇）"><a href="#PyTorch深度学习实践Part10——卷积神经网络（高级篇）" class="headerlink" title="PyTorch深度学习实践Part10——卷积神经网络（高级篇）"></a>PyTorch深度学习实践Part10——卷积神经网络（高级篇）</h1><h2 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h2><p>寻找超参数是十分困难的，GoogleNet把不同的模型作成块Inception，在训练时优秀的超参数模块权重自然增加。</p><p>Concatenate拼接四个分支算出来的张量。</p><p>不同的分支，可以有不同的channel，但要有相同的width、height。</p><p>pooling也可以设置padding=1、stride=1来保证输出大小一样。</p><p>1*1卷积，其数量取决于输入张量的通道。</p><p>1*1卷积的信息融合，是在每一个像素点多通道方面的融合。</p><p>1*1卷积主要解决运算量过大的问题，可以减少下一层输入通道数量。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/image-20210119191134365.png" alt="GoogleNet"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/image-20210120113458095.png" alt="image-20210120113458095"></p><p>最后输出的大小一般会去掉线性层，先实例化跑一遍输出size。</p><p>在写网络时，要加上一个存盘功能，即每次准确率达到新高时做一次模型数据备份，防止意外。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare dataset</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment"># Compose参数列表：转为张量；归一化,均值和方差</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># design model using class</span></span><br><span class="line"><span class="comment"># network in network</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionA</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line">        <span class="comment"># 4条分支</span></span><br><span class="line">        <span class="comment"># 1. 1*1卷积</span></span><br><span class="line">        self.branch1x1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 1*1卷积+5*5卷积</span></span><br><span class="line">        self.branch5x5_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)  <span class="comment"># 保持图像大小不变，kernel=5，则padding=2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 1*1卷积+3*3卷积+3*3卷积</span></span><br><span class="line">        self.branch3x3_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># 保持图像大小不变，kernel=3，则padding=1</span></span><br><span class="line">        self.branch3x3_3 = nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4, 池化(函数)+1*1卷积</span></span><br><span class="line">        self.branch_pool = nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"></span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">        branch3x3 = self.branch3x3_3(branch3x3)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 池化是函数，不需要训练，只在forward中调用</span></span><br><span class="line">        <span class="comment"># 池化也可以使图像大小不变。1. kernel_size=3，则padding=1；2. stride=1</span></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Concatenate</span></span><br><span class="line">        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)  <span class="comment"># b,c,w,h  c对应的是dim=1，沿着channel的维度拼接</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">88</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)  <span class="comment"># 88 = 24x3 + 16</span></span><br><span class="line"></span><br><span class="line">        self.incep1 = InceptionA(in_channels=<span class="number">10</span>)  <span class="comment"># 与conv1 中的10对应</span></span><br><span class="line">        self.incep2 = InceptionA(in_channels=<span class="number">20</span>)  <span class="comment"># 与conv2 中的20对应</span></span><br><span class="line"></span><br><span class="line">        self.mp = nn.MaxPool2d(<span class="number">2</span>)  <span class="comment"># MaxPooling</span></span><br><span class="line">        self.fc = nn.Linear(<span class="number">1408</span>, <span class="number">10</span>)  <span class="comment"># FullConnecting</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.mp(self.conv1(x)))</span><br><span class="line">        x = self.incep1(x)</span><br><span class="line">        x = F.relu(self.mp(self.conv2(x)))</span><br><span class="line">        x = self.incep2(x)</span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)  <span class="comment"># -1指在不告诉函数有多少列的情况下，根据原tensor数据和batch自动分配列数</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># training cycle</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># enumerate()用于可迭代\可遍历的数据对象组合为一个索引序列，同时列出数据和数据下标</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># data里面包含图像数据inputs(tensor)和标签labels(tensor)</span></span><br><span class="line">        inputs, target = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># update</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            print(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, batch_idx + <span class="number">1</span>, running_loss / <span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 不需要计算张量</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    print(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>``torch.max()[0]` 只返回最大值的每个数</p><p><code>troch.max()[1]</code> 只返回最大值的每个索引</p><p><code>torch.max()[1].data</code> 只返回variable中的数据部分（去掉Variable containing:）</p><p><code>torch.max()[1].data.numpy()</code> 把数据转化成numpy ndarry</p><p><code>torch.max()[1].data.numpy().squeeze()</code> 把数据条目中维度为1 的删除掉`</p><h2 id="残差网络-Residual-Net"><a href="#残差网络-Residual-Net" class="headerlink" title="残差网络(Residual Net)"></a>残差网络(Residual Net)</h2><p>随着网络层数增加，越靠近输入模块的梯度更新就越慢，很可能导致<strong>梯度消失</strong>。</p><p>为了解决梯度消失，会在激活之前加入一个跳连接。</p><p>在使用Residual Block时要保持输入和输出通道相同。</p><p>Residual Block相当于把一串Weight Layer包裹起来。</p><p>写神经网络也要写测试方法，检验输出是否和预计相同，逐步增加网络规模（增量式开发）。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/image-20210120182501998.png" alt="image-20210120182501998"></p><blockquote><ol><li>从数学和工程学方面重新理解深度学习理论。《深度学习》花书。</li><li>通读PyTorch文档。</li><li>复现经典工作、论文。读代码→写代码</li><li>扩充视野</li></ol></blockquote><p>两篇论文：</p><ol><li>He K, Zhang X, Ren S, et al. Identity Mappings in Deep Residual Networks[C]</li><li>Huang G, Liu Z, Laurens V D M, et al. Densely Connected Convolutional Networks[J]. 2016:2261-2269.</li></ol>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part10</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part10——卷积神经网络（基础篇）"><a href="#PyTorch深度学习实践Part10——卷积神经网络（基础篇）" class="headerlink" title="PyTorch深度学习实践Part10——卷积神经网络（基础篇）"></a>PyTorch深度学习实践Part10——卷积神经网络（基础篇）</h1><h2 id="Basic-CNN"><a href="#Basic-CNN" class="headerlink" title="Basic CNN"></a>Basic CNN</h2><p>CNN(Convolutional Neural Network)结构：特征提取+分类。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118185651529.png" alt="image-20210118185651529"></p><p>通道(Channel)×纵轴(Width)×横轴(Height)，起点为左上角。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118194753606.png" alt="image-20210118194753606"></p><p>Patch逐Width扫描，矩阵作数乘(哈达玛积)。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118195245850.png" alt="image-20210118195245850"></p><p>多通道的卷积中，每一个通道都要配一个卷积核，并相加。</p><p>深度学习里的卷积是数学中的互相关，但是惯例称为卷积，和数学中的卷积有点不同，但是不影响。</p><p>n*n的卷积核，上下各-(n-1)/2，原长宽-(n-1)。n一般采用奇数，卷积形状一般都是正方形，在pytorch中奇偶、长方形都行。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118200458601.png" alt="image-20210118200458601"></p><p>每一组卷积核的通道数量要求和输入通道是一样的。这种卷积核组的总数和输出通道的数量是一样的。卷积过后，通道就与RGB没有关系了。</p><p>卷积(convolution)后，C(Channels)变，W(width)和H(Height)可变可不变，取决于是否填充边缘(padding)，不填充则会有边缘损失。</p><p>卷积层：保留图像的空间信息。卷积本质上也是线性计算，也是可以优化的权重。</p><p>卷积神经网络要求输入输出层是四维张量(Batch, Channel, Width, Height)，卷积层是(m输出通道数量, n输入通道数量, w卷积核宽, h卷积核长)，全连接层的输入与输出都是二维张量(B, Input_feature)。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118200942877.png" alt="image-20210118200942877"></p><p>下采样(subsampling)或池化(pooling)后，C不变，W和H变成 原长度/池化长度。（MaxPool2d是下采样常用的一种，n*n最大池化默认步长为n）</p><p>池化层与sigmoid一样，没有权重。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118222617886.png" alt="image-20210118222617886"></p><p>卷积(线性变换)，激活函数(非线性变换)，池化；这个过程若干次后，view打平，进入全连接层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">in_channels, out_channels = <span class="number">1</span>, <span class="number">10</span></span><br><span class="line">width, height = <span class="number">10</span>, <span class="number">10</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"><span class="built_in">input</span> = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">2</span>,</span><br><span class="line">         <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">7</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">7</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line"><span class="comment"># view()将其转化成4维</span></span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(batch_size, </span><br><span class="line">                                 in_channels, </span><br><span class="line">                                 width, </span><br><span class="line">                                 height)</span><br><span class="line"><span class="comment"># 卷积模型的构造函数中，输入通道数量在前，输出通道数量在后；但是卷积的权重shape是先输出后输入</span></span><br><span class="line"><span class="comment"># padding边缘填充，bias一般卷积不用加偏置，stride步长，kernel_size核大小</span></span><br><span class="line">conv_layer = torch.nn.Conv2d(in_channels,</span><br><span class="line">                             out_channels,</span><br><span class="line">                             kernel_size=kernel_size)</span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line">print(<span class="built_in">input</span>.shape)  <span class="comment"># torch.Size([1, 1, 10, 10])</span></span><br><span class="line">print(output.shape)  <span class="comment"># torch.Size([1, 10, 8, 8])</span></span><br><span class="line">print(conv_layer.weight.shape)  <span class="comment"># torch.Size([10, 1, 3, 3])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210119001547566.png" alt="image-20210119001547566"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210119002051268.png" alt="image-20210119002051268"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare dataset</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># design model using class</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)  <span class="comment"># 卷积</span></span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)  <span class="comment"># 池化</span></span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)  <span class="comment"># 线性</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># flatten data from (n,1,28,28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)  <span class="comment"># 先求batch，多少条记录</span></span><br><span class="line">        x = self.pooling(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pooling(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>)  <span class="comment"># -1 此处自动算出的是320</span></span><br><span class="line">        <span class="comment"># print(&quot;x.shape&quot;,x.shape)</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)  <span class="comment"># GPU加速</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># training cycle forward, backward, update</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        inputs, target = inputs.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            print(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, batch_idx + <span class="number">1</span>, running_loss / <span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            images, labels = images.to(device), labels.to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    print(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line">    <span class="keyword">return</span> correct / total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    epoch_list = []</span><br><span class="line">    acc_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        acc = test()</span><br><span class="line">        epoch_list.append(epoch)</span><br><span class="line">        acc_list.append(acc)</span><br><span class="line"></span><br><span class="line">    plt.plot(epoch_list, acc_list)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part9</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part9——多分类问题"><a href="#PyTorch深度学习实践Part9——多分类问题" class="headerlink" title="PyTorch深度学习实践Part9——多分类问题"></a>PyTorch深度学习实践Part9——多分类问题</h1><h2 id="二分类与多分类"><a href="#二分类与多分类" class="headerlink" title="二分类与多分类"></a>二分类与多分类</h2><ol><li><p>多输出之间会有抑制关系，不能用二分类分别对n个目标输出n次。</p></li><li><p>二分类对0/1只需要求对一个的概率就行，但是多分类需要研究分布差异。</p></li><li><p>中间层用Sigmoid变换，最终输出层用Softmax输出一个分布，将每个最终输出z都变化成<strong>大于0且和为1</strong>(先转正，再归一)。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/image-20210118152538935.png" alt="image-20210118152538935"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/image-20210118152129056.png" alt="image-20210118152129056"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/image-20210118153734193.png" alt="image-20210118153734193"></p></li><li><p>在使用交叉熵损失时，最后一层线性输出不用做激活变换。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/image-20210118153824744.png" alt="image-20210118153824744"></p></li><li><p>要理解 CrossEntropyLoss 和 LogSoftmax + NLLLoss 之间的区别</p><p>• <a href="https://pytorch.org/docs/stable/nn.html#crossentropyloss">https://pytorch.org/docs/stable/nn.html#crossentropyloss</a></p><p>• <a href="https://pytorch.org/docs/stable/nn.html#nllloss">https://pytorch.org/docs/stable/nn.html#nllloss</a></p></li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms  <span class="comment"># 针对图像进行的处理</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare dataset</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment"># 神经网络输入值在[-1,1]效果最好，服从正态分布</span></span><br><span class="line"><span class="comment"># 构建的是Compose类的对象，参数是列表[]</span></span><br><span class="line"><span class="comment"># transforms.ToTensor()：PIL Image =&gt; PyTorch Tensor，单通道变多通道</span></span><br><span class="line"><span class="comment"># transforms.Normalize((mean,), (std,)：归一化，正态分布需要的期望和标准差，映射到[0,1]分布。数据是算好的，换成标准之后可以解决梯度爆炸问题</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># design model using class</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">784</span>, <span class="number">512</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.linear4 = torch.nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear5 = torch.nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">784</span>)  <span class="comment"># -1自动获取mini-batch：N。把样本[N,1,28,28]转变成[N,784]</span></span><br><span class="line">        x = F.relu(self.linear1(x))</span><br><span class="line">        x = F.relu(self.linear2(x))</span><br><span class="line">        x = F.relu(self.linear3(x))</span><br><span class="line">        x = F.relu(self.linear4(x))</span><br><span class="line">        <span class="keyword">return</span> self.linear5(x)  <span class="comment"># 最后一层不做非线性变换</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)  <span class="comment"># momentum冲量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># training cycle</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># update</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            print(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, batch_idx + <span class="number">1</span>, running_loss / <span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 包裹的一部分不会构建计算图</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)  <span class="comment"># 列是dim=0，行是dim=1。返回最大值和最大值的下标</span></span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()  <span class="comment"># 序列求和，一共猜对的数量</span></span><br><span class="line">    print(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br><span class="line"></span><br><span class="line"><span class="comment"># [1,   300] loss: 2.244</span></span><br><span class="line"><span class="comment"># [1,   600] loss: 1.005</span></span><br><span class="line"><span class="comment"># [1,   900] loss: 0.437</span></span><br><span class="line"><span class="comment"># ..........................</span></span><br><span class="line"><span class="comment"># [8,   300] loss: 0.045</span></span><br><span class="line"><span class="comment"># [8,   600] loss: 0.051</span></span><br><span class="line"><span class="comment"># [8,   900] loss: 0.048</span></span><br><span class="line"><span class="comment"># accuracy on test set: 97 % </span></span><br><span class="line"><span class="comment"># [9,   300] loss: 0.034</span></span><br><span class="line"><span class="comment"># [9,   600] loss: 0.039</span></span><br><span class="line"><span class="comment"># [9,   900] loss: 0.044</span></span><br><span class="line"><span class="comment"># accuracy on test set: 97 % </span></span><br><span class="line"><span class="comment"># [10,   300] loss: 0.030</span></span><br><span class="line"><span class="comment"># [10,   600] loss: 0.027</span></span><br><span class="line"><span class="comment"># [10,   900] loss: 0.036</span></span><br><span class="line"><span class="comment"># accuracy on test set: 96 %</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li><a href="https://blog.csdn.net/Answer3664/article/details/99460175">torch.no_grad()</a>  <a href="https://blog.csdn.net/ego_bai/article/details/80873242">Python中with的用法</a></li><li><a href="https://zhuanlan.zhihu.com/p/105783765?utm_source=com.miui.notes">Python中各种下划线的操作</a> </li><li><a href="https://blog.csdn.net/Z_lbj/article/details/79766690">torch.max( )的用法</a> <a href="https://blog.csdn.net/qq_40210586/article/details/103874000">torch.max( )使用讲解</a></li><li>用全连接神经网络训练图像会忽略局部信息的利用，在距离很远的两个点都会产生联系，而这个是没必要的。</li><li>图像的特征提取：傅里叶变换（缺点：都是正弦波）、Wavelet、小波。但是这些都是人工提取。自动提取的有：CNN</li></ol>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part8</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part8——加载数据集"><a href="#PyTorch深度学习实践Part8——加载数据集" class="headerlink" title="PyTorch深度学习实践Part8——加载数据集"></a>PyTorch深度学习实践Part8——加载数据集</h1><h2 id="Dataset-and-DataLoader"><a href="#Dataset-and-DataLoader" class="headerlink" title="Dataset and DataLoader"></a>Dataset and DataLoader</h2><ol><li><p>Dataset：主要构造数据集，支持索引。</p></li><li><p>DataLoader：主要能拿出mini-batch，拿出一组组数据以快速使用。</p><p>改成mini-batch之后，训练循环会变成一个二层的嵌套循环，第一层迭代epoch，第二层迭代mini-batch。</p></li><li><p>Epoch：将所有的样本都参与了一次正向传播、训练，是一次epoch。</p></li><li><p>Batch-Size：每次训练(前馈+反馈+更新)所用的样本数量。</p></li><li><p>Iteration：batch分了多少批，内层的迭代执行多少次。例如：1w个样本，1k个batch，iteration为10。</p></li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/image-20210118100748425.png" alt="image-20210118100748425"></p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>处理数据方式</p><ol><li>全部读取到内存，适用于关系表或者小批量结构化的数据。</li><li>将数据文件分开，路径存放在列表中打包，适用于图像、音频等非结构化数据。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset  <span class="comment"># Dataset是抽象类，需要继承</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理数据</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filepath</span>):</span>  <span class="comment"># 初始化，提供数据集路径加载</span></span><br><span class="line">        xy = np.loadtxt(filepath, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]  <span class="comment"># shape(行数,列数)是元组</span></span><br><span class="line">        self.x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span>  <span class="comment"># 获取数据索引</span></span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]  <span class="comment"># 返回的是元组</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span>  <span class="comment"># 获取数据总量</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = DiabetesDataset(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>)</span><br><span class="line"><span class="comment"># shuffle=True打乱mini-batch保证随机，num_workers多线程</span></span><br><span class="line">train_loader = DataLoader(dataset=dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.activate = torch.nn.Sigmoid()  <span class="comment"># 是模块而不是函数，没有参数，没有需要训练的地方，只用来构建计算图</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.activate(self.linear1(x))</span><br><span class="line">        x = self.activate(self.linear2(x))</span><br><span class="line">        x = self.activate(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失&amp;优化</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"><span class="comment"># model.parameters()会扫描module中的所有成员，如果成员中有相应权重，那么都会将结果加到要训练的参数集合上</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1</span>)</span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  <span class="comment"># 在windows系统下要用if封装训练循环，否则会报错</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        loss_sum = <span class="number">0</span></span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            <span class="comment"># prepare data</span></span><br><span class="line">            inputs, labels = data  <span class="comment"># 此时两个已经转化成tensor</span></span><br><span class="line">            <span class="comment"># Forward</span></span><br><span class="line">            y_pred = model(inputs)</span><br><span class="line">            loss = criterion(y_pred, labels)</span><br><span class="line">            print(epoch, i, loss.item())</span><br><span class="line"></span><br><span class="line">            loss_sum += loss.item()</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Backward</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># Update</span></span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        loss_list.append(loss_sum / num)</span><br><span class="line"></span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), loss_list)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/image-20210118110538070.png" alt="image-20210118110538070"></p><p>二层循环速度反而变慢了，效率也没有很大提升？t</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part7</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part7——处理多维特征的输入"><a href="#PyTorch深度学习实践Part7——处理多维特征的输入" class="headerlink" title="PyTorch深度学习实践Part7——处理多维特征的输入"></a>PyTorch深度学习实践Part7——处理多维特征的输入</h1><h2 id="多维特征输入"><a href="#多维特征输入" class="headerlink" title="多维特征输入"></a>多维特征输入</h2><p>从单一特征的数据，转而输入多为特征的数据，模型发生以下改变：</p><ol><li><p>对于每一条(/第i条)有n个特征(x1…xn)的数据，则有n个不同的weight(w1…wn)和1个相同的bias(b将进行广播)，并通过非线性激活函数，得出一个y_hat（假设输出维度为1）。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117203233443.png" alt="image-20210117203233443"></p></li><li><p>对于每个zn(=xn*wn+b)都要通过非线性激活函数，Sigmoid函数是对于每个元素的，类似于numpy。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117203726634.png" alt="image-20210117203726634"></p></li><li><p>转换成矩阵运算可以发挥cpu/gpu并行运算的优势。</p></li><li><p>在之前的代码上，想要进行多维输入，只需要修改样本以及模型构造函数。</p></li></ol><h2 id="增加神经网络层数"><a href="#增加神经网络层数" class="headerlink" title="增加神经网络层数"></a>增加神经网络层数</h2><ol><li>如何增加神经网络层数？将多层模型输入和输出，<strong>头尾相连</strong>。例如：torch.nn.Linear(8, 6)、torch.nn.Linear(6, 4)、torch.nn.Linear(4, 1)。</li><li>什么是矩阵？矩阵是<strong>空间变换函数</strong>。例如：y=A*x，y是M×1的矩阵，x是N×1的矩阵，A是M×N的矩阵，则A就是将x从N维转换到y这个M维空间的空间变换函数。</li><li>矩阵是线性变换，但是很多实际情况都是复杂、非线性的。所以，需要用多个线性变换层，通过找到最优的权重组合起来，来模拟非线性的变换。<strong>寻找非线性变换函数</strong>，就是神经网络的本质。</li><li>多层神经网络可以降维也可以升维，至于如何达到最优，则是<strong>超参数的搜索</strong>。</li><li>神经元、网络层数越多，学习能力就越强，但是同时要小心过拟合的问题。要学习数据真值和具备泛化的能力。</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117212227590.png" alt="image-20210117212227590"></p><blockquote><p><strong>能在编程道路上立稳脚跟的核心能力：</strong></p><ol><li>读文档</li><li>基本架构理念(cpu、操作系统、主机、编译原理)</li></ol></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理数据</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)  <span class="comment"># 分隔符&#x27;,&#x27;，大多数显卡只支持32位float</span></span><br><span class="line">x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])  <span class="comment"># 左闭右开，取所有行、第一列到最后第二列。torch.from_numpy返回tensor</span></span><br><span class="line">y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])  <span class="comment"># 取所有行、最后一列。[-1]表示拿出来的是矩阵，-1表示拿出来的是向量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.activate = torch.nn.Sigmoid()  <span class="comment"># 是模块而不是函数，没有参数，没有需要训练的地方，只用来构建计算图</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.activate(self.linear1(x))</span><br><span class="line">        x = self.activate(self.linear2(x))</span><br><span class="line">        x = self.activate(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失&amp;优化</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1.0</span>)</span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500000</span>):</span><br><span class="line">    <span class="comment"># Forward</span></span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># 这里并没有用到mini-batch</span></span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    print(epoch, loss.item())</span><br><span class="line">    loss_list.append(loss.item())</span><br><span class="line">    <span class="comment"># Backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># Update</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">500000</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同网络层参数</span></span><br><span class="line">layer1_weight = model.linear1.weight.data</span><br><span class="line">layer1_bias = model.linear1.bias.data</span><br><span class="line">print(<span class="string">&quot;layer1_weight&quot;</span>, layer1_weight)</span><br><span class="line">print(<span class="string">&quot;layer1_weight.shape&quot;</span>, layer1_weight.shape)</span><br><span class="line">print(<span class="string">&quot;layer1_bias&quot;</span>, layer1_bias)</span><br><span class="line">print(<span class="string">&quot;layer1_bias.shape&quot;</span>, layer1_bias.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li>在100次训练时，损失卡在了0.65。</li><li>在1w次训练时，损失跨过0.65停在了0.45。</li><li>将学习率提升到10.0，1w次训练可以看出图像震荡，无法收敛，但是损失突破0.4以下。</li><li>将学习率调整到1.0，10w次训练，损失突破0.3以下</li><li>学习率1.0，50w次训练，损失达到0.28</li><li><a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">pytorch激活函数文档</a></li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117233025403.png" alt="image-20210117233025403"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117235832948.png" alt="image-20210117235832948"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part6</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part6——逻辑斯蒂回归"><a href="#PyTorch深度学习实践Part6——逻辑斯蒂回归" class="headerlink" title="PyTorch深度学习实践Part6——逻辑斯蒂回归"></a>PyTorch深度学习实践Part6——逻辑斯蒂回归</h1><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>逻辑斯蒂回归是处理分类问题，而不是回归任务。</p><p>处理分类问题，不能使用回归的思想，即使输出可以为0或1。原因在于：若有一个0-9，10个手写数字的分类问题，在回归模型中，1和0距离很近，0和9离得很远，但是在分类模型中，7和9的相似度就比8与7或9的相似度要高。</p><p>分类问题本质上输出的是概率，例如P(0)、P(1)…。</p><h3 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h3><p>通过考试的概率是多少</p><h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><p>0-9手写数字检测分类</p><h2 id="torchvision工具包"><a href="#torchvision工具包" class="headerlink" title="torchvision工具包"></a>torchvision工具包</h2><p>指定目录，训练/测试，是否需要下载</p><p>MNIST、CIFAR10…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">train_set = torchvision.datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="饱和函数"><a href="#饱和函数" class="headerlink" title="饱和函数"></a>饱和函数</h2><h3 id="逻辑斯蒂函数"><a href="#逻辑斯蒂函数" class="headerlink" title="逻辑斯蒂函数"></a>逻辑斯蒂函数</h3><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117161720715.png" alt="image-20210117161720715"></p><h3 id="其他Sigmoid-functions"><a href="#其他Sigmoid-functions" class="headerlink" title="其他Sigmoid functions"></a>其他Sigmoid functions</h3><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117161834404.png" alt="image-20210117161834404"></p><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><ol><li><p>Logistic Regression类似于正态分布。</p></li><li><p>Logistic Regression是Sigmoid functions中最著名的，所以有些地方用Sigmoid指代Logistic。</p></li><li><p>逻辑斯蒂回归和线性模型的明显区别是在线性模型的后面，添加了激活函数(非线性变换)，将y_hat代入逻辑斯蒂公式中的x。</p></li><li><p><a href="https://blog.csdn.net/C_chuxin/article/details/86174807">交叉熵损失函数的推导过程与直观理解</a></p></li><li><p>y_hat是预测的值[0,1]之间的概率，y是真实值，预测与标签越接近，BCE损失越小。</p></li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117164616620.png" alt="image-20210117164616620"></p><blockquote><p>要计算的是分布的差异，而不是数值上的距离</p></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><a href="https://blog.csdn.net/weixin_42621901/article/details/107664771">torch.sigmoid()、torch.nn.Sigmoid()和torch.nn.functional.sigmoid()三者之间的区别</a></li><li>BCELoss(Binary CrossEntropyLoss)是CrossEntropyLoss的一个特例，只用于二分类问题，而CrossEntropyLoss可以用于二分类，也可以用于多分类。</li><li><a href="https://www.cnblogs.com/samwoog/p/13857843.html">BCE和CE交叉熵损失函数的区别</a></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------Prepare dataset----------------------------#</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])  <span class="comment"># 二分类</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------Design model using Class----------------------------#</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># nn.functional.sigmoid is deprecated</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(self.linear(x))  <span class="comment"># 激活函数sigmoid不需要参数训练，直接调用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line"><span class="comment"># --------------------------Construct loss and optimizer-----------------------------#</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># 交叉熵，size_average=False已经被弃用</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># --------------------------Training cycle-----------------------------#</span></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># 正向传播</span></span><br><span class="line">    loss = criterion(y_pred, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    print(epoch, loss.item())</span><br><span class="line">    loss_list.append(loss.item())  <span class="comment"># 保存loss</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将梯度归零</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line">print(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line">print(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.item())</span><br><span class="line"></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1000</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># y_pred =  0.8808996081352234</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117172329760.png" alt="image-20210117172329760"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part5</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part5——线性回归"><a href="#PyTorch深度学习实践Part5——线性回归" class="headerlink" title="PyTorch深度学习实践Part5——线性回归"></a>PyTorch深度学习实践Part5——线性回归</h1><h2 id="PyTorch周期"><a href="#PyTorch周期" class="headerlink" title="PyTorch周期"></a>PyTorch周期</h2><ol><li>prepare dataset</li><li>design model using Class 目的是为了前馈forward，即计算y hat(预测值)</li><li>Construct loss and optimizer (using PyTorch API) 其中，计算loss是为了进行反向传播，optimizer是为了更新梯度。</li><li>Training cycle (<u><strong><em>forward,backward,update</em></strong></u>)</li></ol><h2 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h2><p>原本w只是1×1的矩阵，比如tensor([0.], requires_grad=True)，很有可能行列数量与xy对不上，这个时候pytorch会进行<strong>广播</strong>，将w<strong>扩展成一个3×1矩阵</strong>。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117122903497.png" alt="image-20210117122903497"></p><blockquote><p>pytorch直接写“*”表示矩阵<strong>对应位置元素相乘</strong>（哈达玛积），数学上的矩阵乘法有另外的函数torch.matmul</p></blockquote><p>这里x、y的维度都是1（有可能不是1），但是都应当看成一个<strong>矩阵</strong>，而不能是向量。</p><blockquote><p>x、y的列是维度/特征，行是记录/样本</p></blockquote><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ol><li>要把计算模型定义成一个类，<strong>继承于torch.nn.Model</strong>。（nn：neural network）</li><li>如果有pytorch没有提供的需求，或者其效率不够高，可以从Function中继承，构造自己的计算块。</li><li>Linear类包括成员变量weight和bias，默认bias=True，同样继承于torch.nn.Model，可以进行反向传播。</li><li>权重放在x右边，或者转置放在左边。（不管怎么放都是为了凑矩阵基本积）</li><li>父类实现了callable函数，让其能够被调用。在call中会调用前馈forward()，所以必须重写forward()。</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117132843838.png" alt="image-20210117132843838"></p><p>*args, **kwargs的用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;args:&#x27;</span>, args)</span><br><span class="line">    print(<span class="string">&#x27;kwargs:&#x27;</span>, kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fun(<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>, x=<span class="number">6</span>, y=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># args: (1, 2, 7)</span></span><br><span class="line"><span class="comment"># kwargs: &#123;&#x27;x&#x27;: 6&#125;</span></span><br></pre></td></tr></table></figure><h2 id="损失-amp-优化"><a href="#损失-amp-优化" class="headerlink" title="损失&amp;优化"></a>损失&amp;优化</h2><ol><li>计算损失使用现成的类torch.nn.MSELoss。</li><li>一般使用随机梯度下降算法，求和平均是没有必要的，torch.nn.MSELoss(size_average=<strong>False</strong>)</li><li>使用现成的优化器类torch.optim.SGD</li><li><a href="https://pytorch.org/docs/1.7.0/optim.html">不同的优化器，官方文档</a></li><li>控制训练次数，不能过少（训练不到位），也不能过多（过拟合）</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据，要是矩阵</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型，继承、重写</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成模型的对象</span></span><br><span class="line">model = LinearModel()</span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># size_average=False已经被弃用</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># __call__()调用forward()正向传播计算预测值</span></span><br><span class="line">    loss = criterion(y_pred, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    print(epoch, loss.item())  <span class="comment"># 可以直接打印loss，因为调用的是__str__()不会产生计算图</span></span><br><span class="line">    loss_list.append(loss.item())  <span class="comment"># 保存loss</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将梯度归零</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line">print(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)  <span class="comment"># 使用训练好的模型进行预测</span></span><br><span class="line">print(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印图表</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><p>测试不同的优化器。除了LBFGS，只需要修改调用对应优化器的构造器。</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>w =  0.20570674538612366</p><p>b =  -0.5057424902915955</p><p>y_pred =  0.31708449125289917</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117152240566.png" alt="image-20210117152240566"></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>w =  1.466607928276062</p><p>b =  0.14079217612743378</p><p>y_pred =  6.007224082946777</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117152322232.png" alt="image-20210117152322232"></p><h3 id="Adamax"><a href="#Adamax" class="headerlink" title="Adamax"></a>Adamax</h3><p>w =  -0.022818174213171005</p><p>b =  0.9245702028274536</p><p>y_pred =  0.8332974910736084</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117152149494.png" alt="image-20210117152149494"></p><h3 id="ASGD"><a href="#ASGD" class="headerlink" title="ASGD"></a>ASGD</h3><p>w =  1.6153326034545898</p><p>b =  0.87442547082901</p><p>y_pred =  7.335755825042725</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117151915659.png" alt="image-20210117151915659"></p><h3 id="LBFGS"><a href="#LBFGS" class="headerlink" title="LBFGS"></a>LBFGS</h3><p>由于LBFGS算法需要重复多次计算函数，因此需要传入一个闭包去允许它们重新计算模型。这个闭包应当清空梯度， 计算损失，然后返回。</p><p>参考<a href="https://blog.csdn.net/ys1305/article/details/94332643">一篇关于优化器的博文</a></p><p>训练模型部分代码应修改为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">closure</span>():</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将梯度归零</span></span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># __call__()调用forward()正向传播计算预测值</span></span><br><span class="line">    loss = criterion(y_pred, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    print(epoch, loss.item())  <span class="comment"># 可以直接打印loss，因为调用的是__str__()不会产生计算图</span></span><br><span class="line">    loss_list.append(loss.item())  <span class="comment"># 保存loss</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.step(closure())  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>w =  1.7660775184631348</p><p>b =  0.531760573387146</p><p>y_pred =  7.596070766448975</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117153648037.png" alt="image-20210117153648037"></p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>w =  1.734222650527954</p><p>b =  0.5857117176055908</p><p>y_pred =  7.522602081298828</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117150905304.png" alt="image-20210117150905304"></p><h3 id="Rprop"><a href="#Rprop" class="headerlink" title="Rprop"></a>Rprop</h3><p>w =  1.9997763633728027</p><p>b =  0.0004527860146481544</p><p>y_pred =  7.999558448791504</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/Rprop.png" alt="image-20210117150540886"></p><h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>w =  1.8483222723007202</p><p>b =  0.3447989821434021</p><p>y_pred =  7.738088130950928</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/SGD.png" alt="image-20210117150219223"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part4</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part4——反向传播"><a href="#PyTorch深度学习实践Part4——反向传播" class="headerlink" title="PyTorch深度学习实践Part4——反向传播"></a>PyTorch深度学习实践Part4——反向传播</h1><p>对于简单模型可以手动求解析式，但是对于复杂模型求解析式几乎不可能。</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>每一层神经网络包括一次矩阵乘法(Matrix Multiplication)、一次向量加法、非线性变化函数(为了防止展开函数而导致深层神经网络无意义)</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/image-20210116225529823.png" alt="image-20210116225529823"></p><h2 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h2><p>在pytorch中，梯度存在变量而不是计算模块里。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/image-20210116230629292.png" alt="image-20210116230629292"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/image-20210117091957454.png" alt="image-20210117091957454"></p><p>在计算过程中中，虽然有些变量可以不求导，但是一样要具备能够求导的能力。比如x的值就有可能是前一层网络的y_hat传递下来的。</p><p>核心在于梯度，loss虽然不会作为变量参与计算过程，但是同样需要保留，作为图像数据来判断最终是否收敛。</p><h2 id="PyTorch实现反向传播"><a href="#PyTorch实现反向传播" class="headerlink" title="PyTorch实现反向传播"></a>PyTorch实现反向传播</h2><p>线性模型y=w*x，用pytorch实现反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = torch.Tensor([<span class="number">1.0</span>])  <span class="comment"># data必须是一个序列</span></span><br><span class="line">w.requires_grad = <span class="literal">True</span>  <span class="comment"># 需要计算梯度</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w  <span class="comment"># w是Tensor，运算符重载，x也会转成tensor</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span>  <span class="comment"># 看到代码一定要有意识想到如何构建计算图</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        l = loss(x, y)  <span class="comment"># 前馈过程，正式构建计算图，计算损失更新l</span></span><br><span class="line">        l.backward()  <span class="comment"># 反向传播，计算梯度，释放计算图</span></span><br><span class="line">        print(<span class="string">&#x27;\tgrad:&#x27;</span>, x, y, w.grad.item())</span><br><span class="line">        w.data = w.data - <span class="number">0.01</span> * w.grad.data  <span class="comment"># 更新权重w，注意grad也是一个tensor，不使用.data的话相当于在构建计算图</span></span><br><span class="line"></span><br><span class="line">        w.grad.data.zero_()  <span class="comment"># 将梯度w.grad.data清零</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;progress:&#x27;</span>, epoch, l.item())  <span class="comment"># 取出loss使用l.item，不要直接使用l（l是tensor会构建计算图）</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"><span class="comment"># predict (after training) 4 7.999998569488525</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><p>二次模型y=w1<em>x²+w2</em>x+b的反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># y = x ** 2 + 2 * x + 1</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">4.0</span>, <span class="number">9.0</span>, <span class="number">16.0</span>]</span><br><span class="line"></span><br><span class="line">w1 = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w2 = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">b = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w1.requires_grad, w2.requires_grad, b.requires_grad = <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> w1 * x * x + w2 * x + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        l = loss(x, y)  <span class="comment"># 前馈过程，正式构建计算图，计算损失更新l</span></span><br><span class="line">        l.backward()  <span class="comment"># 反向传播，计算梯度，释放计算图</span></span><br><span class="line">        print(<span class="string">&#x27;\tgrad:&#x27;</span>, x, y, w1.grad.item(), w2.grad.item(), b.grad.item())</span><br><span class="line">        w1.data = w1.data - <span class="number">0.01</span> * w1.grad.data</span><br><span class="line">        w2.data = w2.data - <span class="number">0.01</span> * w2.grad.data</span><br><span class="line">        b.data = b.data - <span class="number">0.01</span> * b.grad.data</span><br><span class="line">        w1.grad.data.zero_()</span><br><span class="line">        w2.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    print(<span class="string">&#x27;progress:&#x27;</span>, epoch, l.item())  <span class="comment"># 取出loss使用l.item，不要直接使用l（l是tensor会构建计算图）</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line">print(w1.data, w2.data, b.data)</span><br><span class="line"><span class="comment"># predict (after training) 4 25.259323120117188</span></span><br><span class="line"><span class="comment"># tensor([1.1145]) tensor([1.4928]) tensor([1.4557])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>改了一下原本的数据集，更符合二次函数，但是因为样本量过少，预测的权重并不是很好。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part3</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part3——梯度下降算法"><a href="#PyTorch深度学习实践Part3——梯度下降算法" class="headerlink" title="PyTorch深度学习实践Part3——梯度下降算法"></a>PyTorch深度学习实践Part3——梯度下降算法</h1><h2 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h2><ol><li><p>上讲是穷举所有可能值并肉眼搜索损失最低点。</p></li><li><p>分治法可能错失关键，最终只找到局部最优</p><blockquote><p>穷举和分治都不能有效解决大数据</p></blockquote></li><li><p>梯度(gradient)决定权重w往哪个方向走，梯度即成本对权重求导，为了控制步伐需要设定一个较小的学习率。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/image-20210116203946162.png" alt="image-20210116203946162"></p></li><li><p>在大量的实验中发现，其实很多情况下，我们很难陷入到局部最优点。但是存在另外一个问题，鞍点。鞍点会导致无法继续迭代，可以选择通过引入动量解决。</p></li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">xs, ys</span>):</span></span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):  <span class="comment"># 3行数据</span></span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred - y) ** <span class="number">2</span>  <span class="comment"># 累加损失平方</span></span><br><span class="line">    <span class="keyword">return</span> cost / <span class="built_in">len</span>(xs)  <span class="comment"># 平均</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">xs, ys</span>):</span></span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        grad += <span class="number">2</span> * x * (x * w - y)  <span class="comment"># 成本对权重求导</span></span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Predict(before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">mse_list = []  <span class="comment"># 保存损失的变化曲线</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost_val = cost(x_data, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    mse_list.append(cost_val)  <span class="comment"># 记录损失变化</span></span><br><span class="line">    grad_val = gradient(x_data, y_data)  <span class="comment"># 计算梯度</span></span><br><span class="line">    w -= <span class="number">0.01</span> * grad_val  <span class="comment"># 改善权重</span></span><br><span class="line">    print(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line">print(<span class="string">&#x27;Predict(after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图表打印</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/image-20210116215632394.png" alt="image-20210116215632394"></p><ol><li>绘图时想要消除局部震荡，可以使用指数加权均值方法，使其变成更加平滑的曲线</li><li>如果训练的图像发散，则表明这次训练失败了。其原因有很多，比如，学习率取太大。</li></ol><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>使用梯度下降方法时，更加常用随机梯度下降(Stochastic Gradient Descent)。</p><p>随机梯度下降也是跨越鞍点的一种方法，同时也可以大幅减少计算量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x * (x * w - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Predict(before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">mse_list = []  <span class="comment"># 保存损失的变化曲线</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    x, y, i = <span class="number">0</span>, <span class="number">0</span>, random.randint(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 样本原本就是随机的，所以不需要打乱样本</span></span><br><span class="line">    <span class="keyword">for</span> m, n, j <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data, <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)):</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            x, y = m, n  <span class="comment"># 3组中随机选取一组</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    grad = gradient(x, y)  <span class="comment"># 计算梯度</span></span><br><span class="line">    w -= <span class="number">0.01</span> * grad  <span class="comment"># 改善权重</span></span><br><span class="line">    cost_val = loss(x, y)  <span class="comment"># 计算损失</span></span><br><span class="line">    mse_list.append(cost_val)  <span class="comment"># 记录损失变化</span></span><br><span class="line">    print(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line">print(<span class="string">&#x27;Predict(after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图表打印</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/image-20210116222847776.png" alt="image-20210116222847776"></p><p>随机梯度下降可能享受不到并行计算的效率加成，因此会使用折中方法，批量随机梯度下降(Mini-Batch/Batch)</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part2</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part2——线性模型"><a href="#PyTorch深度学习实践Part2——线性模型" class="headerlink" title="PyTorch深度学习实践Part2——线性模型"></a>PyTorch深度学习实践Part2——线性模型</h1><h2 id="一般过程"><a href="#一般过程" class="headerlink" title="一般过程"></a>一般过程</h2><ol><li>Data Set </li><li>Model（神经网络、决策树、朴素贝叶斯）</li><li>Trainning</li><li>Infering</li></ol><h2 id="训练-amp-测试"><a href="#训练-amp-测试" class="headerlink" title="训练&amp;测试"></a>训练&amp;测试</h2><h3 id="训练集拆分"><a href="#训练集拆分" class="headerlink" title="训练集拆分"></a>训练集拆分</h3><p>在竞赛中，训练集是可见的，测试集一般是不可见的。为了提高或验证模型的准确度，一般会把手中的训练集拆分，以及交叉验证。</p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>当模型对训练集的噪声也学习进去的时候，对训练集以外的数据可能会出现准确率下降的情况。因此一个好的模型需要有良好的泛化能力。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/image-20210116101533492.png" alt="image-20210116101533492"></p><p>平均平方误差（MSE MeanSquareError）</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前馈</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 存放结果，所有权重和对应的均方差</span></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"><span class="comment"># 穷举所有权重0.0-4.1步长0.1</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    print(<span class="string">&#x27;w=&#x27;</span>, w)</span><br><span class="line">    l_sum = <span class="number">0</span>  <span class="comment"># 损失的和</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):  <span class="comment"># 打包，一共三行</span></span><br><span class="line">        y_pred_val = forward(x_val)  <span class="comment"># 前馈算出此权重和样本得出的预测值，其实已经包含在loss()中，只是为了打印</span></span><br><span class="line">        loss_val = loss(x_val, y_val)  <span class="comment"># 计算该权重预测值得损失</span></span><br><span class="line">        l_sum += loss_val  <span class="comment"># 求损失和</span></span><br><span class="line">        print(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)  <span class="comment"># 当前的x、y值、预测值、损失</span></span><br><span class="line">    print(<span class="string">&#x27;MSE=&#x27;</span>, l_sum / <span class="number">3</span>)  <span class="comment"># 求损失的平均</span></span><br><span class="line">    <span class="comment"># 保存记录</span></span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum / <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 图表打印</span></span><br><span class="line">plt.plot(w_list, mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/image-20210116155722125.png" alt="image-20210116155722125"></p><blockquote><p>做深度学习要定期存盘，防止意外导致数据丢失</p></blockquote><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里设函数为y=3x+2</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">5.0</span>, <span class="number">8.0</span>, <span class="number">11.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.arange()左闭右开；2.打印 0.0、1.0 时只会显示 0.、1.；3.meshgrid()之后w、b都是41*41矩阵</span></span><br><span class="line"><span class="comment"># 4.这里前馈中是矩阵点对点的运算，但注意并不是矩阵运算，好处是省去了n层for循环，举例：</span></span><br><span class="line"><span class="comment"># a=[[1 2 3][1 2 3]]</span></span><br><span class="line"><span class="comment"># b=[[7 7 7][8 8 8]]</span></span><br><span class="line"><span class="comment"># a*b=[[ 7 14 21][ 8 16 24]]</span></span><br><span class="line">w_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">b_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">w, b = np.meshgrid(w_list, b_list)</span><br><span class="line"></span><br><span class="line">l_sum = <span class="number">0</span>  <span class="comment"># 损失的和</span></span><br><span class="line"><span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):  <span class="comment"># 遍历三次</span></span><br><span class="line">    <span class="comment"># y_pred_val、loss_val都是41*41的矩阵，即41个w和41个b组合的预测结果和损失</span></span><br><span class="line">    y_pred_val = forward(x_val)</span><br><span class="line">    loss_val = loss(x_val, y_val)</span><br><span class="line">    l_sum += loss_val</span><br><span class="line">    print(<span class="string">&#x27;\nx_val：&#x27;</span>, x_val,<span class="string">&#x27;\ny_val：&#x27;</span>, y_val, <span class="string">&#x27;\ny_pred_val：&#x27;</span>,y_pred_val, <span class="string">&#x27;\nloss_val：&#x27;</span>,loss_val)  <span class="comment"># 当前的x、y值、预测值、损失</span></span><br><span class="line">mse_list = l_sum / <span class="number">3</span></span><br><span class="line"><span class="comment"># mse_list也是一个ndarray类型的41*41矩阵</span></span><br><span class="line">print(<span class="string">&#x27;MSE=&#x27;</span>, mse_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3d图表</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(w, b, mse_list)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里设函数为y=3x+2</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">5.0</span>, <span class="number">8.0</span>, <span class="number">11.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里都是矩阵的运算</span></span><br><span class="line"><span class="comment"># 1.arange()左闭右开；2.打印 0.0、1.0 时只会显示 0.、1.；3.meshgrid之后w、b都是41*41矩阵</span></span><br><span class="line">w_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">b_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">w, b = np.meshgrid(w_list, b_list)</span><br><span class="line"></span><br><span class="line">l_sum = <span class="number">0</span>  <span class="comment"># 损失的和</span></span><br><span class="line"><span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):  <span class="comment"># 遍历三次</span></span><br><span class="line">    y_pred_val = forward(x_val)</span><br><span class="line">    loss_val = loss(x_val, y_val)</span><br><span class="line">    l_sum += loss_val</span><br><span class="line">    print(<span class="string">&#x27;\nx_val：&#x27;</span>, x_val,<span class="string">&#x27;\ny_val：&#x27;</span>, y_val, <span class="string">&#x27;\ny_pred_val：&#x27;</span>,y_pred_val, <span class="string">&#x27;\nloss_val：&#x27;</span>,loss_val)  <span class="comment"># 当前的x、y值、预测值、损失</span></span><br><span class="line">mse_list = l_sum / <span class="number">3</span></span><br><span class="line"><span class="comment"># mse_list也是一个ndarray类型的41*41矩阵</span></span><br><span class="line">print(<span class="string">&#x27;MSE=&#x27;</span>, mse_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3d图表</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(w, b, mse_list)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/image-20210116155626917.png" alt="image-20210116155626917"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part1</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part1——概论"><a href="#PyTorch深度学习实践Part1——概论" class="headerlink" title="PyTorch深度学习实践Part1——概论"></a>PyTorch深度学习实践Part1——概论</h1><h2 id="技术成熟度曲线"><a href="#技术成熟度曲线" class="headerlink" title="技术成熟度曲线"></a>技术成熟度曲线</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/3b87e950352ac65ca9eb5abaa5e3f21692138a21.jpeg" alt="技术成熟度曲线"></p><p><strong>科技诞生的促动期</strong> (Technology Trigger)：在此阶段，随着媒体大肆的报道过度，非理性的渲染，产品的知名度无所不在，然而随着这个科技的缺点、问题、限制出现，失败的案例大于成功的案例，例如:.com公司 1998~2000年之间的非理性疯狂飙升期。</p><p><strong>过高期望的峰值</strong>（Peak of Inflated Expectations）：早期公众的过分关注演绎出了一系列成功的故事——当然同时也有众多失败的例子。对于失败，有些公司采取了补救措施，而大部分却无动于衷。</p><p><strong>泡沫化的底谷期</strong> (Trough of Disillusionment)：在历经前面阶段所存活的科技经过多方扎实有重点的试验，而对此科技的适用范围及限制是以客观的并实际的了解，成功并能存活的经营模式逐渐成长。</p><p><strong>稳步爬升的光明期</strong> (Slope of Enlightenment)：在此阶段，有一新科技的诞生，在市面上受到主要媒体与业界高度的注意，例如:1996年的Internet ，Web。</p><p><strong>实质生产的高峰期</strong> (Plateau of Productivity)：在此阶段，新科技产生的利益与潜力被市场实际接受，实质支援此经营模式的工具、方法论经过数代的演进，进入了非常成熟的阶段。</p><blockquote><p>在使用pytorch或一系列新技术的时候，一定要学会看官方文档，这是一个非常重要的能力！</p></blockquote><h2 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210115233930514.png" alt="AI技术"></p><p>AI除了machine learning之外还有机器视觉、自然语言处理nlp、因果推断等。</p><p>机器学习大部分都是监督学习，即用一组标签过的值进行模型训练。</p><p>机器学习中的算法区别于普通的算法（穷举、贪心等），是通过数据训练并验证得出一个好用的模型，其计算过程来自于数据而不是人工的设计。</p><p>深度学习从模型上看用的是神经网络，从目标上看属于表示学习的分支。方法有，多层感知机、卷积神经网络、循环神经网络等。</p><h2 id="维度诅咒"><a href="#维度诅咒" class="headerlink" title="维度诅咒"></a>维度诅咒</h2><p>随着feature上升，为了保持准确性，其所需的数据量将急速上升，然而获取打过标签的数据，工作量大、成本高。</p><p>n<em>1的向量采样点需要一个3</em>n的矩阵来映射到3*1的向量，实现降维（PCA主成成分分析）。但是降维的同时也要尽量保证高维空间的度量信息，这个过程叫做表示学习（Present）。这个数据分布是在高维空间里的低维流行（Manifold）。</p><h2 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210116083615697.png" alt="image-20210116083615697"></p><h2 id="传统机器学习分类"><a href="#传统机器学习分类" class="headerlink" title="传统机器学习分类"></a>传统机器学习分类</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210116083809610.png" alt="image-20210116083809610"></p><h2 id="神经网络发展"><a href="#神经网络发展" class="headerlink" title="神经网络发展"></a>神经网络发展</h2><p>由生物实验得出，哺乳动物的视觉神经是分层的。浅层只检测物体的运动等，深层才开始识别物体的分类。由此出现了感知机。</p><p>现在神经网络早已不是生物的范畴，而是工程与数学方面。</p><p>真正让神经网络发展起来的是反向传播（Back Propagation），其核心在于计算图。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210116090403146.png" alt="image-20210116090403146"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>个人博客建站教程-完结目录</title>
      <link href="/2021/01/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E6%95%99%E7%A8%8B-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
      <url>/2021/01/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E6%95%99%E7%A8%8B-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="个人博客建站教程-完结目录"><a href="#个人博客建站教程-完结目录" class="headerlink" title="个人博客建站教程-完结目录"></a>个人博客建站教程-完结目录</h1><h2 id="个人博客网站教程"><a href="#个人博客网站教程" class="headerlink" title="个人博客网站教程"></a>个人博客网站教程</h2><p>其实不是一件很难的事，花个一点时间，祝每个人都能做出自己风格的博客小家。</p><table><thead><tr><th align="center"><a href="2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/">Part1——博客搭建与部署</a></th></tr></thead><tbody><tr><td align="center"><a href="2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/"><strong>Part2——主题安装与魔改</strong></a></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Hexo博客搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客网站 </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Butterfly主题安装和魔改</title>
      <link href="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/"/>
      <url>/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/</url>
      
        <content type="html"><![CDATA[<h1 id="安装butterfly"><a href="#安装butterfly" class="headerlink" title="安装butterfly"></a>安装butterfly</h1><ol><li>运行<code>git clone https://github.com/jerryc127/hexo-theme-butterfly themes/butterfly</code></li><li>打开_config.yml找到这一行<code>theme: landspace</code>然后将landspace替换butterfly</li><li>安装插件<code>cnpm install hexo-renderer-pug hexo-renderer-stylus</code></li><li>安装插件<code>cnpm install --save hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive</code></li></ol><blockquote><p>为了以后升级方便，这里不推荐直接对主题的配置文件进行修改，而是复制配置文件进行修改。个人推荐把主題的配置文件_config.yml复制到 Hexo 工作目录下的source/_data/butterfly.yml，如果目录不存在那就创建一个。</p></blockquote><h1 id="butterfly主题魔改"><a href="#butterfly主题魔改" class="headerlink" title="butterfly主题魔改"></a>butterfly主题魔改</h1><p>自己一开始动手做的时候大部分都参考Dreamy.TZK的博客</p><p><a href="https://www.antmoe.com/posts/75a6347a/index.html">Hexo安装并使用Butterfly主题</a></p><p>但是改到后来就越来越觉得，版本问题导致的主题修改不兼容，问题实在很大。甚至到后来想要获得自己的预期效果时，已经不得不去在源代码上下手<del>，因为还没有学过前端，改的属实面目全非</del>。</p><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>这里只能为想要自己动手改的小伙伴一些建议，比如想修改文章页，可以结合浏览器的开发者工具来找到相应的参数，来修改对应的值。</p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/Untitled.png" alt="Butterfly主题安装和魔改/Untitled.png"></p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/Untitled%201.png" alt="Butterfly主题安装和魔改/Untitled%201.png"></p><h2 id="相册的使用"><a href="#相册的使用" class="headerlink" title="相册的使用"></a>相册的使用</h2><p><a href="https://blog.ahzoo.cn/2020/07/20/b7201/">https://blog.ahzoo.cn/2020/07/20/b7201/</a></p><h2 id="关于文章中插入图片"><a href="#关于文章中插入图片" class="headerlink" title="关于文章中插入图片"></a>关于文章中插入图片</h2><p>先把hexo的配置文件中的 relative_link 参数确保为false。否则会导致butterfly各分页面的链接错乱</p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/image-20201231210020980.png" alt="image-20201231210020980"></p><p>把每个文章开头部分加一个参数 relative_link: true。使每个文章部分遵从相对位置的引用，这样可以将文章的图片，不仅在typora或是在服务器上，都能够实时看到自己文章图片引用的效果。</p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/image-20201231205840731.png" alt="image-20201231205840731"></p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/image-20201231210339382.png" alt="image-20201231210339382"></p><h2 id="关于分类管理-post下的文章"><a href="#关于分类管理-post下的文章" class="headerlink" title="关于分类管理_post下的文章"></a>关于分类管理_post下的文章</h2><p>主要参考<a href="https://blog.csdn.net/maosidiaoxian/article/details/85220394">如何在Hexo中对文章md文件分类</a></p><p>现在文章中的permalink:参数会完全覆盖_config.yml中的设置，要注意。</p><h1 id="后面应该还会慢慢更新一些有用的东西"><a href="#后面应该还会慢慢更新一些有用的东西" class="headerlink" title="后面应该还会慢慢更新一些有用的东西~"></a>后面应该还会慢慢更新一些有用的东西~</h1>]]></content>
      
      
      <categories>
          
          <category> Hexo博客搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hexo静态博客搭建和部署</title>
      <link href="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><h2 id="Nodejs"><a href="#Nodejs" class="headerlink" title="Nodejs"></a>Nodejs</h2><p><a href="https://nodejs.org/en/">Node.js</a></p><ul><li><code>node -v</code>确认nodejs版本，安装成功</li></ul><h2 id="Git-Bash"><a href="#Git-Bash" class="headerlink" title="Git Bash"></a>Git Bash</h2><p><a href="https://www.git-scm.com/download/win">Downloading Git</a></p><ul><li><code>git --version</code>确认nodejs版本，安装成功</li></ul><blockquote><p>nodejs和git自己选好安装位置之后无脑下一步就行。以下都以我个人的安装目录（D:\ProgrammingKits\nodejs 和 D:\ProgrammingKits\Git）为前提，请大家各自修改为自己的路径。</p></blockquote><h1 id="Nodejs插件安装"><a href="#Nodejs插件安装" class="headerlink" title="Nodejs插件安装"></a>Nodejs插件安装</h1><ul><li><p>最新的Nodejs自带npm，但是默认安装和缓存地址不在Nodejs根目录下</p><p>  npm的默认全局模块的安装地址是 C:\Users\Administrator\AppData\Roaming\npm</p><p>  npm的默认缓存的地址是 C:\Users\Administrator\AppData\Roaming\npm_cache</p></li></ul><p>首先修改nodejs的prefix（全局）和cache（缓存）文件夹地址</p><ul><li>运行<code>npm config set cache &quot;D:\ProgrammingKits\nodejs\node_cache&quot;</code>设置缓存文件夹</li><li>运行<code>npm config set prefix &quot;D:\ProgrammingKits\nodejs\nodejs&quot;</code>设置全局模块存放路径。</li></ul><p><del>这种方法可以不用像<code>npm config set prefix &quot;D:\ProgrammingKits\nodejs\node_global&quot;</code>需要修改环境变量。</del></p><p>以后npm和cnpm安装的全局模块都会被放到 D:\ProgrammingKits\nodejs\node_modules 下，跟自带的npm模块本体在一个文件夹中。</p><h2 id="cnpm"><a href="#cnpm" class="headerlink" title="cnpm"></a>cnpm</h2><p>这里安装淘宝的cnpm包管理器，以提高下载速度。</p><ul><li>运行<code>npm install -g cnpm --registry=http://registry.npm.taobao.org</code></li><li><code>cnpm -v</code> 确认cnpm版本，安装成功</li></ul><h2 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h2><p>静态博客框架</p><ul><li>运行<code>cnpm install -g hexo-cli</code> 安装hexo框架</li><li><code>hexo -v</code>确认hexo版本，安装成功</li></ul><h1 id="Hexo框架的使用"><a href="#Hexo框架的使用" class="headerlink" title="Hexo框架的使用"></a>Hexo框架的使用</h1><ul><li>hexo常用命令<ul><li><code>hexo init</code>初始化博客</li><li><code>hexo clean</code>清理缓存文件</li><li><code>hexo g</code>生成文件</li><li><code>hexo s</code>运行本地服务器</li><li><code>hexo d</code>部署到服务器</li><li><code>hexo n &quot;MyBlog&quot;</code>创建新的文章</li></ul></li></ul><p>现在我们只需要在 D:\MyBlog\HexoBlog 下运行<code>hexo init &amp; hexo s</code>，在浏览器中输入 <a href="http://localhost:4000/">localhost:4000</a> 即为最初始的博客内容。</p><h1 id="部署博客"><a href="#部署博客" class="headerlink" title="部署博客"></a>部署博客</h1><h2 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h2><p>用来存放你的代码/网站供别人访问</p><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled.png" alt="Hexo静态博客搭建和部署/Untitled.png"></p><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%201.png" alt="Hexo静态博客搭建和部署/Untitled%201.png"></p><h2 id="创建部署分支"><a href="#创建部署分支" class="headerlink" title="创建部署分支"></a>创建部署分支</h2><p>master用来放代码，ph-pages用来部署网站</p><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%202.png" alt="Hexo静态博客搭建和部署/Untitled%202.png"></p><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%203.png" alt="Hexo静态博客搭建和部署/Untitled%203.png"></p><h2 id="开启Gitee-Pages服务"><a href="#开启Gitee-Pages服务" class="headerlink" title="开启Gitee Pages服务"></a>开启Gitee Pages服务</h2><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%204.png" alt="Hexo静态博客搭建和部署/Untitled%204.png"></p><h2 id="创建公钥"><a href="#创建公钥" class="headerlink" title="创建公钥"></a>创建公钥</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;邮箱地址&quot;</span></span><br></pre></td></tr></table></figure><p>密钥对生成后默认的位置是在 C:\Users\Administrator.ssh 的目录下。</p><p>其中 id_rsa 是私钥，id_rsa.pub 是公钥。</p><p>用记事本打开并复制公钥。</p><h2 id="添加公钥"><a href="#添加公钥" class="headerlink" title="添加公钥"></a>添加公钥</h2><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%205.png" alt="Hexo静态博客搭建和部署/Untitled%205.png"></p><p>将复制到的公钥粘贴进去并确定保存。</p><h2 id="安装hexo-deployer-git"><a href="#安装hexo-deployer-git" class="headerlink" title="安装hexo-deployer-git"></a>安装hexo-deployer-git</h2><ul><li>运行<code>npm install hexo-deployer-git --save</code></li></ul><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>打开D:\MyBlog\HexoBlog_config.yml查找deploy，并行修改下面这段代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line"><span class="built_in">type</span>: git</span><br><span class="line">repo: https://gitee.com/NephrenCake/NephrenCake.git</span><br><span class="line">branch: ph-pages</span><br></pre></td></tr></table></figure><h2 id="部署至云端"><a href="#部署至云端" class="headerlink" title="部署至云端"></a>部署至云端</h2><ul><li>运行<code>hexo d</code></li></ul><blockquote><p><a href="https://nephrencake.gitee.io/">https://nephrencake.gitee.io/</a> 即静态博客的地址了。</p></blockquote><ul><li>如果有网页不同步的时候<ul><li>在Gitee Pages 服务中更新部署（每次deploy之后都要手动更新）</li><li>清理浏览器缓存</li></ul></li></ul><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%206.png" alt="Hexo静态博客搭建和部署/Untitled%206.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hexo博客搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
