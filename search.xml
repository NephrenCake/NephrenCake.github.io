<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>浙江大学机器学习课程-完结目录</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>b站 - 胡浩基老师 - 浙江大学研究生机器学习课程 完结合集 - 笔记</p><p>教程视频传送门：<a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=1">浙江大学-研究生机器学习课程-</a></p><p>对于本人来说是看完一些机器学习入门教程之后的理论补充。这个教程包含大量具体详细的理论推导，有一定难度，但是对于想要熟悉理论的还是比较推荐。本人在以后会把推导不足的部分补全。</p><table><thead><tr><th align="center"><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart1/">Part1——课程概论</a></th></tr></thead><tbody><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/">Part2——支持向量机</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/">Part3——人工神经网络</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/">Part4——深度学习</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/">Part5——强化学习</a></strong></td></tr><tr><td align="center"><strong><a href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart6/">Part6——传统的机器学习</a></strong></td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part6</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart6/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart6/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part6——传统的机器学习"><a href="#浙江大学机器学习课程Part6——传统的机器学习" class="headerlink" title="浙江大学机器学习课程Part6——传统的机器学习"></a>浙江大学机器学习课程Part6——传统的机器学习</h1><p>[TOC]</p><h2 id="特征选择与特征提取"><a href="#特征选择与特征提取" class="headerlink" title="特征选择与特征提取"></a>特征选择与特征提取</h2><p>特征选择与特征提取(Feature Selection and Extraction)</p><p>特征选择是一个”物理”过程，不会产生新特征；特征提取是一个”化学”过程，会产生新特征。</p><ol><li>特征提取：主成分分析(Principle Component Analysis)</li><li>特征选择：自适应提升(AdaBoost)</li></ol><h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><h3 id="主成分分析与神经网络"><a href="#主成分分析与神经网络" class="headerlink" title="主成分分析与神经网络"></a>主成分分析与神经网络</h3><p>多层的神经网络，其本质也是一个特征提取器。但是，主成分分析主要目标是减少计算量。</p><p>主成分分析：构造一个A，b使：Y<del>(M*1)</del> = A<del>(M*N)</del> * X<del>(N*1)</del> + b<del>(M*1)</del></p><p>主成分分析可以看成是一个一层的，有m个神经元的神经网络。</p><h3 id="主成分分析的过程"><a href="#主成分分析的过程" class="headerlink" title="主成分分析的过程"></a>主成分分析的过程</h3><p>主成分分析：寻找方差最大的方向，并在该方向投影。在降维的同时保存最大的区分度。</p><p>这里方差最大方向指的是投影之后方差和最大，因为如果投影之后点都汇集在一起的话，那么可以近似成一个点，就区分不出来了。</p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=38">主成分分析详细推导P38</a></p><ol><li><p>最大化寻找的特征方向上的方差和。</p><p>max： a<del>1</del>∑a<del>1</del>^T^=λ；s.t.：a<del>1</del>*a<del>1</del>^T^=||a<del>1</del>||^2^=1 (a<del>i</del>是A的行向量)</p></li><li><p>寻找下方差和最大化的特征方向，并且需要与刚才的方向正交。</p><p>max： a<del>2</del>∑a<del>2</del>^T^=λ；s.t.：a<del>2</del>*a<del>2</del>^T^=||a<del>2</del>||^2^=1、a<del>1</del>*a<del>2</del>^T^=a<del>2</del>*a<del>1</del>^T^=0</p></li><li><p>a<del>2</del>是∑的特征向量，λ是∑的第二大特征值</p></li><li><p>loop</p></li></ol><h3 id="PCA算法全程"><a href="#PCA算法全程" class="headerlink" title="PCA算法全程"></a>PCA算法全程</h3><ol><li><p>求 ∑=∑<del>i=1</del>^i^(X<del>i</del>-E(x))^T^</p></li><li><p>求∑的特征值并从大到小排序[λ<del>1</del>, λ<del>1</del>, λ<del>2</del>,…, λ<del>M</del>, λ<del>M+1</del>,… ]</p><p>对应特征向量[a<del>1</del>^T^, a<del>2</del>^T^, …, a<del>M</del>^T^, a<del>M+1</del>^T^, …]</p></li><li><p>归一化所有a<del>i</del>，使a<del>i</del>a<del>i</del>^T^=1</p></li><li><p>A=[[–a<del>1</del>–],[–a<del>2</del>–],…,[–a<del>m</del>–]]</p></li><li><p>Y<del>i</del>=A(X<del>i</del>-E(X))</p></li></ol><h3 id="其他算法"><a href="#其他算法" class="headerlink" title="其他算法"></a>其他算法</h3><p>奇异值分解SVD(Singular Value Decomposation)快速求出特征值来完成PCA算法。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在特征数比较多，样本数又比较少的情况下，采用PCA，效果不会差。</p><h2 id="自适应提升-AdaBoost"><a href="#自适应提升-AdaBoost" class="headerlink" title="自适应提升(AdaBoost)"></a>自适应提升(AdaBoost)</h2><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>从N个特征中选择M个使识别率更高。</p><p>（启发式方法，如模拟退火、基因算法）①递增法②递减法。不常用，因为神经网络已经做了这些事情，相关性不高的连线之间w会变得很小。</p><h3 id="自适应提升"><a href="#自适应提升" class="headerlink" title="自适应提升"></a>自适应提升</h3><p>针对大规模冗余的特征样本时，是一个非常好的算法。</p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=39&spm_id_from=pageDriver">详细推导P39</a></p><ol><li>初始化采样权值</li><li>用D<del>m</del>采样N个样本（错的样本出现更多，对的样本出现更少），获得弱分类器</li><li>计算加权错误率</li><li>更新权值分布</li><li>-&gt; 2. loop </li><li>最终识别器</li></ol><p>定理：随着M增加，AdaBoost最终分类器在训练集上错误率越来越小。</p><p>AdaBoost过拟合速度不会上升太快。</p><h2 id="概率分类法-重点复习"><a href="#概率分类法-重点复习" class="headerlink" title="概率分类法==重点复习=="></a>概率分类法==重点复习==</h2><p>一定要特别注意先验概率！！！</p><h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart6/image-20210213210453656.png" alt="image-20210213210453656"></p><p>限制条件：</p><ol><li>每个维度都是离散的</li><li>每个维度互相独立</li></ol><p>对于每个P(特征|类别)有：<br>$$<br>P(w|C_j)={count(w,C_j)+1\over{\sum\limits_{w∈V}cout(w,C_j)+|v|}}<br>$$<br>其中，C<del>j</del>指某个类别j，V指特征集合，|v|指特征数。</p><h3 id="高斯概率密度函数"><a href="#高斯概率密度函数" class="headerlink" title="高斯概率密度函数"></a>高斯概率密度函数</h3><p>正态分布（Normal distribution）又名高斯分布（Gaussian distribution）。</p><h4 id="多维高斯分布："><a href="#多维高斯分布：" class="headerlink" title="多维高斯分布："></a>多维高斯分布：</h4><p>$$<br>P(X|C)= {1\over{\sqrt[]{(1π)^d|∑|}}}exp[-{1\over2}(x-μ)^T∑^{-1}(x-μ)]<br>$$</p><p>已知{X<del>i</del>}<del>i={1-N}</del> ，求待求参数：∑(d×d矩阵)、μ(d×1向量)</p><p>构造目标函数（极大似然法Maximum Likedihood）<br>$$<br>E(μ,∑)=\sum\limits_{i=1}^NlnP(X_i|C)<br>$$<br>假设：①所有{X<del>i</del>}<del>i={1-N}</del>独立同分布 undependent and identical distribution ( i.i.d. )；②设定μ<del>1</del>、∑<del>1</del>使出现{X<del>i</del>}<del>i={1-N}</del>概率最大。</p><p>先是概率累乘作为似然函数，取对数方便运算，连乘就变成求和了</p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=43">详细推导p43</a></p><h3 id="EM算法-Expectation-Maximization"><a href="#EM算法-Expectation-Maximization" class="headerlink" title="EM算法(Expectation-Maximization)"></a>EM算法(Expectation-Maximization)</h3><h4 id="混合高斯模型-Gaussian-Mixture-Model"><a href="#混合高斯模型-Gaussian-Mixture-Model" class="headerlink" title="混合高斯模型(Gaussian Mixture Model)"></a>混合高斯模型(Gaussian Mixture Model)</h4><p>叠加多个高斯分布拟合整个复杂的分布。</p><p>这是一个非凸问题，只能求局部极值，不能求全局极值。</p><p>求局部极值的一种方法，而且只对某一类局部极值问题可解。</p><p>优点：①不需要调任何参数，必定收敛②编程简单③理论优美</p><p>步骤：</p><ol><li>随机化，先假设类别</li><li>E-step 计算第n个样本在k个高斯的概率</li><li>M-step 更新所有N个样本中有多少个属于第k个高斯</li><li>-&gt;2 loop</li></ol><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=45">详细推导p45</a></p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=48&spm_id_from=pageDriver">EM算法收敛性推导</a></p><h4 id="k-均值聚类-K-means-Clustering"><a href="#k-均值聚类-K-means-Clustering" class="headerlink" title="k-均值聚类(K-means Clustering)"></a>k-均值聚类(K-means Clustering)</h4><ol><li>随机化μ<del>1</del>、…、μ<del>k</del></li><li>E-step 离哪个类近，重新归属于哪一类</li><li>M-step n个样本中有多少个属于第k类，重新分配第k类的均值μ<del>k</del></li><li>-&gt;2 loop</li></ol><h2 id="GMM在说话人识别中的应用"><a href="#GMM在说话人识别中的应用" class="headerlink" title="GMM在说话人识别中的应用"></a>GMM在说话人识别中的应用</h2><ol><li>去除静音(将不说话的低能量片段去除，但保留同为低能量的辅音(使用过零率判别))</li><li>提取的特征：MEL倒谱系数 （Mel-frequency Cepstrum Coefficients, MFCC）</li></ol><p>缺点：对噪声要求严苛，因为加了噪声就相当于改变分布。</p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part5</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part5——强化学习"><a href="#浙江大学机器学习课程Part5——强化学习" class="headerlink" title="浙江大学机器学习课程Part5——强化学习"></a>浙江大学机器学习课程Part5——强化学习</h1><p>[TOC]</p><h2 id="增强学习与监督学习的区别"><a href="#增强学习与监督学习的区别" class="headerlink" title="增强学习与监督学习的区别"></a>增强学习与监督学习的区别</h2><ol><li>训练数据中没有标签，只有奖励函数（Reward Function）。</li><li>训练数据不是现成给定，而是由行为（Action）获得。</li><li>现在的行为（Action）不仅影响后续训练数据的获得，也影响奖励函数（Reward Function）的取值。</li><li>训练的目的是构建一个“<strong>状态-&gt;行为</strong>”*(内部状态和外部状态，外部状态不由我们的行为控制)*的函数，其中状态（State）描述了目前内部和外部的环境，在此情况下，要使一个智能体（Agent）在某个特定的状态下，通过这个函数，决定此时应该采取的行为。希望采取这些行为后，最终获得最大化的奖励函数值。</li></ol><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ol><li>R<del>t</del>：t时刻的奖励函数</li><li>S<del>t</del>：t时刻的状态</li><li>A<del>t</del>：t时刻的行为</li></ol><h2 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h2><ol><li><p>马尔科夫假设：P[S<del>t+1</del>|S<del>t</del>]=P[S<del>t+1</del>|S1,…,S<del>t</del>]</p></li><li><p>下一个时刻的状态只与这一时刻的状态以及这一时刻的行为有关：</p><p>P<del>SS’</del>^a^=P[S<del>t+1</del>=s’|S<del>t</del>=s, A<del>t</del>=a]</p></li><li><p>下一时刻的奖励函数值值域这一时刻的状态以及这一时刻的行为有关：</p><p>P<del>S</del>^a^=E[R<del>t+1</del>|S<del>t</del>=s, A<del>t</del>=a]</p></li></ol><h2 id="Markov-decision-Process-MDP"><a href="#Markov-decision-Process-MDP" class="headerlink" title="Markov decision Process (MDP)"></a>Markov decision Process (MDP)</h2><ol><li>在t=0时候，环境给出一个初始状态 s ~ p(s<del>0</del>)</li><li>循环：<pre><code> -- 智能体选择行为：a~t~ -- 环境采样奖励函数：r~t~ ~ R( . |s~t~, a~t~) -- 环境产生下一个状态：s~t+1~ ~ R( . |s~t~, a~t~) -- 智能体获得奖励函数 r~t~ 和下一个状态 s~t+1~</code></pre></li><li>我们需要学习一个策略（Policy）π^*^(s<del>t</del>,a<del>t</del>)=P(a<del>t</del>|s<del>t</del>)   , 这是一个从状态到行为的映射函数，使得最大化累积的奖励。</li></ol><h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><ol><li>增强学习中已经知道的的函数是：P<del>S</del>^a^=E[R<del>t+1</del>|S<del>t</del>=s, A<del>t</del>=a]</li><li>需要学习的函数是：P<del>SS’</del>^a^=P[S<del>t+1</del>=s’|S<del>t</del>=s, A<del>t</del>=a]</li><li>根据一个决策机制（Policy），我们可以获得一条路径：s<del>0</del> -&gt; a<del>0</del> -&gt; r<del>0</del> -&gt; s<del>1</del> -&gt; a<del>1</del> -&gt; r<del>1</del> …</li><li>定义1：估值函数（Value Function）是衡量某个状态最终能获得多少累积奖励的函数：<img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/image-20210210225702878.png" alt="image-20210210225702878"></li><li>定义2：Q函数是衡量某个状态下采取某个行为后，最终能获得多少累积奖励的函数：<img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/image-20210210225720116.png" alt="image-20210210225720116"></li><li>在s状态下做出行为a的概率，和这种情况下的奖励，得到估值 V^π^(s)=∑<del>a∈A</del>P(a|s)Q^π^(s,a)</li></ol><h3 id="劣势"><a href="#劣势" class="headerlink" title="劣势"></a>劣势</h3><ol><li>对于状态数和行为数很多时，使Q函数非常复杂，难以收敛。例如：①对一个ATARI游戏，状态数是相邻几帧所有像素的取值组合，这是一个天文数字；②图像方面的应用，状态数是(像素值取值范围数)^(像素个数)</li><li>很多程序，如下棋程序等，REWARD是最后获得（输或赢），不需要对每一个中间步骤都计算REWARD。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><p>目前强化学习的发展状况：在一些特定的任务上达到人的水平或胜过人，但在一些相对复杂的任务上，例如自动驾驶等，和人存在差距。</p></li><li><p>和真人的差距，可能不完全归咎于算法。传感器、机械的物理限制等，也是决定性因素。</p></li><li><p>机器和人的另一差距是：人有一些基本的概念，依据这些概念，人能只需要很少的训练就能学会很多，但机器只有通过大规模数据，才能学会。</p></li><li><p>但是，机器速度快，机器永不疲倦，只要有源源不断的数据，在特定的任务上，机器做得比人好，是可以期待的。</p></li></ol><h2 id="Alpha-Go"><a href="#Alpha-Go" class="headerlink" title="Alpha Go"></a>Alpha Go</h2><p>围棋有必胜策略</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart5/image-20210210235412124.png" alt="image-20210210235412124"></p><p><a href="https://github.com/Rochester-NRT/RocAlphaGo">Alpha Go 开源地址</a></p><p>每隔一定的轮次，训练过后的网络将和训练之前的网络对抗，已获得更多的样本数据继续训练。</p><p>另外有一个更加简单的深度策略网络(Rollout Policy Network)，牺牲准确率来换取速度，在对局后期通过不断演算，将赢的落子概率增加，输的概率减少。</p><p>蒙特卡洛树搜索 （Monte Carlo Tree Search）：多次模拟未来棋局，然后选择在模拟中选择次数最多的走法。</p><p>同时要采用多样化的步骤来增加随机性。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><p>Reinforcement Learning an introduction. R. S. Sutton and A. G. Barto, 2005</p></li><li><p><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">UCL Course on RL</a></p></li><li><p>例程程序</p><p><a href="http://karpathy.github.io/2016/05/31/rl/?_utm_source=1-2-2">http://karpathy.github.io/2016/05/31/rl/?_utm_source=1-2-2</a><br><a href="https://gym.openai.com/https://github.com/openai/gym">https://gym.openai.com/https://github.com/openai/gym</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part4</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part4——深度学习"><a href="#浙江大学机器学习课程Part4——深度学习" class="headerlink" title="浙江大学机器学习课程Part4——深度学习"></a>浙江大学机器学习课程Part4——深度学习</h1><p>[TOC]</p><h2 id="多层神经网络的优劣"><a href="#多层神经网络的优劣" class="headerlink" title="多层神经网络的优劣"></a>多层神经网络的优劣</h2><h3 id="多层神经网络的优势"><a href="#多层神经网络的优势" class="headerlink" title="多层神经网络的优势"></a>多层神经网络的优势</h3><ol><li>基本单元简单，多个基本单元可扩展为非常复杂的非线性函数。因此易于构建，同时模型有很强的表达能力。</li><li>训练和测试的计算并行性非常好，有利于在分布式系统上的应用。</li><li>模型构建来源于对人脑的仿生，话题丰富，各种领域的研究人员都有兴趣，都能做贡献。</li></ol><h3 id="多层神经网络的劣势"><a href="#多层神经网络的劣势" class="headerlink" title="多层神经网络的劣势"></a>多层神经网络的劣势</h3><ol><li>数学不漂亮，优化算法只能获得局部极值，算法性能与初始值有关。</li><li>不可解释。训练神经网络获得的参数与实际任务的关联性非常模糊。</li><li>模型可调整的参数很多 （网络层数、每层神经元个数、非线性函数、学习率、优化方法、终止条件等等），使得训练神经网络变成了一门“艺术”。</li><li>如果要训练相对复杂的网络，需要大量的训练样本。</li></ol><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><ol><li>Mnist：手写数字数据库（LeCun 在1998年创造）</li><li>ImageNet：（Fei-fei Li等 2007年创造）</li></ol><h2 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h2><h3 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h3><p>神经网络是目前处理大数据最优的算法。</p><p>模拟退火和遗传算法还处于沉寂期。</p><h3 id="转机"><a href="#转机" class="headerlink" title="转机"></a>转机</h3><p>2006年是深度学习的起始年，Hinton在SCIENCE上发文，提出一种叫做自动编码机（Auto-encoder）的方法，部分解决了神经网络参数初始化的问题。</p><p>但是目前为止，自动编码机并没有什么用。</p><h2 id="卷积神经网络-CNN"><a href="#卷积神经网络-CNN" class="headerlink" title="卷积神经网络(CNN)"></a>卷积神经网络(CNN)</h2><h3 id="发展"><a href="#发展" class="headerlink" title="发展"></a>发展</h3><p>是深度学习神经网络流行起来最大的因素。</p><p>由手工设计卷积核变成自动学习卷积核。</p><h3 id="如何卷积"><a href="#如何卷积" class="headerlink" title="如何卷积"></a>如何卷积</h3><blockquote><p>一个讲解比较清晰的视频：<a href="https://www.bilibili.com/video/BV1JX4y1K7Dr">什么是卷积？</a></p></blockquote><p>详细图解原理在另一份笔记中，不重复记述：<a href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/">PyTorch深度学习实践Part10</a></p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210160012660.png" alt="image-20210210160012660"></p><p>卷积核的数量等于输出通道数。卷积核的长度等于输入通道数。</p><p>卷积过后得到的叫做特征图。</p><p>在边缘可能会丢失数据的时候，用padding补零。</p><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210160114566.png" alt="image-20210210160114566"></p><p>卷积神经网络与全连接的区别。</p><ol><li>局部感受野</li><li>权值共享</li></ol><p>卷积虽然复杂，但是计算量更少。</p><h3 id="激活"><a href="#激活" class="headerlink" title="激活"></a>激活</h3><p>在卷积神经网络中，最常用的非线性函数为ReLu。</p><h3 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h3><p>导数会平均反向传播。</p><blockquote><p>整个网络的计算速度取决于卷积层，整个网络的参数个数取决于全连接层。</p><p>即：如果要加速神经网络，则在卷积层做文章；如果要让其内存占用更少的话，则在全连接层做文章。</p></blockquote><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h3 id="改进1——ReLU"><a href="#改进1——ReLU" class="headerlink" title="改进1——ReLU"></a>改进1——ReLU</h3><p>以ReLU函数代替sigmoid或tanh函数。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210163727240.png" alt="image-20210210163727240"></p><p>实践证明，这样做能使网络训练以更快速度收敛。</p><blockquote><p>如果数据是靠近0的正态分布，则每次只激活一半的神经元。</p></blockquote><h3 id="改进2——MaxPooling"><a href="#改进2——MaxPooling" class="headerlink" title="改进2——MaxPooling"></a>改进2——MaxPooling</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210163808698.png" alt="image-20210210163808698"></p><p>在AlexNet中，提出了最大池化(Max Pooling)的概念，即对每一个邻近像素组成的“池子”，选取像素最大值作为输出。在LeNet中，池化的像素是不重叠的；而在AlexNet中进行的是有重叠的池化。实践表明，有重叠的最大池化能够很好的克服过拟合问题，提升系统性能。</p><p>在反向传播时，只传播最大值，其他都为0。</p><blockquote><p>MaxPooling不仅是一个降采样的操作，同时还是一个非线性操作。只采用最大的神经元，有效降低了激活的神经元个数，从而加快了收敛的速率。</p></blockquote><h3 id="改进3——Dropout"><a href="#改进3——Dropout" class="headerlink" title="改进3——Dropout"></a>改进3——Dropout</h3><p>随机丢弃（Dropout）。为了避免系统参数更新过快导致过拟合，每次利用训练样本更新参数时候，随机的“丢弃”一定比例的神经元，被丢弃的神经元将不参加训练过程，输入和输出该神经元的权重系数也不做更新。这样每次训练时，训练的网络架构都不一样，而这些不同的网络架构却分享共同的权重系数。实验表明，随机丢弃技术减缓了网络收敛速度，也以大概率避免了过拟合的发生。</p><blockquote><p>道理和改进2一样，每次训练都只激活有限个神经元，而不要让整个网络同时改变所有的参数，导致整个网络不稳定。</p></blockquote><h3 id="改进4——增加训练样本"><a href="#改进4——增加训练样本" class="headerlink" title="改进4——增加训练样本"></a>改进4——增加训练样本</h3><p>增加训练样本。尽管ImageNet的训练样本数量有超过120万幅图片，但相对于6亿待估计参数来说，训练图像仍然不够。Alex等人采用了多种方法增加训练样本，包括：1. 将原图水平翻转；2. 将256×256的图像随机选取224×224的片段作为输入图像。运用上面两种方法的组合可以将一幅图像变为2048幅图像。还可以对每幅图片引入一定的噪声，构成新的图像。这样做可以较大规模增加训练样本，避免由于训练样本不够造成的性能损失。</p><h3 id="改进5——GPU加速"><a href="#改进5——GPU加速" class="headerlink" title="改进5——GPU加速"></a>改进5——GPU加速</h3><p>用GPU加速训练过程。采用2片GTX 580 GPU对训练过程进行加速，由于GPU强大的并行计算能力，使得训练过程的时间缩短数十倍，哪怕这样，训练时间仍然用了六天。</p><h2 id="近年来流行的网络结构"><a href="#近年来流行的网络结构" class="headerlink" title="近年来流行的网络结构"></a>近年来流行的网络结构</h2><h3 id="VGGNet-（Simonyan-and-Zisserman-2014）"><a href="#VGGNet-（Simonyan-and-Zisserman-2014）" class="headerlink" title="VGGNet: （Simonyan and Zisserman, 2014）"></a>VGGNet: （Simonyan and Zisserman, 2014）</h3><p>3个叠到一起的3 * 3卷积核，感受野（Receptive Field）是7 * 7,大致可以替代7 * 7卷积核的作用。但这样做可以使参数更少 ，参数比例大致为27:49。</p><p>但是，运算速度会因为卷积核数量的增加而大幅下降。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210173031450.png" alt="image-20210210173031450"></p><h3 id="GoogLeNet-（Szegedy-2014）"><a href="#GoogLeNet-（Szegedy-2014）" class="headerlink" title="GoogLeNet: （Szegedy, 2014）"></a>GoogLeNet: （Szegedy, 2014）</h3><p>inception 结构，用一些1<em>1, 3</em>3和5*5的小卷积核用固定方式组合到一起，来代替大的卷积核，从而达到增加感受野和减少参数的目的。500万参数，比ALEXNET小了12倍。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210173440780.png" alt="image-20210210173440780"></p><h3 id="残差网络Residual-Net-ResNet-（He-et-al-2015）"><a href="#残差网络Residual-Net-ResNet-（He-et-al-2015）" class="headerlink" title="残差网络Residual Net(ResNet): （He et al, 2015）"></a>残差网络Residual Net(ResNet): （He et al, 2015）</h3><p>加入了前向输入机制，将前面层获得的特征图作为监督项输入到后面层。用这样的方法使深层网络训练能够更好地收敛。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210210183347601.png" alt="image-20210210183347601"></p><h2 id="迁移学习-Transfer-Learning"><a href="#迁移学习-Transfer-Learning" class="headerlink" title="迁移学习(Transfer Learning )"></a>迁移学习(Transfer Learning )</h2><p>迁移学习是指，从一个domain训练好的神经网络，加入新的domain的样本进行调优，从而获得一个更好的识别效果。</p><p>例如，训练好的 Alex Net ，在最后的1k个分类后，外加一层全连接神经网络，输出十几种水果的分类，用十种水果进行调优。虽然这原本1k个分类中或许没有很明显的这十几种水果分类，但这样它能很容易排除除了这些水果以外的物品，同时经过调优之后会更容易贴合我们想要分出的十几种水果。</p><h2 id="目标检测与分割"><a href="#目标检测与分割" class="headerlink" title="目标检测与分割"></a>目标检测与分割</h2><p>这里其实算深度学习（</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211213322900.png" alt="image-20210211213322900"></p><h3 id="目标定位与识别"><a href="#目标定位与识别" class="headerlink" title="目标定位与识别"></a>目标定位与识别</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211213742143.png" alt="image-20210211213742143"></p><h3 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h3><p>在一幅图片中可能有多种类别，不能确定数量。</p><p>现在这个问题基本解决，有三篇逐步递进的三篇论文。</p><h4 id="Regional-CNN-R-CNN"><a href="#Regional-CNN-R-CNN" class="headerlink" title="Regional CNN(R-CNN)"></a>Regional CNN(R-CNN)</h4><p>目标候选区域(Region Proposal)：先用传统方法或者图像处理的方法，确定可能有物体的地方候选项，再放在卷积神经网络检测。</p><h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><ol><li>Selective Search：产生RP</li><li>CNN：检测这些候选区域</li><li>SVM：分类</li></ol><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211222650735.png" alt="image-20210211222650735"></p><h5 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h5><ol><li>High cost to perform Selective Search (~5s per image)</li><li>Too many passes to CNN (~2000 proposals per image)</li><li>Lead to unacceptable test time (~50s per image)</li><li>High space cost to train SVM (millions of 1024-d features)</li></ol><h4 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h4><p>用 Regions of Interest(RoIs) 把不同长宽的候选区域，在Pooling层归一化成同一形状。最后依然是一路预测label，一路预测Bounding-box regressors。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211224649852.png" alt="image-20210211224649852"></p><h5 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h5><p>主要通过小的Region Proposal Network产生粗略位置，来代替Selective Search</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211221620685.png" alt="image-20210211221620685"></p><h3 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h3><h4 id="全卷积网络FCN"><a href="#全卷积网络FCN" class="headerlink" title="全卷积网络FCN"></a>全卷积网络FCN</h4><p>全卷积网络(Fully Convolutional Networks)</p><p>先训练前面一半，再训练后一半。先降采样，后升采样。</p><p>卷积层的上采样（Upsampling）也叫反卷积（Deconvolution）或 转置卷积（Transpose Convolution）。</p><p>全卷积网络也可以用于边缘提取。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210211230604746.png" alt="image-20210211230604746"></p><h2 id="隐含马尔科夫过程-HMM-与递归神经网络-RNN"><a href="#隐含马尔科夫过程-HMM-与递归神经网络-RNN" class="headerlink" title="隐含马尔科夫过程(HMM)与递归神经网络(RNN)"></a>隐含马尔科夫过程(HMM)与递归神经网络(RNN)</h2><p>对连续信息的判断有几个问题：不知道如何划分每一个状态，因为①持续时间可能各不相同；②对于语音模型的建模是以一个音节而不是一个单词为基础。</p><h3 id="Hidden-Markov-Models-复习"><a href="#Hidden-Markov-Models-复习" class="headerlink" title="Hidden Markov Models==复习=="></a>Hidden Markov Models==复习==</h3><p>一个HMM模型是由三部分组成：λ = {A, B, π}。其中，A为状态转移矩阵，B为观测概率，π为状态先验概率。</p><ol><li>π(S<del>i</del>)表示一开始在S<del>i</del>状态的概率。</li><li>A是一个P×P的矩阵。马尔科夫链假设是强假设，后一个时刻状态和前一个(或者多个)时刻状态有关（注意这是固定假设 a<del>i,j</del>=P(q<del>t+1</del>=S<del>j</del>|q<del>t</del>=S<del>i</del>)。当t时刻的q样本是状态为S<del>i</del>类，则在t+1时刻q样本转变为状态为S<del>j</del>类的概率）</li><li>B={b<del>j</del>(0)} 若输入向量O属于S<del>j</del>，则它的概率分布用b<del>j</del>(0)表示。</li></ol><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=50">详细推导p50</a></p><h3 id="大词汇量连续语音识别（LVCSR）"><a href="#大词汇量连续语音识别（LVCSR）" class="headerlink" title="大词汇量连续语音识别（LVCSR）"></a>大词汇量连续语音识别（LVCSR）</h3><p>大词汇量连续语音识别（Large-scale Vocabulary Continuous Speech Recognition, LVCSR）</p><ol><li>每一个HMM模型所表达的“单词”是什么？英语中有效的Triphone个数大致在55000左右，模型过多而训练样本不足，所以需要多个Triphone 合并（Tying）、多个Triphone 联合训练（Tying）</li><li>在识别流程中如何对测试声音文件做时间轴的划分，使每一个分段（SEGMENT）对应一个“单词”？如何搜索最佳的“单词”组合？VITERBI搜索（有多种形式，如Two-Level Dynamic Programming）、A*搜索、随机搜索</li><li>如何构造语言模型 (Language Model)? 定义(N-gram): 一个单词出现的概率，只与它前面的N个单词相关。</li></ol><h3 id="结合深度网络模型的语音识别"><a href="#结合深度网络模型的语音识别" class="headerlink" title="结合深度网络模型的语音识别"></a>结合深度网络模型的语音识别</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214220419460.png" alt="image-20210214220419460"></p><h3 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h3><h4 id="输入与输出多对多："><a href="#输入与输出多对多：" class="headerlink" title="输入与输出多对多："></a>输入与输出多对多：</h4><p>大词汇连续语音识别、机器翻译</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214215442390.png" alt="image-20210214215442390"></p><h4 id="输入与输出多对一："><a href="#输入与输出多对一：" class="headerlink" title="输入与输出多对一："></a>输入与输出多对一：</h4><p>动作识别、行为识别、单词量有限的语音识别</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214215758906.png" alt="image-20210214215758906"></p><h4 id="输入与输出一对多："><a href="#输入与输出一对多：" class="headerlink" title="输入与输出一对多："></a>输入与输出一对多：</h4><p>文本生成、图像文字标注</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214215916422.png" alt="image-20210214215916422"></p><h3 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long-Short Term Memory (LSTM)"></a>Long-Short Term Memory (LSTM)</h3><p>相比VANILLA RNN， LSTM的误差反向传播更方便和直接，梯度更新不存在RNN中的暴涨或消失现象。因此，建议涉及RNN的应用都用LSTM或LSTM相关的变种。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart4/image-20210214221615263.png" alt="image-20210214221615263"></p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part3</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part3——人工神经网络"><a href="#浙江大学机器学习课程Part3——人工神经网络" class="headerlink" title="浙江大学机器学习课程Part3——人工神经网络"></a>浙江大学机器学习课程Part3——人工神经网络</h1><p>[TOC]</p><h2 id="神经网络的生物及数学模型"><a href="#神经网络的生物及数学模型" class="headerlink" title="神经网络的生物及数学模型"></a>神经网络的生物及数学模型</h2><ol><li>硬件算力的提升</li><li>数据样本的增加</li><li>但是，<strong>其最基本的神经元模型至今没有重大改变</strong></li></ol><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210209144613134.png" alt="image-20210209144613134"></p><h2 id="感知器算法-Perceptron-Algorithm"><a href="#感知器算法-Perceptron-Algorithm" class="headerlink" title="感知器算法(Perceptron Algorithm)"></a>感知器算法(Perceptron Algorithm)</h2><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210209144950398.png" alt="image-20210209144950398"></p><ul><li>效果比SVM差得多，但是是机器学习最早提出的算法。</li><li>感知器每次只取单个样本，SVM从全局样本考虑。</li></ul><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=20">收敛性证明看p20</a></p><h2 id="多层神经网络-Multi-Layer-Neural-Networks"><a href="#多层神经网络-Multi-Layer-Neural-Networks" class="headerlink" title="多层神经网络(Multi-Layer Neural Networks)"></a>多层神经网络(Multi-Layer Neural Networks)</h2><p>线性不可分的数据集困扰了早期神经网络算法长达十年之久。</p><p>因此，我们需要用非线性的函数集合来分开非线性可分的数据集。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210209160042159.png" alt="image-20210209160042159"></p><p>二层神经网络模型举例。</p><h2 id="三层神经网络模拟任意决策面"><a href="#三层神经网络模拟任意决策面" class="headerlink" title="三层神经网络模拟任意决策面"></a>三层神经网络模拟任意决策面</h2><p>我个人认为：过了第一层，数据样本基本不会保持原貌，但是保留了特征，或者说特点。</p><p>在老师画的示例中，实际的数学意义为：①第一层是：坐标(x,y)进入多个函数输入相对位置，进入激活函数输出0-1；②第二层：根据上一层判断的0-1，进入函数判断是否为某一块图形内部，返回0-1；③第三层：根据上一层返回的是否在某图形内部的0-1，进入函数判断该图形是否为class1，通过或关系，输出最终结果0-1.</p><p>详情见<a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=23">p23</a></p><h2 id="后向传播算法-Back-Propagation"><a href="#后向传播算法-Back-Propagation" class="headerlink" title="后向传播算法(Back Propagation)"></a>后向传播算法(Back Propagation)</h2><blockquote><p>在对于某种问题，我们究竟应该选择哪一种参数组合，如何搭建完美的网络结构。这依然是一个至今为止不完备的，对于一种问题，最好的方法还是不断试验。</p></blockquote><p>主要思想就是梯度向下法(Gradient Descent Method)来求局部极值。</p><p>常用的激活函数：sigmoid、tanh、ReLu(Rectified Linear Units)、LeakReLu</p><p><a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=24">==BP推导P24==</a></p><h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><ol><li>不用每输入一个样本就去变换参数，而是输入一批样本（叫做一个BATCH或MINI-BATCH），求出这些样本的梯度平均值后，根据这个平均值改变参数。</li><li>在神经网络训练中，BATCH的样本数大致设置为50-200不等。  </li></ol><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210210140226760.png" alt="image-20210210140226760"></p><h3 id="梯度消失与归一化"><a href="#梯度消失与归一化" class="headerlink" title="梯度消失与归一化"></a>梯度消失与归一化</h3><ol><li>当样本都非常大或者非常小，在sigmoid函数中就可以发现，他们的梯度将会非常小，这样意味着他们更加符合我们想要得到的预测结果，即二分类。但是这并不是我们在训练过程中想要的，因为梯度太小而导致参数无法更新以进行训练。</li><li>防止梯度消失，那么就需要使样本更加靠近在0附近，更加具有像线性模型一样的特性。因此我们经常对<strong>初</strong>始数据以及在<strong>训练过程中</strong>进行归一化Batch Normalization。</li></ol><h3 id="目标函数选择"><a href="#目标函数选择" class="headerlink" title="目标函数选择"></a>目标函数选择</h3><ol><li><p>SOFTMAX函数</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210210131234229.png" alt="image-20210210131234229"> 处理多分类</p></li><li><p>交叉熵(Cross Entropy)</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart3/image-20210210131306125.png" alt="image-20210210131306125"> 处理二分类</p></li></ol><h3 id="参数更新策略"><a href="#参数更新策略" class="headerlink" title="参数更新策略"></a>参数更新策略</h3><p>优化器不一定是MSE，因为MSE的更新通常容易出现z字路径，一般可以选择使用其他更加平滑的优化器，比如AdaGrad、RMSProp。</p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part2</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part2——支持向量机"><a href="#浙江大学机器学习课程Part2——支持向量机" class="headerlink" title="浙江大学机器学习课程Part2——支持向量机"></a>浙江大学机器学习课程Part2——支持向量机</h1><p>[TOC]</p><h2 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h2><h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><p>对样本数较少的时候，都会得到一个比较好的结果。</p><p>如何在两种训练集上画一条直线来分类：</p><ol><li>线性可分(Linear Separable)训练样本集</li><li>非线性可分(Non-Linear Separable)样本集</li></ol><p>将线(在多维特征下是超平面Hyperplane)向两边移动直到擦到样本点，其中间隔(Margin)最大，且线在d/2处。</p><h3 id="数学描述"><a href="#数学描述" class="headerlink" title="数学描述"></a>数学描述</h3><ol><li>将平行线擦到的向量称作支持向量(Support Vector)</li><li>训练数据及标签：(Xn, Yn)…… X为特征向量，Y为标签。Y取+1或-1来表示，方便推导。</li><li>线性模型：(W, b) 超平面：Wt * X + b = 0</li></ol><h3 id="机器学习过程"><a href="#机器学习过程" class="headerlink" title="机器学习过程"></a>机器学习过程</h3><ol><li>用复杂的函数来限定模型框架</li><li>留出待定参数</li><li>用训练样本来确定参数取值</li></ol><h3 id="线性可分的定义和条件"><a href="#线性可分的定义和条件" class="headerlink" title="线性可分的定义和条件"></a>线性可分的定义和条件</h3><p>{(Xi, Yi)}i = 1<del>N, 存在(W, b), 则对任意 i = 1</del>N. 有：</p><p>①若Yi = +1, 则 Wt * X + b &gt; 0</p><p>②若Yi = -1, 则 Wt * X + b &lt; 0</p><p>为什么要取Y=±1？因为可以得到①②等价于 Yi ( Wt * X + b ) &gt; 0</p><h2 id="优化问题-优化目标函数和限制条件"><a href="#优化问题-优化目标函数和限制条件" class="headerlink" title="优化问题(优化目标函数和限制条件)"></a>优化问题(优化目标函数和限制条件)</h2><h3 id="两个要点"><a href="#两个要点" class="headerlink" title="两个要点"></a>两个要点</h3><ol><li>点最小化(Minimize / min): 1 / 2 * ||W||² 。<strong>这里1/2只是为了求导方便</strong></li><li>限制条件(Subject to / s.t.): Y<del>i</del> ( W^T^ * X + b ) ≥ 1, (i=1~N)</li></ol><h3 id="两个事实"><a href="#两个事实" class="headerlink" title="两个事实"></a>两个事实</h3><p>事实1：W^T^ * X + b = 0 与 a * W^T^ * X + a * b = 0, (a∈R+) 是同一个平面。</p><p>​           即: 若 (W, b) 满足 W^T^ * X + b = 0 , 则 (a * W, a * b) 也满足 W^T^ * X + b = 0 </p><p>事实2：向量到超平面(点到平面)的距离公式。d = | Wt * X0 + b | / || W || *<em>1.这里的X0代表的是包含多个维度的坐标[x,y,z…]，而Y0是分类标签，不能与坐标混为一谈；2.Wt</em>X0结果是一个数而不是矩阵向量**</p><p>​           其中，模 || W || = √(W1²+W2²…+Wn²)</p><h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><ol><li>用a去缩放超平面参数：(W, b) &lt;=&gt; ( a * W, a * b )。根据事实1，这两组不同的参数代表的是同一个平面。</li><li>最终使在支持向量上：| W^T^ * X<del>0</del> + b | = 1。<strong>这里的W和b都是经过系数a缩放过后的，目的是为了凑出1(当然你可以凑出任意常数)，而a具体是多少，我们不需要关注。需要注意的是，这里只是带入支持向量的值，并不是指支持向量的那个点在超平面上，否则值为0。</strong></li><li>由<em>推导2</em>和<em>事实2</em>可得：d = | W^T^ * X<del>0</del> + b | / || W || = 1 / || W ||。可得<em>要点1</em>：最小化 ||W|| 即最大化 d 。</li><li>其他不是支持向量的点到超平面的距离，则大于 1 / || W || 。可得 | W^T^ * X<del>0</del> + b | &gt; 1 。可得<em>要点2</em>：限制条件 Y<del>i</del> * ( W^T^ * X + b ) ≥ 1</li></ol><p>举例：原来平面是 W^T^ * X + b = 0 , 假设带入X<del>0</del>后的值 | W^T^ * X<del>0</del> + b | = M , 现在把超平面缩放为 a * W^T^ * X + a * b = 0 , 其中a是1/M, 那么把X<del>0</del>带入则 | a * W^T^ * X<del>0</del> + a * b | = M/M = 1。与此同时，计算d的时候，因为分子分母同乘a=1/M，a不需要求出，所以我们不需要关心a的取值，只是为了凑一个 | W^T^ * X<del>0</del> + b | = 1 ，当然你可以凑出任意常数。</p><h3 id="二次优化问题-Quadratic-Programming"><a href="#二次优化问题-Quadratic-Programming" class="headerlink" title="二次优化问题(Quadratic Programming)"></a>二次优化问题(Quadratic Programming)</h3><p>二次优化问题属于凸优化问题</p><ol><li>目标函数(Obejective Function)是二次项</li><li>限制条件是一次项</li></ol><p>要么无解，要么只有唯一极值。即局部极值就是全局极值。</p><h3 id="SVM处理非线性"><a href="#SVM处理非线性" class="headerlink" title="SVM处理非线性"></a>SVM处理非线性</h3><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207133321372.png" alt="image-20210207133321372" style="zoom: 200%;"><p>处理这种问题的一种方案就是加上正则项(Regulation Term)（结构损失函数），在解集合中挑选出一组参数（解），使经验损失和结构损失都较低。</p><p>c是正则化的强度，是事先设定好的超参数。</p><p>ζ是松弛变量(Slack Variable)。</p><p>放到场景中就是，样本数小于参数量，在只优化经验误差函数的时候很容易发生过拟合。这个公式依然可以适用于处理线性Linear SVM。</p><h2 id="低维到高维映射"><a href="#低维到高维映射" class="headerlink" title="低维到高维映射"></a>低维到高维映射</h2><p>在低维空间中，一些线性不可分的数据集，在高维空间中，更有可能被分开。因此可以把Xi通过函数φ(x)变换映射到高维空间，通过泛函分析满足某种条件，把核函数W*φ(x)拆成两个高维向量的内积。</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>我们可以不在意无限维映射φ(x)的显示表达，我们只要知道一个核函数(kernel Function)，<img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207142313276.png" alt="image-20210207142313276">，则优化问题仍然可解。</p><p>线性内核相当于没有用核。多项式核的待定系数d取越大，则越复杂。高斯核是无限维度，分类效果最高，待定系数为σ。Tanh核的待定参数是β和b。这些待定参数的选取只能不停地试。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210206144627709.png" alt="image-20210206144627709"></p><h3 id="充要条件"><a href="#充要条件" class="headerlink" title="充要条件"></a>充要条件</h3><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207142313276.png" alt="image-20210207142313276">能够成立的充要条件：(Mercer’s Theorem)</p><ol><li>K(X<del>1</del>, X<del>2</del>) = K(X<del>2</del>, X<del>1</del>) <strong>(交换性)</strong></li><li>对任意 常数C<del>i</del>, 向量X<del>i</del> (i=1<del>N)，有 ∑(i=1</del>N) ∑(j=1<del>N) C</del>i~ * C<del>j</del> * K(X<del>i</del>, X<del>j</del>) ≥ 0 <strong>(半正定性)</strong></li></ol><h2 id="原问题和对偶问题-重点复习"><a href="#原问题和对偶问题-重点复习" class="headerlink" title="原问题和对偶问题==(重点复习)=="></a>原问题和对偶问题==(重点复习)==</h2><h3 id="优化理论"><a href="#优化理论" class="headerlink" title="优化理论"></a>优化理论</h3><p>优化理论(运筹学)是工程里最本质的问题</p><h4 id="推荐书籍"><a href="#推荐书籍" class="headerlink" title="推荐书籍"></a>推荐书籍</h4><ol><li>Convex optimization - Stephen Boyd - b站吴立德</li><li>Nonlinear Programming</li></ol><h3 id="原问题-Prime-Problem"><a href="#原问题-Prime-Problem" class="headerlink" title="原问题(Prime Problem)"></a>原问题(Prime Problem)</h3><p>min: f(w)</p><p>s.t. : g<del>i</del>(w)≤0 (i=1<del>K) , h</del>i<del>(w)=0 (i=1</del>M)</p><h3 id="对偶问题-Dual-Problem"><a href="#对偶问题-Dual-Problem" class="headerlink" title="对偶问题(Dual Problem)"></a>对偶问题(Dual Problem)</h3><ol><li><p>定义：<img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207153808814.png" alt="image-20210207153808814"></p><p>x是前面的w，α是一个K维的向量，β是一个M维的向量。</p><p>拉格朗日对偶问题是运筹学基础知识。*(KKT条件求解、拉格朗日传乘数法、弱对偶性定理)*</p></li><li><p>对偶问题定义：</p><p>max: <img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207154702680.png" alt="image-20210207154702680">(inf:求最小值)</p><p>s.t. : λ<del>i</del> ≥ 0 (i=1~K)</p></li></ol><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207161614446.png" alt="p11-19:17"></p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207162438113.png" alt="p11-22:56"></p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207162758286.png" alt="p11-26:14"></p><p>这里可以去看《Convex optimization》前150页内容，学习推导过程。</p><p>结论可推出：</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207163413689.png" alt="image-20210207163413689"></p><h2 id="支持向量机原问题转化为对偶问题-重点复习"><a href="#支持向量机原问题转化为对偶问题-重点复习" class="headerlink" title="支持向量机原问题转化为对偶问题==(重点复习)=="></a>支持向量机原问题转化为对偶问题==(重点复习)==</h2><p>凸函数的定义。w可能是高维向量，这个代数表达在高维依然适用。</p><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210207164852602.png" alt="image-20210207164852602"></p><p>原问题证明建议重看<a href="https://www.bilibili.com/video/BV1dJ411B7gh?p=12">p12-23:37</a></p><h2 id="支持向量机的应用——兵王问题"><a href="#支持向量机的应用——兵王问题" class="headerlink" title="支持向量机的应用——兵王问题"></a>支持向量机的应用——兵王问题</h2><h3 id="n折交叉验证"><a href="#n折交叉验证" class="headerlink" title="n折交叉验证"></a>n折交叉验证</h3><p>对于<strong>每一组超参数</strong>，进行<strong>n折交叉验证</strong>求损失，最终选取的是损失最小的那一组超参数。</p><h3 id="测试结果中的支持向量"><a href="#测试结果中的支持向量" class="headerlink" title="测试结果中的支持向量"></a>测试结果中的支持向量</h3><p>当支持向量占比非常高，甚至是几乎等于训练样本。则表明这次训练失败，或者数据集本身没有规律，或者SVM 没法找到他的规律。</p><h3 id="评判模型好坏"><a href="#评判模型好坏" class="headerlink" title="评判模型好坏"></a>评判模型好坏</h3><h4 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h4><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210208230203359.png" alt="image-20210208230203359"></p><p>TP: 将正样本识别为正样本的数量（或概率）</p><p>FN: 将正样本识别为负样本的数量（或概率）</p><p>FP: 将负样本识别为正样本的数量（或概率）</p><p>TN: 将负样本识别为负样本的数量（或概率）</p><p>FN减少 &lt;=&gt; TP增加 &lt;=&gt; FP增加 &lt;=&gt; TN减少</p><h4 id="ROC曲线-Receiver-Operating-Character"><a href="#ROC曲线-Receiver-Operating-Character" class="headerlink" title="ROC曲线(Receiver Operating Character)"></a>ROC曲线(Receiver Operating Character)</h4><p><img src="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart2/image-20210209123230240.png" alt="image-20210209123230240"></p><p>四条线表示四个不同的系统</p><p>等错误率 (Equal Error Rate, EER)是两类错误FP和FN相等时候的错误率，这时错误率越小，表示系统性能约好。</p><p>AUC(Area Under Curve)曲线右下角的一块面积，面积的大小也能体现模型的好坏。</p><blockquote><p>判别模型的好坏要看具体的应用，并不是准确率越高，模型高就越好</p></blockquote><h3 id="兵王问题的Python实现-补充"><a href="#兵王问题的Python实现-补充" class="headerlink" title="兵王问题的Python实现==(补充)=="></a>兵王问题的Python实现==(补充)==</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="处理多分类问题"><a href="#处理多分类问题" class="headerlink" title="处理多分类问题"></a>处理多分类问题</h2><ol><li>改造优化的目标函数和限制条件，使之能处理多类<br>论文 SVM-Multiclass Multi-class Support Vector Machine</li><li>一类 VS 其他类</li><li>一类 VS 另一类</li></ol><p>其中，①方法通常不是很好，因为SVM是针对二分类问题开发的。</p><p>在n类问题分类中，②方法主要构造n个SVM，③方法要构造 n * (n-1) / 2 个SVM。</p><p>通常来说，③方法分类噢效果更好，但是也更复杂。</p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学机器学习课程Part1</title>
      <link href="/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart1/"/>
      <url>/2021/02/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8BPart1/</url>
      
        <content type="html"><![CDATA[<h1 id="浙江大学机器学习课程Part1——课程概论"><a href="#浙江大学机器学习课程Part1——课程概论" class="headerlink" title="浙江大学机器学习课程Part1——课程概论"></a>浙江大学机器学习课程Part1——课程概论</h1><p>[TOC]</p><h2 id="推荐书籍-amp-课程"><a href="#推荐书籍-amp-课程" class="headerlink" title="推荐书籍&amp;课程"></a>推荐书籍&amp;课程</h2><ol><li>机器学习， 周志华，清华大学出版社，2016</li><li>统计学习方法，李航，清华大学出版社，2012</li><li>Machine Learning in Action, P. Harrington,人民邮电出版社</li><li>Pattern Recognition and Machine Learning (模式识别与机器学习)，Christopher M. Bishop, 2006</li><li>Machine Learning: A Probabilistic Perspective, K. P. Murphy, </li><li>Machine Learning (机器学习), Tom M. Mitchell, 机械工业出版社，2003年</li><li>Deep Learning, I. Goodfellow, Y. Bengio and A. Courville, 2016</li><li><a href="https://www.coursera.org/course/ml">Stanfrod Web course by Andrew Ng</a></li><li><a href="https://cs231n.stanford.edu/">Stanfrod Web course by Fei-fei Li</a></li></ol><h2 id="机器学习算法分类"><a href="#机器学习算法分类" class="headerlink" title="机器学习算法分类"></a>机器学习算法分类</h2><ol><li>Supervised learning 监督式学习 – SVM, NEURAL NETWORKS</li><li>Unsupervised learning 无监督式学习 – CLUSTERING, EM ALGORITHM, PCA</li><li>Semi-Supervised Learning 半监督式学习</li><li>Reinforcement learning 增强学习</li></ol><p>以预测标签为导向的分类：监督式学习、无监督式学习、半监督式学习。</p><p>不注重过程而关心结果的分类：增强学习。例如：智能驾驶（在不违反交通规则的情况下最快到达目的地）、下棋对战AI（为了最终的输赢有很多种不同的下法）</p><p>Supervised learning:  The machine learning task of inferring a function from labeled training data. Supervised learning can be further divided into </p><ol><li>classification 分类（二分类/多分类） - 离散值标签</li><li>regression 回归（单响应/多响应） - 连续值标签</li></ol><p>两者没有明确界限，以至于有些分类算法也能做回归算法。</p><h2 id="机器学习两大难点"><a href="#机器学习两大难点" class="headerlink" title="机器学习两大难点"></a>机器学习两大难点</h2><ol><li>维度</li><li>标准</li></ol><h2 id="没有免费午餐定理-No-Free-Lunch-Theorem"><a href="#没有免费午餐定理-No-Free-Lunch-Theorem" class="headerlink" title="没有免费午餐定理(No Free Lunch Theorem)"></a>没有免费午餐定理(No Free Lunch Theorem)</h2><p>任何一个预测函数，如果在一些训练样本上表现好，那么必然在另一些训练样本上表现不好，表现好与表现不好的情况一样多。</p><p>如果我们不对特征空间有先验假设，则所有算法的平均表现是一样的。</p><p>我们认为：特征差距小的样本更有可能是同一类。但是，在没有任何先前给定特征意义的情况下，我们都不能确定预测的下一个是什么。</p><p>在这个领域没有最好的算法，但是有公认的好方法。</p>]]></content>
      
      
      <categories>
          
          <category> 浙江大学机器学习课程 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践-完结目录</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践"><a href="#PyTorch深度学习实践" class="headerlink" title="PyTorch深度学习实践"></a>PyTorch深度学习实践</h1><p>b站 - 刘二大人 - 《PyTorch深度学习实践》完结合集 - 笔记</p><p>教程视频传送门：<a href="https://www.bilibili.com/video/BV1Y7411d7Ys?p=13">《PyTorch深度学习实践》完结合集</a></p><p>总之就是非常推荐，很适合新手入门，原理循序渐进，建议认真听，认真做。</p><table><thead><tr><th align="center"><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/">Part1——概论</a></th></tr></thead><tbody><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/">Part2——线性模型</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/">Part3——梯度下降算法</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/">Part4——反向传播</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/">Part5——线性回归</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/">Part6——逻辑斯蒂回归</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/">Part7——处理多维特征的输入</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/">Part8——加载数据集</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/">Part9——多分类问题</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/">Part10——卷积神经网络（基础篇）</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/">Part11——卷积神经网络（高级篇）</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/">Part12——循环神经网络（基础篇）</a></strong></td></tr><tr><td align="center"><strong><a href="../PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/">Part13——循环神经网络（高级篇）</a></strong></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part13</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part13——循环神经网络（高级篇）"><a href="#PyTorch深度学习实践Part13——循环神经网络（高级篇）" class="headerlink" title="PyTorch深度学习实践Part13——循环神经网络（高级篇）"></a>PyTorch深度学习实践Part13——循环神经网络（高级篇）</h1><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210121201135070.png" alt="image-20210121201135070"></p><p>双向循环神经网络</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122101740766.png" alt="image-20210122101740766"></p><h2 id="人名处理"><a href="#人名处理" class="headerlink" title="人名处理"></a>人名处理</h2><ol><li>切分字符串</li><li>转ASCII码</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122103511843.png" alt="image-20210122103511843"></p><ol start="3"><li>填充</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122103548168.png" alt="image-20210122103548168"></p><ol start="4"><li>转置</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122103627700.png" alt="image-20210122103627700"></p><ol start="5"><li>排序</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part13/image-20210122103701821.png" alt="image-20210122103701821"></p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line">HIDDEN_SIZE = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line">N_LAYER = <span class="number">2</span></span><br><span class="line">N_EPOCHS = <span class="number">100</span></span><br><span class="line">N_CHARS = <span class="number">128</span></span><br><span class="line">USE_GPU = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NameDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, is_train_set=<span class="literal">True</span></span>):</span></span><br><span class="line">        filename = <span class="string">&#x27;../names_train.csv.gz&#x27;</span> <span class="keyword">if</span> is_train_set <span class="keyword">else</span> <span class="string">&#x27;../names_test.csv.gz&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            reader = csv.reader(f)</span><br><span class="line">            rows = <span class="built_in">list</span>(reader)</span><br><span class="line">        self.names = [row[<span class="number">0</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows]</span><br><span class="line">        self.<span class="built_in">len</span> = <span class="built_in">len</span>(self.names)</span><br><span class="line">        self.countries = [row[<span class="number">1</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows]</span><br><span class="line">        self.country_list = <span class="built_in">list</span>(<span class="built_in">sorted</span>(<span class="built_in">set</span>(self.countries)))</span><br><span class="line">        self.country_dict = self.getCountryDict()</span><br><span class="line">        self.country_num = <span class="built_in">len</span>(self.country_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.names[index], self.country_dict[self.countries[index]]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getCountryDict</span>(<span class="params">self</span>):</span></span><br><span class="line">        country_dict = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> idx, country_name <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.country_list, <span class="number">0</span>):</span><br><span class="line">            country_dict[country_name] = idx</span><br><span class="line">        <span class="keyword">return</span> country_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">idx2country</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.country_list[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getCountriesNum</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.country_num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">trainset = NameDataset(is_train_set=<span class="literal">True</span>)</span><br><span class="line">trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line">testset = NameDataset(is_train_set=<span class="literal">False</span>)</span><br><span class="line">testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=<span class="literal">False</span>)</span><br><span class="line">N_COUNTRY = trainset.getCountriesNum()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNClassifier</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, n_layers=<span class="number">1</span>, bidirectional=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNNClassifier, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.n_directions = <span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        self.embedding = torch.nn.Embedding(input_size, hidden_size)</span><br><span class="line">        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers,</span><br><span class="line">                                bidirectional=bidirectional)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hidden</span>(<span class="params">self, batch_size</span>):</span></span><br><span class="line">        hidden = torch.zeros(self.n_layers * self.n_directions,</span><br><span class="line">                             batch_size, self.hidden_size)</span><br><span class="line">        <span class="keyword">return</span> create_tensor(hidden)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, seq_lengths</span>):</span></span><br><span class="line">        <span class="comment"># input shape : B x S -&gt; S x B</span></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.t()</span><br><span class="line">        batch_size = <span class="built_in">input</span>.size(<span class="number">1</span>)</span><br><span class="line">        hidden = self._init_hidden(batch_size)</span><br><span class="line">        embedding = self.embedding(<span class="built_in">input</span>)</span><br><span class="line">        <span class="comment"># pack them up</span></span><br><span class="line">        gru_input = pack_padded_sequence(embedding, seq_lengths)</span><br><span class="line">        output, hidden = self.gru(gru_input, hidden)</span><br><span class="line">        <span class="keyword">if</span> self.n_directions == <span class="number">2</span>:</span><br><span class="line">            hidden_cat = torch.cat([hidden[-<span class="number">1</span>], hidden[-<span class="number">2</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden_cat = hidden[-<span class="number">1</span>]</span><br><span class="line">        fc_output = self.fc(hidden_cat)</span><br><span class="line">        <span class="keyword">return</span> fc_output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name2list</span>(<span class="params">name</span>):</span></span><br><span class="line">    arr = [<span class="built_in">ord</span>(c) <span class="keyword">for</span> c <span class="keyword">in</span> name]</span><br><span class="line">    <span class="keyword">return</span> arr, <span class="built_in">len</span>(arr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tensor</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="keyword">if</span> USE_GPU:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        tensor = tensor.to(device)</span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_tensors</span>(<span class="params">names, countries</span>):</span></span><br><span class="line">    sequences_and_lengths = [name2list(name) <span class="keyword">for</span> name <span class="keyword">in</span> names]</span><br><span class="line">    name_sequences = [sl[<span class="number">0</span>] <span class="keyword">for</span> sl <span class="keyword">in</span> sequences_and_lengths]</span><br><span class="line">    seq_lengths = torch.LongTensor([sl[<span class="number">1</span>] <span class="keyword">for</span> sl <span class="keyword">in</span> sequences_and_lengths])</span><br><span class="line">    countries = countries.long()</span><br><span class="line">    <span class="comment"># make tensor of name, BatchSize x SeqLen</span></span><br><span class="line">    seq_tensor = torch.zeros(<span class="built_in">len</span>(name_sequences), seq_lengths.<span class="built_in">max</span>()).long()</span><br><span class="line">    <span class="keyword">for</span> idx, (seq, seq_len) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(name_sequences, seq_lengths), <span class="number">0</span>):</span><br><span class="line">        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)</span><br><span class="line">    <span class="comment"># sort by length to use pack_padded_sequence</span></span><br><span class="line">    seq_lengths, perm_idx = seq_lengths.sort(dim=<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    seq_tensor = seq_tensor[perm_idx]</span><br><span class="line">    countries = countries[perm_idx]</span><br><span class="line">    <span class="keyword">return</span> create_tensor(seq_tensor), create_tensor(seq_lengths), create_tensor(countries)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_since</span>(<span class="params">since</span>):</span></span><br><span class="line">    s = time.time() - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;%dm %ds&#x27;</span> % (m, s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainModel</span>():</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, (names, countries) <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">1</span>):</span><br><span class="line">        inputs, seq_lengths, target = make_tensors(names, countries)</span><br><span class="line">        output = classifier(inputs, seq_lengths)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f&#x27;[<span class="subst">&#123;time_since(start)&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            print(<span class="string">f&#x27;[<span class="subst">&#123;i * <span class="built_in">len</span>(inputs)&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(trainset)&#125;</span>] &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            print(<span class="string">f&#x27;loss=<span class="subst">&#123;total_loss / (i * <span class="built_in">len</span>(inputs))&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testModel</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="built_in">len</span>(testset)</span><br><span class="line">    print(<span class="string">&quot;evaluating trained model ...&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (names, countries) <span class="keyword">in</span> <span class="built_in">enumerate</span>(testloader, <span class="number">1</span>):</span><br><span class="line">            inputs, seq_lengths, target = make_tensors(names, countries)</span><br><span class="line">            output = classifier(inputs, seq_lengths)</span><br><span class="line">            pred = output.<span class="built_in">max</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()</span><br><span class="line">        percent = <span class="string">&#x27;%.2f&#x27;</span> % (<span class="number">100</span> * correct / total)</span><br><span class="line">        print(<span class="string">f&#x27;Test set: Accuracy <span class="subst">&#123;correct&#125;</span>/<span class="subst">&#123;total&#125;</span> <span class="subst">&#123;percent&#125;</span>%&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> correct / total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)</span><br><span class="line">    <span class="keyword">if</span> USE_GPU:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        classifier.to(device)</span><br><span class="line"></span><br><span class="line">    criterion = torch.nn.CrossEntropyLoss()  <span class="comment"># 做的是分类问题，用交叉熵</span></span><br><span class="line">    optimizer = torch.optim.Adam(classifier.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    print(<span class="string">&quot;Training for %d epochs...&quot;</span> % N_EPOCHS)</span><br><span class="line">    acc_list = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N_EPOCHS + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Train cycle</span></span><br><span class="line">        trainModel()</span><br><span class="line">        acc = testModel()</span><br><span class="line">        acc_list.append(acc)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part12</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part12——循环神经网络（基础篇）"><a href="#PyTorch深度学习实践Part12——循环神经网络（基础篇）" class="headerlink" title="PyTorch深度学习实践Part12——循环神经网络（基础篇）"></a>PyTorch深度学习实践Part12——循环神经网络（基础篇）</h1><h2 id="循环神经网络RNN"><a href="#循环神经网络RNN" class="headerlink" title="循环神经网络RNN"></a>循环神经网络RNN</h2><p>之前一开始用的是稠密网络DNN，因为是全连接，所以对每个元素都有相应的权重，因此其计算量是远大于看似复杂但是具有权重共享特性的CNN的。而RNN就是延续权重共享理念的网络。</p><p>RNN主要处理有序列连接的数据，比如自然语言、天气、股市、视频等。</p><p>RNN本质是一个线性层，与DNN不同是RNN Cell是共享的。</p><p>从图像到文本的转换：CNN+FC+RNN。</p><p>循环神经网络的激活函数更常用tanh。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121142256148.png" alt="image-20210121142256148"></p><p>可以选择使用RNN Cell自己构建循环，也可以使用RNN。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121145441952.png" alt="image-20210121145441952"></p><h2 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h2><p>处理文本使用独热编码</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121151229487.png" alt="image-20210121151229487"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121151654777.png" alt="image-20210121151654777"></p><h2 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h2><p>独热编码的缺点：</p><ol><li>维度高</li><li>稀疏</li><li>硬编码</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121153403068.png" alt="image-20210121153403068"></p><p>使用Embedding改善优化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare dataset</span></span><br><span class="line">num_class = <span class="number">4</span></span><br><span class="line">input_size = <span class="number">4</span></span><br><span class="line">hidden_size = <span class="number">8</span></span><br><span class="line">embedding_size = <span class="number">10</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">idx2char = [<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>]</span><br><span class="line">x_data = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]]  <span class="comment"># (batch, seq_len)</span></span><br><span class="line">y_data = [<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]  <span class="comment"># (batch * seq_len)</span></span><br><span class="line">inputs = torch.LongTensor(x_data)</span><br><span class="line">labels = torch.LongTensor(y_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># design model using class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.emb = torch.nn.Embedding(input_size, embedding_size)  <span class="comment"># matrix of Embedding:[input_size, embedding_size]</span></span><br><span class="line">        self.rnn = torch.nn.RNN(input_size=embedding_size,</span><br><span class="line">                                hidden_size=hidden_size,</span><br><span class="line">                                num_layers=num_layers,</span><br><span class="line">                                batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_size, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        hidden = torch.zeros(num_layers, x.size(<span class="number">0</span>), hidden_size)</span><br><span class="line">        x = self.emb(</span><br><span class="line">            x)  <span class="comment"># 这里输入需要是长整型longtensor，输出为(𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒆𝒎𝒃𝒆𝒅𝒅𝒊𝒏𝒈𝑺𝒊𝒛𝒆)，注意batch_first=True</span></span><br><span class="line">        x, _ = self.rnn(x, hidden)  <span class="comment"># 输出(𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛e)</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, num_class)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># training cycle forward, backward, update</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    _, idx = outputs.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">    idx = idx.data.numpy()</span><br><span class="line">    print(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27;&#x27;</span>.join([idx2char[x] <span class="keyword">for</span> x <span class="keyword">in</span> idx]), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;, Epoch [%d/15] loss = %.3f&#x27;</span> % (epoch + <span class="number">1</span>, loss.item()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><p>使用LSTM和GRU训练模型</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121192740594.png" alt="image-20210121192740594"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part12/image-20210121192834591.png" alt="image-20210121192834591"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part11</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part10——卷积神经网络（高级篇）"><a href="#PyTorch深度学习实践Part10——卷积神经网络（高级篇）" class="headerlink" title="PyTorch深度学习实践Part10——卷积神经网络（高级篇）"></a>PyTorch深度学习实践Part10——卷积神经网络（高级篇）</h1><h2 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h2><p>寻找超参数是十分困难的，GoogleNet把不同的模型作成块Inception，在训练时优秀的超参数模块权重自然增加。</p><p>Concatenate拼接四个分支算出来的张量。</p><p>不同的分支，可以有不同的channel，但要有相同的width、height。</p><p>pooling也可以设置padding=1、stride=1来保证输出大小一样。</p><p>1*1卷积，其数量取决于输入张量的通道。</p><p>1*1卷积的信息融合，是在每一个像素点多通道方面的融合。</p><p>1*1卷积主要解决运算量过大的问题，可以减少下一层输入通道数量。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/image-20210119191134365.png" alt="GoogleNet"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/image-20210120113458095.png" alt="image-20210120113458095"></p><p>最后输出的大小一般会去掉线性层，先实例化跑一遍输出size。</p><p>在写网络时，要加上一个存盘功能，即每次准确率达到新高时做一次模型数据备份，防止意外。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare dataset</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment"># Compose参数列表：转为张量；归一化,均值和方差</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">False</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># design model using class</span></span><br><span class="line"><span class="comment"># network in network</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionA</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line">        <span class="comment"># 4条分支</span></span><br><span class="line">        <span class="comment"># 1. 1*1卷积</span></span><br><span class="line">        self.branch1x1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 1*1卷积+5*5卷积</span></span><br><span class="line">        self.branch5x5_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)  <span class="comment"># 保持图像大小不变，kernel=5，则padding=2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 1*1卷积+3*3卷积+3*3卷积</span></span><br><span class="line">        self.branch3x3_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># 保持图像大小不变，kernel=3，则padding=1</span></span><br><span class="line">        self.branch3x3_3 = nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4, 池化(函数)+1*1卷积</span></span><br><span class="line">        self.branch_pool = nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"></span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">        branch3x3 = self.branch3x3_3(branch3x3)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 池化是函数，不需要训练，只在forward中调用</span></span><br><span class="line">        <span class="comment"># 池化也可以使图像大小不变。1. kernel_size=3，则padding=1；2. stride=1</span></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Concatenate</span></span><br><span class="line">        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)  <span class="comment"># b,c,w,h  c对应的是dim=1，沿着channel的维度拼接</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">88</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)  <span class="comment"># 88 = 24x3 + 16</span></span><br><span class="line"></span><br><span class="line">        self.incep1 = InceptionA(in_channels=<span class="number">10</span>)  <span class="comment"># 与conv1 中的10对应</span></span><br><span class="line">        self.incep2 = InceptionA(in_channels=<span class="number">20</span>)  <span class="comment"># 与conv2 中的20对应</span></span><br><span class="line"></span><br><span class="line">        self.mp = nn.MaxPool2d(<span class="number">2</span>)  <span class="comment"># MaxPooling</span></span><br><span class="line">        self.fc = nn.Linear(<span class="number">1408</span>, <span class="number">10</span>)  <span class="comment"># FullConnecting</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.mp(self.conv1(x)))</span><br><span class="line">        x = self.incep1(x)</span><br><span class="line">        x = F.relu(self.mp(self.conv2(x)))</span><br><span class="line">        x = self.incep2(x)</span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)  <span class="comment"># -1指在不告诉函数有多少列的情况下，根据原tensor数据和batch自动分配列数</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># training cycle</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># enumerate()用于可迭代\可遍历的数据对象组合为一个索引序列，同时列出数据和数据下标</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># data里面包含图像数据inputs(tensor)和标签labels(tensor)</span></span><br><span class="line">        inputs, target = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># update</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            print(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, batch_idx + <span class="number">1</span>, running_loss / <span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 不需要计算张量</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    print(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>``torch.max()[0]` 只返回最大值的每个数</p><p><code>troch.max()[1]</code> 只返回最大值的每个索引</p><p><code>torch.max()[1].data</code> 只返回variable中的数据部分（去掉Variable containing:）</p><p><code>torch.max()[1].data.numpy()</code> 把数据转化成numpy ndarry</p><p><code>torch.max()[1].data.numpy().squeeze()</code> 把数据条目中维度为1 的删除掉`</p><h2 id="残差网络-Residual-Net"><a href="#残差网络-Residual-Net" class="headerlink" title="残差网络(Residual Net)"></a>残差网络(Residual Net)</h2><p>随着网络层数增加，越靠近输入模块的梯度更新就越慢，很可能导致<strong>梯度消失</strong>。</p><p>为了解决梯度消失，会在激活之前加入一个跳连接。</p><p>在使用Residual Block时要保持输入和输出通道相同。</p><p>Residual Block相当于把一串Weight Layer包裹起来。</p><p>写神经网络也要写测试方法，检验输出是否和预计相同，逐步增加网络规模（增量式开发）。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part11/image-20210120182501998.png" alt="image-20210120182501998"></p><blockquote><ol><li>从数学和工程学方面重新理解深度学习理论。《深度学习》花书。</li><li>通读PyTorch文档。</li><li>复现经典工作、论文。读代码→写代码</li><li>扩充视野</li></ol></blockquote><p>两篇论文：</p><ol><li>He K, Zhang X, Ren S, et al. Identity Mappings in Deep Residual Networks[C]</li><li>Huang G, Liu Z, Laurens V D M, et al. Densely Connected Convolutional Networks[J]. 2016:2261-2269.</li></ol>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part10</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part10——卷积神经网络（基础篇）"><a href="#PyTorch深度学习实践Part10——卷积神经网络（基础篇）" class="headerlink" title="PyTorch深度学习实践Part10——卷积神经网络（基础篇）"></a>PyTorch深度学习实践Part10——卷积神经网络（基础篇）</h1><h2 id="Basic-CNN"><a href="#Basic-CNN" class="headerlink" title="Basic CNN"></a>Basic CNN</h2><p>CNN(Convolutional Neural Network)结构：特征提取+分类。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118185651529.png" alt="image-20210118185651529"></p><p>通道(Channel)×纵轴(Width)×横轴(Height)，起点为左上角。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118194753606.png" alt="image-20210118194753606"></p><p>Patch逐Width扫描，矩阵作数乘(哈达玛积)。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118195245850.png" alt="image-20210118195245850"></p><p>多通道的卷积中，每一个通道都要配一个卷积核，并相加。</p><p>深度学习里的卷积是数学中的互相关，但是惯例称为卷积，和数学中的卷积有点不同，但是不影响。</p><p>n*n的卷积核，上下各-(n-1)/2，原长宽-(n-1)。n一般采用奇数，卷积形状一般都是正方形，在pytorch中奇偶、长方形都行。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118200458601.png" alt="image-20210118200458601"></p><p>每一组卷积核的通道数量要求和输入通道是一样的。这种卷积核组的总数和输出通道的数量是一样的。卷积过后，通道就与RGB没有关系了。</p><p>卷积(convolution)后，C(Channels)变，W(width)和H(Height)可变可不变，取决于是否填充边缘(padding)，不填充则会有边缘损失。</p><p>卷积层：保留图像的空间信息。卷积本质上也是线性计算，也是可以优化的权重。</p><p>卷积神经网络要求输入输出层是四维张量(Batch, Channel, Width, Height)，卷积层是(m输出通道数量, n输入通道数量, w卷积核宽, h卷积核长)，全连接层的输入与输出都是二维张量(B, Input_feature)。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118200942877.png" alt="image-20210118200942877"></p><p>下采样(subsampling)或池化(pooling)后，C不变，W和H变成 原长度/池化长度。（MaxPool2d是下采样常用的一种，n*n最大池化默认步长为n）</p><p>池化层与sigmoid一样，没有权重。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210118222617886.png" alt="image-20210118222617886"></p><p>卷积(线性变换)，激活函数(非线性变换)，池化；这个过程若干次后，view打平，进入全连接层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">in_channels, out_channels = <span class="number">1</span>, <span class="number">10</span></span><br><span class="line">width, height = <span class="number">10</span>, <span class="number">10</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"><span class="built_in">input</span> = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">2</span>,</span><br><span class="line">         <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">7</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">6</span>,</span><br><span class="line">         <span class="number">7</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line"><span class="comment"># view()将其转化成4维</span></span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(batch_size, </span><br><span class="line">                                 in_channels, </span><br><span class="line">                                 width, </span><br><span class="line">                                 height)</span><br><span class="line"><span class="comment"># 卷积模型的构造函数中，输入通道数量在前，输出通道数量在后；但是卷积的权重shape是先输出后输入</span></span><br><span class="line"><span class="comment"># padding边缘填充，bias一般卷积不用加偏置，stride步长，kernel_size核大小</span></span><br><span class="line">conv_layer = torch.nn.Conv2d(in_channels,</span><br><span class="line">                             out_channels,</span><br><span class="line">                             kernel_size=kernel_size)</span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line">print(<span class="built_in">input</span>.shape)  <span class="comment"># torch.Size([1, 1, 10, 10])</span></span><br><span class="line">print(output.shape)  <span class="comment"># torch.Size([1, 10, 8, 8])</span></span><br><span class="line">print(conv_layer.weight.shape)  <span class="comment"># torch.Size([10, 1, 3, 3])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210119001547566.png" alt="image-20210119001547566"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part10/image-20210119002051268.png" alt="image-20210119002051268"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare dataset</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># design model using class</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)  <span class="comment"># 卷积</span></span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)  <span class="comment"># 池化</span></span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)  <span class="comment"># 线性</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># flatten data from (n,1,28,28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)  <span class="comment"># 先求batch，多少条记录</span></span><br><span class="line">        x = self.pooling(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pooling(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>)  <span class="comment"># -1 此处自动算出的是320</span></span><br><span class="line">        <span class="comment"># print(&quot;x.shape&quot;,x.shape)</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)  <span class="comment"># GPU加速</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># training cycle forward, backward, update</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        inputs, target = inputs.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            print(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, batch_idx + <span class="number">1</span>, running_loss / <span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            images, labels = images.to(device), labels.to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    print(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line">    <span class="keyword">return</span> correct / total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    epoch_list = []</span><br><span class="line">    acc_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        acc = test()</span><br><span class="line">        epoch_list.append(epoch)</span><br><span class="line">        acc_list.append(acc)</span><br><span class="line"></span><br><span class="line">    plt.plot(epoch_list, acc_list)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part9</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part9——多分类问题"><a href="#PyTorch深度学习实践Part9——多分类问题" class="headerlink" title="PyTorch深度学习实践Part9——多分类问题"></a>PyTorch深度学习实践Part9——多分类问题</h1><h2 id="二分类与多分类"><a href="#二分类与多分类" class="headerlink" title="二分类与多分类"></a>二分类与多分类</h2><ol><li><p>多输出之间会有抑制关系，不能用二分类分别对n个目标输出n次。</p></li><li><p>二分类对0/1只需要求对一个的概率就行，但是多分类需要研究分布差异。</p></li><li><p>中间层用Sigmoid变换，最终输出层用Softmax输出一个分布，将每个最终输出z都变化成<strong>大于0且和为1</strong>(先转正，再归一)。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/image-20210118152538935.png" alt="image-20210118152538935"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/image-20210118152129056.png" alt="image-20210118152129056"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/image-20210118153734193.png" alt="image-20210118153734193"></p></li><li><p>在使用交叉熵损失时，最后一层线性输出不用做激活变换。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part9/image-20210118153824744.png" alt="image-20210118153824744"></p></li><li><p>要理解 CrossEntropyLoss 和 LogSoftmax + NLLLoss 之间的区别</p><p>• <a href="https://pytorch.org/docs/stable/nn.html#crossentropyloss">https://pytorch.org/docs/stable/nn.html#crossentropyloss</a></p><p>• <a href="https://pytorch.org/docs/stable/nn.html#nllloss">https://pytorch.org/docs/stable/nn.html#nllloss</a></p></li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms  <span class="comment"># 针对图像进行的处理</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># prepare dataset</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment"># 神经网络输入值在[-1,1]效果最好，服从正态分布</span></span><br><span class="line"><span class="comment"># 构建的是Compose类的对象，参数是列表[]</span></span><br><span class="line"><span class="comment"># transforms.ToTensor()：PIL Image =&gt; PyTorch Tensor，单通道变多通道</span></span><br><span class="line"><span class="comment"># transforms.Normalize((mean,), (std,)：归一化，正态分布需要的期望和标准差，映射到[0,1]分布。数据是算好的，换成标准之后可以解决梯度爆炸问题</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># design model using class</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">784</span>, <span class="number">512</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.linear4 = torch.nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear5 = torch.nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">784</span>)  <span class="comment"># -1自动获取mini-batch：N。把样本[N,1,28,28]转变成[N,784]</span></span><br><span class="line">        x = F.relu(self.linear1(x))</span><br><span class="line">        x = F.relu(self.linear2(x))</span><br><span class="line">        x = F.relu(self.linear3(x))</span><br><span class="line">        x = F.relu(self.linear4(x))</span><br><span class="line">        <span class="keyword">return</span> self.linear5(x)  <span class="comment"># 最后一层不做非线性变换</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct loss and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)  <span class="comment"># momentum冲量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># training cycle</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># update</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            print(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, batch_idx + <span class="number">1</span>, running_loss / <span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 包裹的一部分不会构建计算图</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)  <span class="comment"># 列是dim=0，行是dim=1。返回最大值和最大值的下标</span></span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()  <span class="comment"># 序列求和，一共猜对的数量</span></span><br><span class="line">    print(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br><span class="line"></span><br><span class="line"><span class="comment"># [1,   300] loss: 2.244</span></span><br><span class="line"><span class="comment"># [1,   600] loss: 1.005</span></span><br><span class="line"><span class="comment"># [1,   900] loss: 0.437</span></span><br><span class="line"><span class="comment"># ..........................</span></span><br><span class="line"><span class="comment"># [8,   300] loss: 0.045</span></span><br><span class="line"><span class="comment"># [8,   600] loss: 0.051</span></span><br><span class="line"><span class="comment"># [8,   900] loss: 0.048</span></span><br><span class="line"><span class="comment"># accuracy on test set: 97 % </span></span><br><span class="line"><span class="comment"># [9,   300] loss: 0.034</span></span><br><span class="line"><span class="comment"># [9,   600] loss: 0.039</span></span><br><span class="line"><span class="comment"># [9,   900] loss: 0.044</span></span><br><span class="line"><span class="comment"># accuracy on test set: 97 % </span></span><br><span class="line"><span class="comment"># [10,   300] loss: 0.030</span></span><br><span class="line"><span class="comment"># [10,   600] loss: 0.027</span></span><br><span class="line"><span class="comment"># [10,   900] loss: 0.036</span></span><br><span class="line"><span class="comment"># accuracy on test set: 96 %</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li><a href="https://blog.csdn.net/Answer3664/article/details/99460175">torch.no_grad()</a>  <a href="https://blog.csdn.net/ego_bai/article/details/80873242">Python中with的用法</a></li><li><a href="https://zhuanlan.zhihu.com/p/105783765?utm_source=com.miui.notes">Python中各种下划线的操作</a> </li><li><a href="https://blog.csdn.net/Z_lbj/article/details/79766690">torch.max( )的用法</a> <a href="https://blog.csdn.net/qq_40210586/article/details/103874000">torch.max( )使用讲解</a></li><li>用全连接神经网络训练图像会忽略局部信息的利用，在距离很远的两个点都会产生联系，而这个是没必要的。</li><li>图像的特征提取：傅里叶变换（缺点：都是正弦波）、Wavelet、小波。但是这些都是人工提取。自动提取的有：CNN</li></ol>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part8</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part8——加载数据集"><a href="#PyTorch深度学习实践Part8——加载数据集" class="headerlink" title="PyTorch深度学习实践Part8——加载数据集"></a>PyTorch深度学习实践Part8——加载数据集</h1><h2 id="Dataset-and-DataLoader"><a href="#Dataset-and-DataLoader" class="headerlink" title="Dataset and DataLoader"></a>Dataset and DataLoader</h2><ol><li><p>Dataset：主要构造数据集，支持索引。</p></li><li><p>DataLoader：主要能拿出mini-batch，拿出一组组数据以快速使用。</p><p>改成mini-batch之后，训练循环会变成一个二层的嵌套循环，第一层迭代epoch，第二层迭代mini-batch。</p></li><li><p>Epoch：将所有的样本都参与了一次正向传播、训练，是一次epoch。</p></li><li><p>Batch-Size：每次训练(前馈+反馈+更新)所用的样本数量。</p></li><li><p>Iteration：batch分了多少批，内层的迭代执行多少次。例如：1w个样本，1k个batch，iteration为10。</p></li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/image-20210118100748425.png" alt="image-20210118100748425"></p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>处理数据方式</p><ol><li>全部读取到内存，适用于关系表或者小批量结构化的数据。</li><li>将数据文件分开，路径存放在列表中打包，适用于图像、音频等非结构化数据。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset  <span class="comment"># Dataset是抽象类，需要继承</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理数据</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filepath</span>):</span>  <span class="comment"># 初始化，提供数据集路径加载</span></span><br><span class="line">        xy = np.loadtxt(filepath, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]  <span class="comment"># shape(行数,列数)是元组</span></span><br><span class="line">        self.x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span>  <span class="comment"># 获取数据索引</span></span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]  <span class="comment"># 返回的是元组</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span>  <span class="comment"># 获取数据总量</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = DiabetesDataset(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>)</span><br><span class="line"><span class="comment"># shuffle=True打乱mini-batch保证随机，num_workers多线程</span></span><br><span class="line">train_loader = DataLoader(dataset=dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.activate = torch.nn.Sigmoid()  <span class="comment"># 是模块而不是函数，没有参数，没有需要训练的地方，只用来构建计算图</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.activate(self.linear1(x))</span><br><span class="line">        x = self.activate(self.linear2(x))</span><br><span class="line">        x = self.activate(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失&amp;优化</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"><span class="comment"># model.parameters()会扫描module中的所有成员，如果成员中有相应权重，那么都会将结果加到要训练的参数集合上</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1</span>)</span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  <span class="comment"># 在windows系统下要用if封装训练循环，否则会报错</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        loss_sum = <span class="number">0</span></span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            <span class="comment"># prepare data</span></span><br><span class="line">            inputs, labels = data  <span class="comment"># 此时两个已经转化成tensor</span></span><br><span class="line">            <span class="comment"># Forward</span></span><br><span class="line">            y_pred = model(inputs)</span><br><span class="line">            loss = criterion(y_pred, labels)</span><br><span class="line">            print(epoch, i, loss.item())</span><br><span class="line"></span><br><span class="line">            loss_sum += loss.item()</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Backward</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># Update</span></span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        loss_list.append(loss_sum / num)</span><br><span class="line"></span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), loss_list)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part8/image-20210118110538070.png" alt="image-20210118110538070"></p><p>二层循环速度反而变慢了，效率也没有很大提升？</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part7</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part7——处理多维特征的输入"><a href="#PyTorch深度学习实践Part7——处理多维特征的输入" class="headerlink" title="PyTorch深度学习实践Part7——处理多维特征的输入"></a>PyTorch深度学习实践Part7——处理多维特征的输入</h1><h2 id="多维特征输入"><a href="#多维特征输入" class="headerlink" title="多维特征输入"></a>多维特征输入</h2><p>从单一特征的数据，转而输入多为特征的数据，模型发生以下改变：</p><ol><li><p>对于每一条(/第i条)有n个特征(x1…xn)的数据，则有n个不同的weight(w1…wn)和1个相同的bias(b将进行广播)，并通过非线性激活函数，得出一个y_hat（假设输出维度为1）。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117203233443.png" alt="image-20210117203233443"></p></li><li><p>对于每个zn(=xn*wn+b)都要通过非线性激活函数，Sigmoid函数是对于每个元素的，类似于numpy。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117203726634.png" alt="image-20210117203726634"></p></li><li><p>转换成矩阵运算可以发挥cpu/gpu并行运算的优势。</p></li><li><p>在之前的代码上，想要进行多维输入，只需要修改样本以及模型构造函数。</p></li></ol><h2 id="增加神经网络层数"><a href="#增加神经网络层数" class="headerlink" title="增加神经网络层数"></a>增加神经网络层数</h2><ol><li>如何增加神经网络层数？将多层模型输入和输出，<strong>头尾相连</strong>。例如：torch.nn.Linear(8, 6)、torch.nn.Linear(6, 4)、torch.nn.Linear(4, 1)。</li><li>什么是矩阵？矩阵是<strong>空间变换函数</strong>。例如：y=A*x，y是M×1的矩阵，x是N×1的矩阵，A是M×N的矩阵，则A就是将x从N维转换到y这个M维空间的空间变换函数。</li><li>矩阵是线性变换，但是很多实际情况都是复杂、非线性的。所以，需要用多个线性变换层，通过找到最优的权重组合起来，来模拟非线性的变换。<strong>寻找非线性变换函数</strong>，就是神经网络的本质。</li><li>多层神经网络可以降维也可以升维，至于如何达到最优，则是<strong>超参数的搜索</strong>。</li><li>神经元、网络层数越多，学习能力就越强，但是同时要小心过拟合的问题。要学习数据真值和具备泛化的能力。</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117212227590.png" alt="image-20210117212227590"></p><blockquote><p><strong>能在编程道路上立稳脚跟的核心能力：</strong></p><ol><li>读文档</li><li>基本架构理念(cpu、操作系统、主机、编译原理)</li></ol></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理数据</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)  <span class="comment"># 分隔符&#x27;,&#x27;，大多数显卡只支持32位float</span></span><br><span class="line">x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])  <span class="comment"># 左闭右开，取所有行、第一列到最后第二列。torch.from_numpy返回tensor</span></span><br><span class="line">y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])  <span class="comment"># 取所有行、最后一列。[-1]表示拿出来的是矩阵，-1表示拿出来的是向量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.activate = torch.nn.Sigmoid()  <span class="comment"># 是模块而不是函数，没有参数，没有需要训练的地方，只用来构建计算图</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.activate(self.linear1(x))</span><br><span class="line">        x = self.activate(self.linear2(x))</span><br><span class="line">        x = self.activate(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失&amp;优化</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1.0</span>)</span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500000</span>):</span><br><span class="line">    <span class="comment"># Forward</span></span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># 这里并没有用到mini-batch</span></span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    print(epoch, loss.item())</span><br><span class="line">    loss_list.append(loss.item())</span><br><span class="line">    <span class="comment"># Backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># Update</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">500000</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同网络层参数</span></span><br><span class="line">layer1_weight = model.linear1.weight.data</span><br><span class="line">layer1_bias = model.linear1.bias.data</span><br><span class="line">print(<span class="string">&quot;layer1_weight&quot;</span>, layer1_weight)</span><br><span class="line">print(<span class="string">&quot;layer1_weight.shape&quot;</span>, layer1_weight.shape)</span><br><span class="line">print(<span class="string">&quot;layer1_bias&quot;</span>, layer1_bias)</span><br><span class="line">print(<span class="string">&quot;layer1_bias.shape&quot;</span>, layer1_bias.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li>在100次训练时，损失卡在了0.65。</li><li>在1w次训练时，损失跨过0.65停在了0.45。</li><li>将学习率提升到10.0，1w次训练可以看出图像震荡，无法收敛，但是损失突破0.4以下。</li><li>将学习率调整到1.0，10w次训练，损失突破0.3以下</li><li>学习率1.0，50w次训练，损失达到0.28</li><li><a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">pytorch激活函数文档</a></li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117233025403.png" alt="image-20210117233025403"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part7/image-20210117235832948.png" alt="image-20210117235832948"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part6</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part6——逻辑斯蒂回归"><a href="#PyTorch深度学习实践Part6——逻辑斯蒂回归" class="headerlink" title="PyTorch深度学习实践Part6——逻辑斯蒂回归"></a>PyTorch深度学习实践Part6——逻辑斯蒂回归</h1><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>逻辑斯蒂回归是处理分类问题，而不是回归任务。</p><p>处理分类问题，不能使用回归的思想，即使输出可以为0或1。原因在于：若有一个0-9，10个手写数字的分类问题，在回归模型中，1和0距离很近，0和9离得很远，但是在分类模型中，7和9的相似度就比8与7或9的相似度要高。</p><p>分类问题本质上输出的是概率，例如P(0)、P(1)…。</p><h3 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h3><p>通过考试的概率是多少</p><h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><p>0-9手写数字检测分类</p><h2 id="torchvision工具包"><a href="#torchvision工具包" class="headerlink" title="torchvision工具包"></a>torchvision工具包</h2><p>指定目录，训练/测试，是否需要下载</p><p>MNIST、CIFAR10…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">train_set = torchvision.datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="饱和函数"><a href="#饱和函数" class="headerlink" title="饱和函数"></a>饱和函数</h2><h3 id="逻辑斯蒂函数"><a href="#逻辑斯蒂函数" class="headerlink" title="逻辑斯蒂函数"></a>逻辑斯蒂函数</h3><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117161720715.png" alt="image-20210117161720715"></p><h3 id="其他Sigmoid-functions"><a href="#其他Sigmoid-functions" class="headerlink" title="其他Sigmoid functions"></a>其他Sigmoid functions</h3><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117161834404.png" alt="image-20210117161834404"></p><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><ol><li><p>Logistic Regression类似于正态分布。</p></li><li><p>Logistic Regression是Sigmoid functions中最著名的，所以有些地方用Sigmoid指代Logistic。</p></li><li><p>逻辑斯蒂回归和线性模型的明显区别是在线性模型的后面，添加了激活函数(非线性变换)，将y_hat代入逻辑斯蒂公式中的x。</p></li><li><p><a href="https://blog.csdn.net/C_chuxin/article/details/86174807">交叉熵损失函数的推导过程与直观理解</a></p></li><li><p>y_hat是预测的值[0,1]之间的概率，y是真实值，预测与标签越接近，BCE损失越小。</p></li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117164616620.png" alt="image-20210117164616620"></p><blockquote><p>要计算的是分布的差异，而不是数值上的距离</p></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ol><li><a href="https://blog.csdn.net/weixin_42621901/article/details/107664771">torch.sigmoid()、torch.nn.Sigmoid()和torch.nn.functional.sigmoid()三者之间的区别</a></li><li>BCELoss(Binary CrossEntropyLoss)是CrossEntropyLoss的一个特例，只用于二分类问题，而CrossEntropyLoss可以用于二分类，也可以用于多分类。</li><li><a href="https://www.cnblogs.com/samwoog/p/13857843.html">BCE和CE交叉熵损失函数的区别</a></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------Prepare dataset----------------------------#</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])  <span class="comment"># 二分类</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------------------Design model using Class----------------------------#</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># nn.functional.sigmoid is deprecated</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(self.linear(x))  <span class="comment"># 激活函数sigmoid不需要参数训练，直接调用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line"><span class="comment"># --------------------------Construct loss and optimizer-----------------------------#</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># 交叉熵，size_average=False已经被弃用</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># --------------------------Training cycle-----------------------------#</span></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># 正向传播</span></span><br><span class="line">    loss = criterion(y_pred, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    print(epoch, loss.item())</span><br><span class="line">    loss_list.append(loss.item())  <span class="comment"># 保存loss</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将梯度归零</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line">print(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line">print(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.item())</span><br><span class="line"></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1000</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># y_pred =  0.8808996081352234</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part6/image-20210117172329760.png" alt="image-20210117172329760"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part5</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part5——线性回归"><a href="#PyTorch深度学习实践Part5——线性回归" class="headerlink" title="PyTorch深度学习实践Part5——线性回归"></a>PyTorch深度学习实践Part5——线性回归</h1><h2 id="PyTorch周期"><a href="#PyTorch周期" class="headerlink" title="PyTorch周期"></a>PyTorch周期</h2><ol><li>prepare dataset</li><li>design model using Class 目的是为了前馈forward，即计算y hat(预测值)</li><li>Construct loss and optimizer (using PyTorch API) 其中，计算loss是为了进行反向传播，optimizer是为了更新梯度。</li><li>Training cycle (<u><strong><em>forward,backward,update</em></strong></u>)</li></ol><h2 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h2><p>原本w只是1×1的矩阵，比如tensor([0.], requires_grad=True)，很有可能行列数量与xy对不上，这个时候pytorch会进行<strong>广播</strong>，将w<strong>扩展成一个3×1矩阵</strong>。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117122903497.png" alt="image-20210117122903497"></p><blockquote><p>pytorch直接写“*”表示矩阵<strong>对应位置元素相乘</strong>（哈达玛积），数学上的矩阵乘法有另外的函数torch.matmul</p></blockquote><p>这里x、y的维度都是1（有可能不是1），但是都应当看成一个<strong>矩阵</strong>，而不能是向量。</p><blockquote><p>x、y的列是维度/特征，行是记录/样本</p></blockquote><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ol><li>要把计算模型定义成一个类，<strong>继承于torch.nn.Model</strong>。（nn：neural network）</li><li>如果有pytorch没有提供的需求，或者其效率不够高，可以从Function中继承，构造自己的计算块。</li><li>Linear类包括成员变量weight和bias，默认bias=True，同样继承于torch.nn.Model，可以进行反向传播。</li><li>权重放在x右边，或者转置放在左边。（不管怎么放都是为了凑矩阵基本积）</li><li>父类实现了callable函数，让其能够被调用。在call中会调用前馈forward()，所以必须重写forward()。</li></ol><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117132843838.png" alt="image-20210117132843838"></p><p>*args, **kwargs的用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;args:&#x27;</span>, args)</span><br><span class="line">    print(<span class="string">&#x27;kwargs:&#x27;</span>, kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fun(<span class="number">1</span>, <span class="number">2</span>, <span class="number">7</span>, x=<span class="number">6</span>, y=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># args: (1, 2, 7)</span></span><br><span class="line"><span class="comment"># kwargs: &#123;&#x27;x&#x27;: 6&#125;</span></span><br></pre></td></tr></table></figure><h2 id="损失-amp-优化"><a href="#损失-amp-优化" class="headerlink" title="损失&amp;优化"></a>损失&amp;优化</h2><ol><li>计算损失使用现成的类torch.nn.MSELoss。</li><li>一般使用随机梯度下降算法，求和平均是没有必要的，torch.nn.MSELoss(size_average=<strong>False</strong>)</li><li>使用现成的优化器类torch.optim.SGD</li><li><a href="https://pytorch.org/docs/1.7.0/optim.html">不同的优化器，官方文档</a></li><li>控制训练次数，不能过少（训练不到位），也不能过多（过拟合）</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据，要是矩阵</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型，继承、重写</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成模型的对象</span></span><br><span class="line">model = LinearModel()</span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># size_average=False已经被弃用</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># __call__()调用forward()正向传播计算预测值</span></span><br><span class="line">    loss = criterion(y_pred, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    print(epoch, loss.item())  <span class="comment"># 可以直接打印loss，因为调用的是__str__()不会产生计算图</span></span><br><span class="line">    loss_list.append(loss.item())  <span class="comment"># 保存loss</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将梯度归零</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line">print(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)  <span class="comment"># 使用训练好的模型进行预测</span></span><br><span class="line">print(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印图表</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><p>测试不同的优化器。除了LBFGS，只需要修改调用对应优化器的构造器。</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>w =  0.20570674538612366</p><p>b =  -0.5057424902915955</p><p>y_pred =  0.31708449125289917</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117152240566.png" alt="image-20210117152240566"></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>w =  1.466607928276062</p><p>b =  0.14079217612743378</p><p>y_pred =  6.007224082946777</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117152322232.png" alt="image-20210117152322232"></p><h3 id="Adamax"><a href="#Adamax" class="headerlink" title="Adamax"></a>Adamax</h3><p>w =  -0.022818174213171005</p><p>b =  0.9245702028274536</p><p>y_pred =  0.8332974910736084</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117152149494.png" alt="image-20210117152149494"></p><h3 id="ASGD"><a href="#ASGD" class="headerlink" title="ASGD"></a>ASGD</h3><p>w =  1.6153326034545898</p><p>b =  0.87442547082901</p><p>y_pred =  7.335755825042725</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117151915659.png" alt="image-20210117151915659"></p><h3 id="LBFGS"><a href="#LBFGS" class="headerlink" title="LBFGS"></a>LBFGS</h3><p>由于LBFGS算法需要重复多次计算函数，因此需要传入一个闭包去允许它们重新计算模型。这个闭包应当清空梯度， 计算损失，然后返回。</p><p>参考<a href="https://blog.csdn.net/ys1305/article/details/94332643">一篇关于优化器的博文</a></p><p>训练模型部分代码应修改为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">closure</span>():</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 将梯度归零</span></span><br><span class="line">    y_pred = model(x_data)  <span class="comment"># __call__()调用forward()正向传播计算预测值</span></span><br><span class="line">    loss = criterion(y_pred, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    print(epoch, loss.item())  <span class="comment"># 可以直接打印loss，因为调用的是__str__()不会产生计算图</span></span><br><span class="line">    loss_list.append(loss.item())  <span class="comment"># 保存loss</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.step(closure())  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>w =  1.7660775184631348</p><p>b =  0.531760573387146</p><p>y_pred =  7.596070766448975</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117153648037.png" alt="image-20210117153648037"></p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>w =  1.734222650527954</p><p>b =  0.5857117176055908</p><p>y_pred =  7.522602081298828</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/image-20210117150905304.png" alt="image-20210117150905304"></p><h3 id="Rprop"><a href="#Rprop" class="headerlink" title="Rprop"></a>Rprop</h3><p>w =  1.9997763633728027</p><p>b =  0.0004527860146481544</p><p>y_pred =  7.999558448791504</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/Rprop.png" alt="image-20210117150540886"></p><h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>w =  1.8483222723007202</p><p>b =  0.3447989821434021</p><p>y_pred =  7.738088130950928</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part5/SGD.png" alt="image-20210117150219223"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part4</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part4——反向传播"><a href="#PyTorch深度学习实践Part4——反向传播" class="headerlink" title="PyTorch深度学习实践Part4——反向传播"></a>PyTorch深度学习实践Part4——反向传播</h1><p>对于简单模型可以手动求解析式，但是对于复杂模型求解析式几乎不可能。</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>每一层神经网络包括一次矩阵乘法(Matrix Multiplication)、一次向量加法、非线性变化函数(为了防止展开函数而导致深层神经网络无意义)</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/image-20210116225529823.png" alt="image-20210116225529823"></p><h2 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h2><p>在pytorch中，梯度存在变量而不是计算模块里。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/image-20210116230629292.png" alt="image-20210116230629292"></p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part4/image-20210117091957454.png" alt="image-20210117091957454"></p><p>在计算过程中中，虽然有些变量可以不求导，但是一样要具备能够求导的能力。比如x的值就有可能是前一层网络的y_hat传递下来的。</p><p>核心在于梯度，loss虽然不会作为变量参与计算过程，但是同样需要保留，作为图像数据来判断最终是否收敛。</p><h2 id="PyTorch实现反向传播"><a href="#PyTorch实现反向传播" class="headerlink" title="PyTorch实现反向传播"></a>PyTorch实现反向传播</h2><p>线性模型y=w*x，用pytorch实现反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = torch.Tensor([<span class="number">1.0</span>])  <span class="comment"># data必须是一个序列</span></span><br><span class="line">w.requires_grad = <span class="literal">True</span>  <span class="comment"># 需要计算梯度</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w  <span class="comment"># w是Tensor，运算符重载，x也会转成tensor</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span>  <span class="comment"># 看到代码一定要有意识想到如何构建计算图</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        l = loss(x, y)  <span class="comment"># 前馈过程，正式构建计算图，计算损失更新l</span></span><br><span class="line">        l.backward()  <span class="comment"># 反向传播，计算梯度，释放计算图</span></span><br><span class="line">        print(<span class="string">&#x27;\tgrad:&#x27;</span>, x, y, w.grad.item())</span><br><span class="line">        w.data = w.data - <span class="number">0.01</span> * w.grad.data  <span class="comment"># 更新权重w，注意grad也是一个tensor，不使用.data的话相当于在构建计算图</span></span><br><span class="line"></span><br><span class="line">        w.grad.data.zero_()  <span class="comment"># 将梯度w.grad.data清零</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;progress:&#x27;</span>, epoch, l.item())  <span class="comment"># 取出loss使用l.item，不要直接使用l（l是tensor会构建计算图）</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"><span class="comment"># predict (after training) 4 7.999998569488525</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><p>二次模型y=w1<em>x²+w2</em>x+b的反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># y = x ** 2 + 2 * x + 1</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">4.0</span>, <span class="number">9.0</span>, <span class="number">16.0</span>]</span><br><span class="line"></span><br><span class="line">w1 = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w2 = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">b = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w1.requires_grad, w2.requires_grad, b.requires_grad = <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> w1 * x * x + w2 * x + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        l = loss(x, y)  <span class="comment"># 前馈过程，正式构建计算图，计算损失更新l</span></span><br><span class="line">        l.backward()  <span class="comment"># 反向传播，计算梯度，释放计算图</span></span><br><span class="line">        print(<span class="string">&#x27;\tgrad:&#x27;</span>, x, y, w1.grad.item(), w2.grad.item(), b.grad.item())</span><br><span class="line">        w1.data = w1.data - <span class="number">0.01</span> * w1.grad.data</span><br><span class="line">        w2.data = w2.data - <span class="number">0.01</span> * w2.grad.data</span><br><span class="line">        b.data = b.data - <span class="number">0.01</span> * b.grad.data</span><br><span class="line">        w1.grad.data.zero_()</span><br><span class="line">        w2.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    print(<span class="string">&#x27;progress:&#x27;</span>, epoch, l.item())  <span class="comment"># 取出loss使用l.item，不要直接使用l（l是tensor会构建计算图）</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line">print(w1.data, w2.data, b.data)</span><br><span class="line"><span class="comment"># predict (after training) 4 25.259323120117188</span></span><br><span class="line"><span class="comment"># tensor([1.1145]) tensor([1.4928]) tensor([1.4557])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>改了一下原本的数据集，更符合二次函数，但是因为样本量过少，预测的权重并不是很好。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part3</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part3——梯度下降算法"><a href="#PyTorch深度学习实践Part3——梯度下降算法" class="headerlink" title="PyTorch深度学习实践Part3——梯度下降算法"></a>PyTorch深度学习实践Part3——梯度下降算法</h1><h2 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h2><ol><li><p>上讲是穷举所有可能值并肉眼搜索损失最低点。</p></li><li><p>分治法可能错失关键，最终只找到局部最优</p><blockquote><p>穷举和分治都不能有效解决大数据</p></blockquote></li><li><p>梯度(gradient)决定权重w往哪个方向走，梯度即成本对权重求导，为了控制步伐需要设定一个较小的学习率。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/image-20210116203946162.png" alt="image-20210116203946162"></p></li><li><p>在大量的实验中发现，其实很多情况下，我们很难陷入到局部最优点。但是存在另外一个问题，鞍点。鞍点会导致无法继续迭代，可以选择通过引入动量解决。</p></li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">xs, ys</span>):</span></span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):  <span class="comment"># 3行数据</span></span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred - y) ** <span class="number">2</span>  <span class="comment"># 累加损失平方</span></span><br><span class="line">    <span class="keyword">return</span> cost / <span class="built_in">len</span>(xs)  <span class="comment"># 平均</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">xs, ys</span>):</span></span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        grad += <span class="number">2</span> * x * (x * w - y)  <span class="comment"># 成本对权重求导</span></span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Predict(before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">mse_list = []  <span class="comment"># 保存损失的变化曲线</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost_val = cost(x_data, y_data)  <span class="comment"># 计算损失</span></span><br><span class="line">    mse_list.append(cost_val)  <span class="comment"># 记录损失变化</span></span><br><span class="line">    grad_val = gradient(x_data, y_data)  <span class="comment"># 计算梯度</span></span><br><span class="line">    w -= <span class="number">0.01</span> * grad_val  <span class="comment"># 改善权重</span></span><br><span class="line">    print(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line">print(<span class="string">&#x27;Predict(after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图表打印</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/image-20210116215632394.png" alt="image-20210116215632394"></p><ol><li>绘图时想要消除局部震荡，可以使用指数加权均值方法，使其变成更加平滑的曲线</li><li>如果训练的图像发散，则表明这次训练失败了。其原因有很多，比如，学习率取太大。</li></ol><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>使用梯度下降方法时，更加常用随机梯度下降(Stochastic Gradient Descent)。</p><p>随机梯度下降也是跨越鞍点的一种方法，同时也可以大幅减少计算量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x * (x * w - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Predict(before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">mse_list = []  <span class="comment"># 保存损失的变化曲线</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    x, y, i = <span class="number">0</span>, <span class="number">0</span>, random.randint(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 样本原本就是随机的，所以不需要打乱样本</span></span><br><span class="line">    <span class="keyword">for</span> m, n, j <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data, <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)):</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            x, y = m, n  <span class="comment"># 3组中随机选取一组</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    grad = gradient(x, y)  <span class="comment"># 计算梯度</span></span><br><span class="line">    w -= <span class="number">0.01</span> * grad  <span class="comment"># 改善权重</span></span><br><span class="line">    cost_val = loss(x, y)  <span class="comment"># 计算损失</span></span><br><span class="line">    mse_list.append(cost_val)  <span class="comment"># 记录损失变化</span></span><br><span class="line">    print(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line">print(<span class="string">&#x27;Predict(after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图表打印</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part3/image-20210116222847776.png" alt="image-20210116222847776"></p><p>随机梯度下降可能享受不到并行计算的效率加成，因此会使用折中方法，批量随机梯度下降(Mini-Batch/Batch)</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part2</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part2——线性模型"><a href="#PyTorch深度学习实践Part2——线性模型" class="headerlink" title="PyTorch深度学习实践Part2——线性模型"></a>PyTorch深度学习实践Part2——线性模型</h1><h2 id="一般过程"><a href="#一般过程" class="headerlink" title="一般过程"></a>一般过程</h2><ol><li>Data Set </li><li>Model（神经网络、决策树、朴素贝叶斯）</li><li>Trainning</li><li>Infering</li></ol><h2 id="训练-amp-测试"><a href="#训练-amp-测试" class="headerlink" title="训练&amp;测试"></a>训练&amp;测试</h2><h3 id="训练集拆分"><a href="#训练集拆分" class="headerlink" title="训练集拆分"></a>训练集拆分</h3><p>在竞赛中，训练集是可见的，测试集一般是不可见的。为了提高或验证模型的准确度，一般会把手中的训练集拆分，以及交叉验证。</p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>当模型对训练集的噪声也学习进去的时候，对训练集以外的数据可能会出现准确率下降的情况。因此一个好的模型需要有良好的泛化能力。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/image-20210116101533492.png" alt="image-20210116101533492"></p><p>平均平方误差（MSE MeanSquareError）</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前馈</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 存放结果，所有权重和对应的均方差</span></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"><span class="comment"># 穷举所有权重0.0-4.1步长0.1</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    print(<span class="string">&#x27;w=&#x27;</span>, w)</span><br><span class="line">    l_sum = <span class="number">0</span>  <span class="comment"># 损失的和</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):  <span class="comment"># 打包，一共三行</span></span><br><span class="line">        y_pred_val = forward(x_val)  <span class="comment"># 前馈算出此权重和样本得出的预测值，其实已经包含在loss()中，只是为了打印</span></span><br><span class="line">        loss_val = loss(x_val, y_val)  <span class="comment"># 计算该权重预测值得损失</span></span><br><span class="line">        l_sum += loss_val  <span class="comment"># 求损失和</span></span><br><span class="line">        print(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)  <span class="comment"># 当前的x、y值、预测值、损失</span></span><br><span class="line">    print(<span class="string">&#x27;MSE=&#x27;</span>, l_sum / <span class="number">3</span>)  <span class="comment"># 求损失的平均</span></span><br><span class="line">    <span class="comment"># 保存记录</span></span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum / <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 图表打印</span></span><br><span class="line">plt.plot(w_list, mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/image-20210116155722125.png" alt="image-20210116155722125"></p><blockquote><p>做深度学习要定期存盘，防止意外导致数据丢失</p></blockquote><h2 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里设函数为y=3x+2</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">5.0</span>, <span class="number">8.0</span>, <span class="number">11.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.arange()左闭右开；2.打印 0.0、1.0 时只会显示 0.、1.；3.meshgrid()之后w、b都是41*41矩阵</span></span><br><span class="line"><span class="comment"># 4.这里前馈中是矩阵点对点的运算，但注意并不是矩阵运算，好处是省去了n层for循环，举例：</span></span><br><span class="line"><span class="comment"># a=[[1 2 3][1 2 3]]</span></span><br><span class="line"><span class="comment"># b=[[7 7 7][8 8 8]]</span></span><br><span class="line"><span class="comment"># a*b=[[ 7 14 21][ 8 16 24]]</span></span><br><span class="line">w_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">b_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">w, b = np.meshgrid(w_list, b_list)</span><br><span class="line"></span><br><span class="line">l_sum = <span class="number">0</span>  <span class="comment"># 损失的和</span></span><br><span class="line"><span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):  <span class="comment"># 遍历三次</span></span><br><span class="line">    <span class="comment"># y_pred_val、loss_val都是41*41的矩阵，即41个w和41个b组合的预测结果和损失</span></span><br><span class="line">    y_pred_val = forward(x_val)</span><br><span class="line">    loss_val = loss(x_val, y_val)</span><br><span class="line">    l_sum += loss_val</span><br><span class="line">    print(<span class="string">&#x27;\nx_val：&#x27;</span>, x_val,<span class="string">&#x27;\ny_val：&#x27;</span>, y_val, <span class="string">&#x27;\ny_pred_val：&#x27;</span>,y_pred_val, <span class="string">&#x27;\nloss_val：&#x27;</span>,loss_val)  <span class="comment"># 当前的x、y值、预测值、损失</span></span><br><span class="line">mse_list = l_sum / <span class="number">3</span></span><br><span class="line"><span class="comment"># mse_list也是一个ndarray类型的41*41矩阵</span></span><br><span class="line">print(<span class="string">&#x27;MSE=&#x27;</span>, mse_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3d图表</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(w, b, mse_list)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里设函数为y=3x+2</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">5.0</span>, <span class="number">8.0</span>, <span class="number">11.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里都是矩阵的运算</span></span><br><span class="line"><span class="comment"># 1.arange()左闭右开；2.打印 0.0、1.0 时只会显示 0.、1.；3.meshgrid之后w、b都是41*41矩阵</span></span><br><span class="line">w_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">b_list = np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>)</span><br><span class="line">w, b = np.meshgrid(w_list, b_list)</span><br><span class="line"></span><br><span class="line">l_sum = <span class="number">0</span>  <span class="comment"># 损失的和</span></span><br><span class="line"><span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):  <span class="comment"># 遍历三次</span></span><br><span class="line">    y_pred_val = forward(x_val)</span><br><span class="line">    loss_val = loss(x_val, y_val)</span><br><span class="line">    l_sum += loss_val</span><br><span class="line">    print(<span class="string">&#x27;\nx_val：&#x27;</span>, x_val,<span class="string">&#x27;\ny_val：&#x27;</span>, y_val, <span class="string">&#x27;\ny_pred_val：&#x27;</span>,y_pred_val, <span class="string">&#x27;\nloss_val：&#x27;</span>,loss_val)  <span class="comment"># 当前的x、y值、预测值、损失</span></span><br><span class="line">mse_list = l_sum / <span class="number">3</span></span><br><span class="line"><span class="comment"># mse_list也是一个ndarray类型的41*41矩阵</span></span><br><span class="line">print(<span class="string">&#x27;MSE=&#x27;</span>, mse_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3d图表</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax.plot_surface(w, b, mse_list)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part2/image-20210116155626917.png" alt="image-20210116155626917"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch深度学习实践Part1</title>
      <link href="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/"/>
      <url>/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch深度学习实践Part1——概论"><a href="#PyTorch深度学习实践Part1——概论" class="headerlink" title="PyTorch深度学习实践Part1——概论"></a>PyTorch深度学习实践Part1——概论</h1><h2 id="技术成熟度曲线"><a href="#技术成熟度曲线" class="headerlink" title="技术成熟度曲线"></a>技术成熟度曲线</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/3b87e950352ac65ca9eb5abaa5e3f21692138a21.jpeg" alt="技术成熟度曲线"></p><p><strong>科技诞生的促动期</strong> (Technology Trigger)：在此阶段，随着媒体大肆的报道过度，非理性的渲染，产品的知名度无所不在，然而随着这个科技的缺点、问题、限制出现，失败的案例大于成功的案例，例如:.com公司 1998~2000年之间的非理性疯狂飙升期。</p><p><strong>过高期望的峰值</strong>（Peak of Inflated Expectations）：早期公众的过分关注演绎出了一系列成功的故事——当然同时也有众多失败的例子。对于失败，有些公司采取了补救措施，而大部分却无动于衷。</p><p><strong>泡沫化的底谷期</strong> (Trough of Disillusionment)：在历经前面阶段所存活的科技经过多方扎实有重点的试验，而对此科技的适用范围及限制是以客观的并实际的了解，成功并能存活的经营模式逐渐成长。</p><p><strong>稳步爬升的光明期</strong> (Slope of Enlightenment)：在此阶段，有一新科技的诞生，在市面上受到主要媒体与业界高度的注意，例如:1996年的Internet ，Web。</p><p><strong>实质生产的高峰期</strong> (Plateau of Productivity)：在此阶段，新科技产生的利益与潜力被市场实际接受，实质支援此经营模式的工具、方法论经过数代的演进，进入了非常成熟的阶段。</p><blockquote><p>在使用pytorch或一系列新技术的时候，一定要学会看官方文档，这是一个非常重要的能力！</p></blockquote><h2 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210115233930514.png" alt="AI技术"></p><p>AI除了machine learning之外还有机器视觉、自然语言处理nlp、因果推断等。</p><p>机器学习大部分都是监督学习，即用一组标签过的值进行模型训练。</p><p>机器学习中的算法区别于普通的算法（穷举、贪心等），是通过数据训练并验证得出一个好用的模型，其计算过程来自于数据而不是人工的设计。</p><p>深度学习从模型上看用的是神经网络，从目标上看属于表示学习的分支。方法有，多层感知机、卷积神经网络、循环神经网络等。</p><h2 id="维度诅咒"><a href="#维度诅咒" class="headerlink" title="维度诅咒"></a>维度诅咒</h2><p>随着feature上升，为了保持准确性，其所需的数据量将急速上升，然而获取打过标签的数据，工作量大、成本高。</p><p>n<em>1的向量采样点需要一个3</em>n的矩阵来映射到3*1的向量，实现降维（PCA主成成分分析）。但是降维的同时也要尽量保证高维空间的度量信息，这个过程叫做表示学习（Present）。这个数据分布是在高维空间里的低维流行（Manifold）。</p><h2 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210116083615697.png" alt="image-20210116083615697"></p><h2 id="传统机器学习分类"><a href="#传统机器学习分类" class="headerlink" title="传统机器学习分类"></a>传统机器学习分类</h2><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210116083809610.png" alt="image-20210116083809610"></p><h2 id="神经网络发展"><a href="#神经网络发展" class="headerlink" title="神经网络发展"></a>神经网络发展</h2><p>由生物实验得出，哺乳动物的视觉神经是分层的。浅层只检测物体的运动等，深层才开始识别物体的分类。由此出现了感知机。</p><p>现在神经网络早已不是生物的范畴，而是工程与数学方面。</p><p>真正让神经网络发展起来的是反向传播（Back Propagation），其核心在于计算图。</p><p><img src="/2021/01/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5Part1/image-20210116090403146.png" alt="image-20210116090403146"></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch深度学习实践 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>个人博客建站教程-完结目录</title>
      <link href="/2021/01/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E6%95%99%E7%A8%8B-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"/>
      <url>/2021/01/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E6%95%99%E7%A8%8B-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="个人博客建站教程-完结目录"><a href="#个人博客建站教程-完结目录" class="headerlink" title="个人博客建站教程-完结目录"></a>个人博客建站教程-完结目录</h1><h2 id="个人博客网站教程"><a href="#个人博客网站教程" class="headerlink" title="个人博客网站教程"></a>个人博客网站教程</h2><p>其实不是一件很难的事，花个一点时间，祝每个人都能做出自己风格的博客小家。</p><table><thead><tr><th align="center"><a href="2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/">Part1——博客搭建与部署</a></th></tr></thead><tbody><tr><td align="center"><a href="2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/"><strong>Part2——主题安装与魔改</strong></a></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Hexo博客搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客网站 </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Butterfly主题安装和魔改</title>
      <link href="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/"/>
      <url>/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/</url>
      
        <content type="html"><![CDATA[<h1 id="安装butterfly"><a href="#安装butterfly" class="headerlink" title="安装butterfly"></a>安装butterfly</h1><ol><li>运行<code>git clone https://github.com/jerryc127/hexo-theme-butterfly themes/butterfly</code></li><li>打开_config.yml找到这一行<code>theme: landspace</code>然后将landspace替换butterfly</li><li>安装插件<code>cnpm install hexo-renderer-pug hexo-renderer-stylus</code></li><li>安装插件<code>cnpm install --save hexo-renderer-jade hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive</code></li></ol><blockquote><p>为了以后升级方便，这里不推荐直接对主题的配置文件进行修改，而是复制配置文件进行修改。个人推荐把主題的配置文件_config.yml复制到 Hexo 工作目录下的source/_data/butterfly.yml，如果目录不存在那就创建一个。</p></blockquote><h1 id="butterfly主题魔改"><a href="#butterfly主题魔改" class="headerlink" title="butterfly主题魔改"></a>butterfly主题魔改</h1><p>自己一开始动手做的时候大部分都参考Dreamy.TZK的博客</p><p><a href="https://www.antmoe.com/posts/75a6347a/index.html">Hexo安装并使用Butterfly主题</a></p><p>但是改到后来就越来越觉得，版本问题导致的主题修改不兼容，问题实在很大。甚至到后来想要获得自己的预期效果时，已经不得不去在源代码上下手<del>，因为还没有学过前端，改的属实面目全非</del>。</p><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>这里只能为想要自己动手改的小伙伴一些建议，比如想修改文章页，可以结合浏览器的开发者工具来找到相应的参数，来修改对应的值。</p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/Untitled.png" alt="Butterfly主题安装和魔改/Untitled.png"></p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/Untitled%201.png" alt="Butterfly主题安装和魔改/Untitled%201.png"></p><h2 id="相册的使用"><a href="#相册的使用" class="headerlink" title="相册的使用"></a>相册的使用</h2><p><a href="https://blog.ahzoo.cn/2020/07/20/b7201/">https://blog.ahzoo.cn/2020/07/20/b7201/</a></p><h2 id="关于文章中插入图片"><a href="#关于文章中插入图片" class="headerlink" title="关于文章中插入图片"></a>关于文章中插入图片</h2><p>先把hexo的配置文件中的 relative_link 参数确保为false。否则会导致butterfly各分页面的链接错乱</p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/image-20201231210020980.png" alt="image-20201231210020980"></p><p>把每个文章开头部分加一个参数 relative_link: true。使每个文章部分遵从相对位置的引用，这样可以将文章的图片，不仅在typora或是在服务器上，都能够实时看到自己文章图片引用的效果。</p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/image-20201231205840731.png" alt="image-20201231205840731"></p><p><img src="/2021/01/Butterfly%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%AD%94%E6%94%B9/image-20201231210339382.png" alt="image-20201231210339382"></p><h2 id="关于分类管理-post下的文章"><a href="#关于分类管理-post下的文章" class="headerlink" title="关于分类管理_post下的文章"></a>关于分类管理_post下的文章</h2><p>主要参考<a href="https://blog.csdn.net/maosidiaoxian/article/details/85220394">如何在Hexo中对文章md文件分类</a></p><p>现在文章中的permalink:参数会完全覆盖_config.yml中的设置，要注意。</p><h1 id="后面应该还会慢慢更新一些有用的东西"><a href="#后面应该还会慢慢更新一些有用的东西" class="headerlink" title="后面应该还会慢慢更新一些有用的东西~"></a>后面应该还会慢慢更新一些有用的东西~</h1>]]></content>
      
      
      <categories>
          
          <category> Hexo博客搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hexo静态博客搭建和部署</title>
      <link href="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/"/>
      <url>/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><h2 id="Nodejs"><a href="#Nodejs" class="headerlink" title="Nodejs"></a>Nodejs</h2><p><a href="https://nodejs.org/en/">Node.js</a></p><ul><li><code>node -v</code>确认nodejs版本，安装成功</li></ul><h2 id="Git-Bash"><a href="#Git-Bash" class="headerlink" title="Git Bash"></a>Git Bash</h2><p><a href="https://www.git-scm.com/download/win">Downloading Git</a></p><ul><li><code>git --version</code>确认nodejs版本，安装成功</li></ul><blockquote><p>nodejs和git自己选好安装位置之后无脑下一步就行。以下都以我个人的安装目录（D:\ProgrammingKits\nodejs 和 D:\ProgrammingKits\Git）为前提，请大家各自修改为自己的路径。</p></blockquote><h1 id="Nodejs插件安装"><a href="#Nodejs插件安装" class="headerlink" title="Nodejs插件安装"></a>Nodejs插件安装</h1><ul><li><p>最新的Nodejs自带npm，但是默认安装和缓存地址不在Nodejs根目录下</p><p>  npm的默认全局模块的安装地址是 C:\Users\Administrator\AppData\Roaming\npm</p><p>  npm的默认缓存的地址是 C:\Users\Administrator\AppData\Roaming\npm_cache</p></li></ul><p>首先修改nodejs的prefix（全局）和cache（缓存）文件夹地址</p><ul><li>运行<code>npm config set cache &quot;D:\ProgrammingKits\nodejs\node_cache&quot;</code>设置缓存文件夹</li><li>运行<code>npm config set prefix &quot;D:\ProgrammingKits\nodejs\nodejs&quot;</code>设置全局模块存放路径。</li></ul><p><del>这种方法可以不用像<code>npm config set prefix &quot;D:\ProgrammingKits\nodejs\node_global&quot;</code>需要修改环境变量。</del></p><p>以后npm和cnpm安装的全局模块都会被放到 D:\ProgrammingKits\nodejs\node_modules 下，跟自带的npm模块本体在一个文件夹中。</p><h2 id="cnpm"><a href="#cnpm" class="headerlink" title="cnpm"></a>cnpm</h2><p>这里安装淘宝的cnpm包管理器，以提高下载速度。</p><ul><li>运行<code>npm install -g cnpm --registry=http://registry.npm.taobao.org</code></li><li><code>cnpm -v</code> 确认cnpm版本，安装成功</li></ul><h2 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h2><p>静态博客框架</p><ul><li>运行<code>cnpm install -g hexo-cli</code> 安装hexo框架</li><li><code>hexo -v</code>确认hexo版本，安装成功</li></ul><h1 id="Hexo框架的使用"><a href="#Hexo框架的使用" class="headerlink" title="Hexo框架的使用"></a>Hexo框架的使用</h1><ul><li>hexo常用命令<ul><li><code>hexo init</code>初始化博客</li><li><code>hexo clean</code>清理缓存文件</li><li><code>hexo g</code>生成文件</li><li><code>hexo s</code>运行本地服务器</li><li><code>hexo d</code>部署到服务器</li><li><code>hexo n &quot;MyBlog&quot;</code>创建新的文章</li></ul></li></ul><p>现在我们只需要在 D:\MyBlog\HexoBlog 下运行<code>hexo init &amp; hexo s</code>，在浏览器中输入 <a href="http://localhost:4000/">localhost:4000</a> 即为最初始的博客内容。</p><h1 id="部署博客"><a href="#部署博客" class="headerlink" title="部署博客"></a>部署博客</h1><h2 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h2><p>用来存放你的代码/网站供别人访问</p><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled.png" alt="Hexo静态博客搭建和部署/Untitled.png"></p><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%201.png" alt="Hexo静态博客搭建和部署/Untitled%201.png"></p><h2 id="创建部署分支"><a href="#创建部署分支" class="headerlink" title="创建部署分支"></a>创建部署分支</h2><p>master用来放代码，ph-pages用来部署网站</p><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%202.png" alt="Hexo静态博客搭建和部署/Untitled%202.png"></p><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%203.png" alt="Hexo静态博客搭建和部署/Untitled%203.png"></p><h2 id="开启Gitee-Pages服务"><a href="#开启Gitee-Pages服务" class="headerlink" title="开启Gitee Pages服务"></a>开启Gitee Pages服务</h2><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%204.png" alt="Hexo静态博客搭建和部署/Untitled%204.png"></p><h2 id="创建公钥"><a href="#创建公钥" class="headerlink" title="创建公钥"></a>创建公钥</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;邮箱地址&quot;</span></span><br></pre></td></tr></table></figure><p>密钥对生成后默认的位置是在 C:\Users\Administrator.ssh 的目录下。</p><p>其中 id_rsa 是私钥，id_rsa.pub 是公钥。</p><p>用记事本打开并复制公钥。</p><h2 id="添加公钥"><a href="#添加公钥" class="headerlink" title="添加公钥"></a>添加公钥</h2><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%205.png" alt="Hexo静态博客搭建和部署/Untitled%205.png"></p><p>将复制到的公钥粘贴进去并确定保存。</p><h2 id="安装hexo-deployer-git"><a href="#安装hexo-deployer-git" class="headerlink" title="安装hexo-deployer-git"></a>安装hexo-deployer-git</h2><ul><li>运行<code>npm install hexo-deployer-git --save</code></li></ul><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>打开D:\MyBlog\HexoBlog_config.yml查找deploy，并行修改下面这段代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line"><span class="built_in">type</span>: git</span><br><span class="line">repo: https://gitee.com/NephrenCake/NephrenCake.git</span><br><span class="line">branch: ph-pages</span><br></pre></td></tr></table></figure><h2 id="部署至云端"><a href="#部署至云端" class="headerlink" title="部署至云端"></a>部署至云端</h2><ul><li>运行<code>hexo d</code></li></ul><blockquote><p><a href="https://nephrencake.gitee.io/">https://nephrencake.gitee.io/</a> 即静态博客的地址了。</p></blockquote><ul><li>如果有网页不同步的时候<ul><li>在Gitee Pages 服务中更新部署（每次deploy之后都要手动更新）</li><li>清理浏览器缓存</li></ul></li></ul><p><img src="/2021/01/Hexo%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E9%83%A8%E7%BD%B2/Untitled%206.png" alt="Hexo静态博客搭建和部署/Untitled%206.png"></p>]]></content>
      
      
      <categories>
          
          <category> Hexo博客搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
