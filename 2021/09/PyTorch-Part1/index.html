<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>PyTorch-Part1 | 浅幽丶奈芙莲的个人博客</title><meta name="author" content="NephrenCake"><meta name="copyright" content="NephrenCake"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="基本概念">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch-Part1">
<meta property="og:url" content="https://nephrencake.github.io/2021/09/PyTorch-Part1/index.html">
<meta property="og:site_name" content="浅幽丶奈芙莲的个人博客">
<meta property="og:description" content="基本概念">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://nephrencake.github.io/2021/09/PyTorch-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/%E4%BA%8Ced10.jpg">
<meta property="article:published_time" content="2021-09-11T14:15:30.000Z">
<meta property="article:modified_time" content="2021-10-26T15:14:14.080Z">
<meta property="article:author" content="NephrenCake">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nephrencake.github.io/2021/09/PyTorch-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/%E4%BA%8Ced10.jpg"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="https://nephrencake.github.io/2021/09/PyTorch-Part1/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'PyTorch-Part1',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-10-26 23:14:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="/css/mouse.css"><link rel="stylesheet" href="/css/scrollbar.css"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/iconfont.css"><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="浅幽丶奈芙莲的个人博客" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">103</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/photo/"><i class="fa-fw fas fa-heart"></i><span> 相册</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2021/09/PyTorch-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/%E4%BA%8Ced10.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">浅幽丶奈芙莲的个人博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/photo/"><i class="fa-fw fas fa-heart"></i><span> 相册</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PyTorch-Part1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-11T14:15:30.000Z" title="发表于 2021-09-11 22:15:30">2021-09-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-26T15:14:14.080Z" title="更新于 2021-10-26 23:14:14">2021-10-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="PyTorch-Part1"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="PyTorch-Part1——基本概念"><a href="#PyTorch-Part1——基本概念" class="headerlink" title="PyTorch-Part1——基本概念"></a>PyTorch-Part1——基本概念</h1><p>[TOC]</p>
<h2 id="资源汇总（后续放入总结篇）"><a href="#资源汇总（后续放入总结篇）" class="headerlink" title="资源汇总（后续放入总结篇）"></a>资源汇总（后续放入总结篇）</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/zergtant/pytorch-handbook">PyTorch-handbook 中文手册</a>：与 PyTorch 版本保持一致。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/TingsongYu/PyTorch_Tutorial">《Pytorch模型训练实用教程》</a>：PyTorch 模型训练方面的干货教程。特别工业化，真的非常棒。</li>
<li><a target="_blank" rel="noopener" href="https://www.pytorch123.com/">PyTorch官方教程中文版</a>：标准教程，主要是因为有stn。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/chenyuntc/pytorch-book">《深度学习框架PyTorch：入门与实践》</a>：理论和实战，动漫头像生成器。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/bharathgs/Awesome-pytorch-list">Awesome-Pytorch-list</a>：庞大的 PyTorch 资源库。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/pytorch/examples">PyTorch Examples</a>：入门案例，可以在这个基础上增改自己的代码。</li>
<li><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/">PyTorch Forums</a>：PyTorch 官方论坛，可以经常翻阅，减少弯路。</li>
</ol>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/tszupup/article/details/112916388">检查是否可导</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/291987781">哪些操作不可微</a></li>
</ol>
<p>本篇笔记只记录 PyTorch 常用操作</p>
<ol>
<li>按照训练顺序记录各步骤常用方法</li>
<li>实战干货总结</li>
<li>不同网络案例代码</li>
<li>底层剖析与数学原理</li>
<li>tf1转pytorch</li>
</ol>
<h2 id="Pytorch-简介"><a href="#Pytorch-简介" class="headerlink" title="Pytorch 简介"></a>Pytorch 简介</h2><ul>
<li>Torch 是一个与 Numpy 类似的张量（Tensor）操作库，与 Numpy 不同的是 Torch 对GPU支持的很好，Lua 是 Torch 的上层包装。</li>
<li>PyTorch 和 Torch 使用包含所有相同性能的C库：TH, THC, THNN, THCUNN，只是使用了不同的上层包装语言。</li>
<li>PyTorch 框架设计相当简洁优雅且高效快速。</li>
<li>与 google 的 Tensorflow 类似，FAIR 的支持足以确保 PyTorch 获得持续的开发更新。</li>
<li>PyTorch 拥有完善的文档，作者亲自维护论坛。</li>
</ul>
<h2 id="PyTorch-安装与测试"><a href="#PyTorch-安装与测试" class="headerlink" title="PyTorch 安装与测试"></a>PyTorch 安装与测试</h2><ul>
<li><p>PyTorch 官网：<a target="_blank" rel="noopener" href="https://pytorch.org/">https://pytorch.org/</a></p>
</li>
<li><p>CUDA安装：<a target="_blank" rel="noopener" href="https://blog.csdn.net/Mind_programmonkey/article/details/99688839/">https://blog.csdn.net/Mind_programmonkey/article/details/99688839/</a></p>
</li>
<li><p>测试安装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.__version__</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure></li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</a>)</p>
</li>
</ul>
<h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><h3 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h3><ul>
<li>快速测试可以经常使用 <code>torch.rand()</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建未初始化的5行3列的矩阵，注意和 torch.zeros 是不同的</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 创建一个随机初始化的矩阵</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 创建一个 0 填充的矩阵，数据类型为 long，long 不允许计算梯度</span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line"><span class="comment"># 创建 tensor 并使用现有数据初始化，只要有一个是 float，则都为 float</span></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># _like 方法: 根据现有的张量创建相同大小的张量</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">x = torch.ones_like(x).to(torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># 获取张量大小。size() 和 shape 是等价的。</span></span><br><span class="line">print(x.size())</span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<ul>
<li>可以好好体会下面一个例子: <code>batch_size=2 channel=3 size=(h=4, w=5)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span> x = torch.rand((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([[[[<span class="number">0.0628</span>, <span class="number">0.6673</span>, <span class="number">0.3958</span>, <span class="number">0.0904</span>, <span class="number">0.2442</span>],</span><br><span class="line">          [<span class="number">0.4635</span>, <span class="number">0.0213</span>, <span class="number">0.2310</span>, <span class="number">0.1643</span>, <span class="number">0.7705</span>],</span><br><span class="line">          [<span class="number">0.6754</span>, <span class="number">0.9084</span>, <span class="number">0.3516</span>, <span class="number">0.8552</span>, <span class="number">0.5362</span>],</span><br><span class="line">          [<span class="number">0.0650</span>, <span class="number">0.8016</span>, <span class="number">0.1424</span>, <span class="number">0.3343</span>, <span class="number">0.0216</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.2237</span>, <span class="number">0.5758</span>, <span class="number">0.1204</span>, <span class="number">0.8498</span>, <span class="number">0.4453</span>],</span><br><span class="line">          [<span class="number">0.0703</span>, <span class="number">0.1054</span>, <span class="number">0.4191</span>, <span class="number">0.1271</span>, <span class="number">0.9603</span>],</span><br><span class="line">          [<span class="number">0.4301</span>, <span class="number">0.9627</span>, <span class="number">0.9707</span>, <span class="number">0.9125</span>, <span class="number">0.9281</span>],</span><br><span class="line">          [<span class="number">0.4365</span>, <span class="number">0.1514</span>, <span class="number">0.9759</span>, <span class="number">0.4679</span>, <span class="number">0.8695</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.4225</span>, <span class="number">0.5115</span>, <span class="number">0.2755</span>, <span class="number">0.1248</span>, <span class="number">0.8858</span>],</span><br><span class="line">          [<span class="number">0.4288</span>, <span class="number">0.8042</span>, <span class="number">0.2394</span>, <span class="number">0.6829</span>, <span class="number">0.5082</span>],</span><br><span class="line">          [<span class="number">0.7765</span>, <span class="number">0.7435</span>, <span class="number">0.2163</span>, <span class="number">0.9029</span>, <span class="number">0.6852</span>],</span><br><span class="line">          [<span class="number">0.2889</span>, <span class="number">0.3367</span>, <span class="number">0.8794</span>, <span class="number">0.9265</span>, <span class="number">0.6639</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">0.8373</span>, <span class="number">0.0672</span>, <span class="number">0.8151</span>, <span class="number">0.7912</span>, <span class="number">0.2508</span>],</span><br><span class="line">          [<span class="number">0.9232</span>, <span class="number">0.5653</span>, <span class="number">0.1964</span>, <span class="number">0.0986</span>, <span class="number">0.5448</span>],</span><br><span class="line">          [<span class="number">0.8444</span>, <span class="number">0.8974</span>, <span class="number">0.0763</span>, <span class="number">0.9074</span>, <span class="number">0.7959</span>],</span><br><span class="line">          [<span class="number">0.6146</span>, <span class="number">0.1738</span>, <span class="number">0.0814</span>, <span class="number">0.7200</span>, <span class="number">0.0448</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.9523</span>, <span class="number">0.3441</span>, <span class="number">0.1840</span>, <span class="number">0.9286</span>, <span class="number">0.4231</span>],</span><br><span class="line">          [<span class="number">0.9800</span>, <span class="number">0.4126</span>, <span class="number">0.8632</span>, <span class="number">0.8323</span>, <span class="number">0.2245</span>],</span><br><span class="line">          [<span class="number">0.9756</span>, <span class="number">0.5459</span>, <span class="number">0.1382</span>, <span class="number">0.2115</span>, <span class="number">0.0617</span>],</span><br><span class="line">          [<span class="number">0.8045</span>, <span class="number">0.4060</span>, <span class="number">0.6943</span>, <span class="number">0.0992</span>, <span class="number">0.4955</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.0712</span>, <span class="number">0.9649</span>, <span class="number">0.7187</span>, <span class="number">0.5300</span>, <span class="number">0.8720</span>],</span><br><span class="line">          [<span class="number">0.2673</span>, <span class="number">0.9442</span>, <span class="number">0.5604</span>, <span class="number">0.2986</span>, <span class="number">0.2902</span>],</span><br><span class="line">          [<span class="number">0.8061</span>, <span class="number">0.5989</span>, <span class="number">0.4864</span>, <span class="number">0.7042</span>, <span class="number">0.1167</span>],</span><br><span class="line">          [<span class="number">0.6609</span>, <span class="number">0.0652</span>, <span class="number">0.9130</span>, <span class="number">0.8308</span>, <span class="number">0.6552</span>]]]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="操作张量"><a href="#操作张量" class="headerlink" title="操作张量"></a>操作张量</h3><ul>
<li>加法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.ones(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 1. 直接运算符相加</span></span><br><span class="line">z = x + y</span><br><span class="line"><span class="comment"># 2. .add() 方法</span></span><br><span class="line">z = torch.add(x, y)</span><br><span class="line"><span class="comment"># 3. 替换指定张量</span></span><br><span class="line">z = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=z)</span><br><span class="line"><span class="comment"># 4. 将 x 加在y上</span></span><br><span class="line">y.add_(x)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>任何 以 <code>_</code> 结尾的操作都会用结果替换原变量。例如：<code>x.copy_(y)</code>、<code>x.t_()</code>，都会改变 <code>x</code>。</p>
</blockquote>
<ul>
<li>使用索引切片操作张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x[:, <span class="number">1</span>])</span><br><span class="line">print(x[:, [<span class="number">1</span>]])  <span class="comment"># 依然保持二维</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>torch.view()</code>：改变张量的维度和大小</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand((<span class="number">4</span>, <span class="number">4</span>), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># -1 为自动推断</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure>
<ul>
<li><code>.item()</code>：以Python数据类型获取张量中的数值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(<span class="built_in">type</span>(x.item()))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>100+ Tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc., are described here <a target="_blank" rel="noopener" href="https://pytorch.org/docs/torch">https://pytorch.org/docs/torch</a>.</p>
</blockquote>
<h3 id="NumPy-转换"><a href="#NumPy-转换" class="headerlink" title="NumPy 转换"></a>NumPy 转换</h3><blockquote>
<p>Torch Tensor与NumPy数组共享底层内存地址，修改任何一个都会导致另一个变化。</p>
</blockquote>
<ul>
<li><code>.numpy()</code>：将一个 Torch Tensor 转换为 NumPy 数组</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)  <span class="comment"># tensor([1., 1., 1., 1., 1.])</span></span><br><span class="line">b = a.numpy()  <span class="comment"># [1. 1. 1. 1. 1.]</span></span><br><span class="line"><span class="comment"># 共享内存的操作</span></span><br><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)  <span class="comment"># tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line">print(b)  <span class="comment"># [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="comment"># 不共享内存的操作</span></span><br><span class="line">a = a + <span class="number">1</span></span><br><span class="line">print(a)  <span class="comment"># tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line">print(b)  <span class="comment"># [1. 1. 1. 1. 1.]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>torch.from_numpy(a)</code>：NumPy Array 转化成 Torch Tensor</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line"><span class="comment"># 共享内存的操作</span></span><br><span class="line">a += <span class="number">1</span></span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line"><span class="comment"># 不共享内存的操作</span></span><br><span class="line">a = a + <span class="number">1</span></span><br><span class="line">a = np.add(a, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="CUDA-张量"><a href="#CUDA-张量" class="headerlink" title="CUDA 张量"></a>CUDA 张量</h3><ul>
<li><code>torch.device(&quot;cuda:0&quot;)</code>：参数可以为 <code>&quot;cuda:0&quot;/&quot;cuda&quot;/&quot;cpu&quot;</code>。</li>
<li><code>device参数</code>：可传参同上。</li>
<li><code>.to(device)</code>：可以指定数据类型，也移动到指定设备。<ul>
<li>同时指定时顺序需要为：<code>.to(device, torch.float)</code></li>
<li>注意：只调用 <code>.to(&quot;cuda&quot;)</code> 并没有复制张量到 GPU 上，而是返回了一个 copy。所以，需要把它赋值给一个新的张量并在GPU上使用这个张量。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)        <span class="comment"># 创建 CUDA 设备对象</span></span><br><span class="line">y = torch.ones_like(x, device=device)  <span class="comment"># 直接在 CUDA 中创建张量</span></span><br><span class="line">x = x.to(device)                       <span class="comment"># .to(&quot;cuda&quot;) 将张量移动到 cuda 中</span></span><br><span class="line">z = x + y</span><br><span class="line">print(z)</span><br><span class="line">print(z.to(<span class="string">&quot;cpu&quot;</span>, torch.double))       <span class="comment"># 同时指定设备与类型</span></span><br></pre></td></tr></table></figure>
<h2 id="自动求导机制-Autograd"><a href="#自动求导机制-Autograd" class="headerlink" title="自动求导机制(Autograd)"></a>自动求导机制(Autograd)</h2><ul>
<li> <code>torch.autograd</code> 包是 PyTorch 中所有神经网络的核心，它为张量上的所有操作提供了自动求导。</li>
<li> <code>torch.autograd</code> 是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。</li>
</ul>
<h3 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h3><ul>
<li><p><code>torch.Tensor</code> 是这个包的核心类。</p>
<ul>
<li>如果设置 <code>.requires_grad=True</code>，那么将会追踪所有对于该张量的操作。当完成计算后通过调用 <code>.backward()</code>，自动计算所有的梯度，这个张量的所有梯度将会积累到 <code>.grad</code> 属性。</li>
<li>要阻止张量跟踪历史记录，可以调用 <code>.detach()</code> 方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。</li>
<li>为了防止跟踪历史记录（和使用内存），可以将代码块包装在<code>with torch.no_grad()：</code>中。在评估模型时特别有用，因为模型可能具有 <code>requires_grad = True</code> 的可训练参数，但是我们不需要梯度计算。</li>
</ul>
</li>
<li><p>在自动梯度计算中还有另外一个重要的类 <code>Function</code>.</p>
<ul>
<li><code>Tensor</code> 和 <code>Function</code> 互相连接并生成一个非循环图，它表示和存储了完整的计算历史。每个张量都有一个 <code>.grad_fn</code> 属性，这个属性引用了一个创建了 <code>Tensor</code> 的 <code>Function</code> （除非这个张量是用户手动创建的，即，这个张量的 <code>grad_fn</code> 是 <code>None</code>）。</li>
<li>如果需要计算导数，你可以在 <code>Tensor</code> 上调用 <code>.backward()</code>。 如果 <code>Tensor</code> 是一个标量（即它包含一个元素数据）则不需要为 <code>backward()</code> 指定任何参数，但是如果它有更多的元素，你需要指定一个<code>gradient</code> 参数来匹配张量的形状。</li>
</ul>
</li>
<li><p>在其他的文章中可能会看到说将 Tensor 包裹到 Variable 中提供自动梯度计算。Variable 在0.41版中已经被标注为过期了，现在可以直接使用 Tensor，官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/autograd.html#variable-deprecated">https://pytorch.org/docs/stable/autograd.html#variable-deprecated</a></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)  <span class="comment"># requires_grad=True</span></span><br><span class="line">y = x + <span class="number">2</span>  <span class="comment"># &lt;AddBackward0 object at 0x0000025D5A03A860&gt;</span></span><br><span class="line">print(y)  <span class="comment"># 进行了一次加运算得出结果 y，因此自动生成了 grad_fn 追踪张量操作，但还没有生成梯度</span></span><br><span class="line"></span><br><span class="line">z = y * y * <span class="number">3</span>  <span class="comment"># grad_fn=&lt;MulBackward0&gt;</span></span><br><span class="line">out = z.mean()  <span class="comment"># grad_fn=&lt;MeanBackward0&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><ul>
<li><img src="/2021/09/PyTorch-Part1/image-20210912171608055.png" alt="image-20210912171608055" style="zoom:150%;">

</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 现在让我们来看一个vector-Jacobian product的例子</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这个情形中，y不再是个标量。torch.autograd无法直接计算出完整的雅可比行列，但是如果我们只想要vector-Jacobian product，只需将向量作为参数传入backward：</span></span><br><span class="line">gradients = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(gradients)</span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果.requires_grad=True但是你又不希望进行autograd的计算， 那么可以将变量包裹在 with torch.no_grad()中:</span></span><br><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>autograd</code> 和 <code>Function</code> 的官方文档 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/autograd">https://pytorch.org/docs/autograd</a></p>
</blockquote>
<p>model.train():<br>在使用pytorch构建神经网络的时候，训练过程中会在程序上方添加一句model.train()，作用是启用batch normalization和drop out。</p>
<p>model.eval():<br>测试过程中会使用model.eval()，这时神经网络会沿用batch normalization的值，并不使用drop out。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://nephrencake.github.io">NephrenCake</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://nephrencake.github.io/2021/09/PyTorch-Part1/">https://nephrencake.github.io/2021/09/PyTorch-Part1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://nephrencake.github.io" target="_blank">浅幽丶奈芙莲的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/2021/09/PyTorch-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/%E4%BA%8Ced10.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/09/PyTorch-Part2/"><img class="prev-cover" src="/2021/09/PyTorch-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/%E4%BA%8Ced10.jpg" onerror="onerror=null;src='/img/404.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">PyTorch-Part2</div></div></a></div><div class="next-post pull-right"><a href="/2021/09/Typora-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/"><img class="next-cover" src="/2021/09/Typora-%E5%AE%8C%E7%BB%93%E7%9B%AE%E5%BD%95/%E4%BA%8Ced09.jpg" onerror="onerror=null;src='/img/404.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Typora-完结目录</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-Part1%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">PyTorch-Part1——基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB%EF%BC%88%E5%90%8E%E7%BB%AD%E6%94%BE%E5%85%A5%E6%80%BB%E7%BB%93%E7%AF%87%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">资源汇总（后续放入总结篇）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch-%E7%AE%80%E4%BB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">Pytorch 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-%E5%AE%89%E8%A3%85%E4%B8%8E%E6%B5%8B%E8%AF%95"><span class="toc-number">1.3.</span> <span class="toc-text">PyTorch 安装与测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">1.4.</span> <span class="toc-text">张量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-number">1.4.1.</span> <span class="toc-text">创建张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E5%BC%A0%E9%87%8F"><span class="toc-number">1.4.2.</span> <span class="toc-text">操作张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NumPy-%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.4.3.</span> <span class="toc-text">NumPy 转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CUDA-%E5%BC%A0%E9%87%8F"><span class="toc-number">1.4.4.</span> <span class="toc-text">CUDA 张量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E6%9C%BA%E5%88%B6-Autograd"><span class="toc-number">1.5.</span> <span class="toc-text">自动求导机制(Autograd)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89"><span class="toc-number">1.5.1.</span> <span class="toc-text">张量（Tensor）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.5.2.</span> <span class="toc-text">梯度</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By NephrenCake</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">这里是浅幽丶奈芙莲的个人博客~</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start -->
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?NephrenCake";
            var git_color =['#ebedf0', '#fdcdec', '#fc9bd9', '#fa6ac5', '#f838b2', '#f5089f', '#c4067e', '#92055e', '#540336', '#48022f', '#30021f'];
            var git_user ="NephrenCake";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><!-- hexo injector body_end end --></body></html>